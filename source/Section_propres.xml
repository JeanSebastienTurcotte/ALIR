<?xml version="1.0" encoding="UTF-8"?>

<!-- Ce fichier constitue une section du livre                              -->
<!--                                                                        -->
<!--      Algèbre linéaire : Intuition et rigueur                           -->
<!--                                                                        -->
<!-- Creative Commons Attribution Share Alike 4.0 International             -->
<!-- CC-BY-SA 4.0                                                               -->
<!-- Jean-Sébastien Turcotte, Philémon Turcotte                             -->

<!-- Les sections sont divisées en quatre parties, en plus du titre. Les parties introduction et conclusion sont facultatives. Le texte de ceux-ci apparait respectivement avant et après les sections. Les exercices sont à la fin de la section -->

<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="sec-propre">   <!-- Ajouter l'identifiant de la section après le - du xml:id -->
    <title> Vecteurs et valeurs propres </title>
    <introduction xml:id= "intro-vecvalpropres">  <!-- Ajouter le même identifiant de la section après le - du xml:id -->
    <p>Aller aux <xref ref="exo-propres">exercices</xref> de la section.</p>
    <p>On considère la matrice 
    <men xml:id="eq-Apropre">
    A=\begin{pmatrix}
    10 &amp; 2 \\
6 &amp; 11
    \end{pmatrix}
    </men>.
    On peut observer l'effet géométrique de cette transformation sur le carré unité à la figure <xref ref="fig-vecpropre1"/>. À priori, il n'est pas évident de voir comment décrire la transformation dans des termes géométriques. Est-ce une rotation, une réflexion, un étirement, un cisaillement ou même une combinaison de tout cela? Par contre si on regarde son effet sur le parallélogramme engendré par les vecteurs <m>\vec{u}=(1,2),\vec{v}=(2,-3)</m>, son effet devient clair. La figure <xref ref="fig-vecpropre2"/> permet de voir cet effet.
    </p>
    <sidebyside>
    <figure xml:id="fig-vecpropre1">
    <caption>Une transformation difficile à cerner</caption>
    <image xml:id="img-vecpropre1">
    <sageplot>
v1=vector([1,2])
v2=vector([2,-3])
P=column_matrix([v1,v2])
Pinv=P.inverse()
D=column_matrix([[14,0],[0,7]])
B=P*D*Pinv
#show(B)
Pointscarre=[vector([0,0]),vector([1,0]),vector([1,1]),vector([0,1])]
Pointspara=[vector([0,0]),v1,v1+v2,v2]
Pointscarretrans=[B*pt for pt in Pointscarre]
Pointsparatrans=[B*pt for pt in Pointspara]
graph1=polygon(Pointscarretrans,color="red",alpha=0.25,aspect_ratio=0.75)+polygon(Pointscarre,color="blue",alpha=0.25,aspect_ratio=0.75)
graph2=polygon(Pointsparatrans,color="red",alpha=0.25,aspect_ratio=0.75)+polygon(Pointspara,color="blue",alpha=0.25,aspect_ratio=0.75)
graph1   
    </sageplot>
    </image>
    </figure>
        <figure xml:id="fig-vecpropre2">
    <caption>La même transformation sous un autre angle</caption>
    <image xml:id="img-vecpropre2">
    <sageplot>
v1=vector([1,2])
v2=vector([2,-3])
P=column_matrix([v1,v2])
Pinv=P.inverse()
D=column_matrix([[14,0],[0,7]])
B=P*D*Pinv
#show(B)
Pointscarre=[vector([0,0]),vector([1,0]),vector([1,1]),vector([0,1])]
Pointspara=[vector([0,0]),v1,v1+v2,v2]
Pointscarretrans=[B*pt for pt in Pointscarre]
Pointsparatrans=[B*pt for pt in Pointspara]
graph1=polygon(Pointscarretrans,color="red",alpha=0.25,aspect_ratio=0.75)+polygon(Pointscarre,color="blue",alpha=0.25,aspect_ratio=0.75)
graph2=polygon(Pointsparatrans,color="red",alpha=0.25,aspect_ratio=0.75)+polygon(Pointspara,color="blue",alpha=0.25,aspect_ratio=0.75)
graph2
    </sageplot>
    </image>
    </figure>
    </sidebyside>
    <p>Il semble que la matrice <m>A</m> représente un étirement d'un certain facteur dans la direction <m>(1,2)</m> et un étirement d'un autre facteur dans la direction <m>(2,-3)</m>. Techniquement, on dévie légèrement de la définition donnée à l'exercice <xref ref="exo-etirquelcr2"/>, mais cela ne cause pas de problème en pratique.</p>
    <p>Dans cette section, on verra comment trouver les bonnes directions pour comprendre la géométrie d'une transformation linéaire. Ces directions sont appelées les vecteurs propres et possèdent des nombres associés appelés valeurs propres. On définit la notion de multiplicité algébrique et multiplicité géométrique.</p>
    </introduction>
    <subsection>
    <title>Vecteurs et valeurs propres</title>
    <p>On considère les matrices carrées de format <m>2\times 2</m>. Inspiré par la matrice de l'introduction, on se demande s'il existe des vecteurs dont l'image est un multiple du vecteur initial. Mathématiquement, on cherche des vecteurs <m>\vec{v}_1,\vec{v}_2</m> et des nombres réels <m>\lambda_1,\lambda_2</m> tels que
    <md>
    <mrow>A\vec{v}_1&amp;=\lambda_1\vec{v}_1 &amp; A\vec{v}_2&amp;=\lambda_2\vec{v}_2</mrow>
    </md>,
    où <m>A</m> est une matrice <m>2\times 2</m> quelconque. On regarde l'exemple d'introduction afin de déterminer les nombres <m>\lambda_1,\lambda_2</m> à partir des directions invariantes. La prochaine sous-section expliquera comment obtenir à la fois les <m>\lambda</m> et les directions invariantes.</p>
    <example xml:id="ex-valpropresA1">
    <title>Un premier calcul avec les directions invariantes</title>
    <statement><p>On cherche les deux nombres <m>\lambda_1,\lambda_2</m> associés aux directions invariantes de la matrice
        <me>
    A=\begin{pmatrix}
    10 &amp; 2 \\
6 &amp; 11
    \end{pmatrix}
    </me>.
    </p>
    <p>Ces directions sont <m>\vec{v}_1=(1,2)</m> et <m>\vec{v}_2=(2,-3)</m>.</p></statement>
    <solution>
    <p>Parce que les directions sont données, il suffit de calculer l'image de chacune de ces directions et de déduire le facteur associé. Pour la direction <m>(1,2)</m>, on a
        <me>
    A=\begin{pmatrix}
    10 &amp; 2 \\
6 &amp; 11
    \end{pmatrix}\begin{pmatrix} 1\\ 2\end{pmatrix}=\begin{pmatrix}14 \\ 28 \end{pmatrix}=14\begin{pmatrix} 1\\ 2\end{pmatrix}
    </me>
    et pour la direction <m>(2,-3)</m>, on a
 <me>
    A=\begin{pmatrix}
    10 &amp; 2 \\
6 &amp; 11
    \end{pmatrix}\begin{pmatrix} 2\\ -3\end{pmatrix}=\begin{pmatrix}14 \\ -21\end{pmatrix}=7\begin{pmatrix} 2\\ -3\end{pmatrix}
    </me>.   
    </p>
    <p>La matrice <m>A</m> représente alors un étirement de facteur <m>14</m> dans la direction du vecteur <m>(1,2)</m> et un étirement de facteur <m>7</m> dans la direction <m>(2,-3)</m>.</p>
    </solution>
    </example>
    <p>Parce que les matrices peuvent aussi représenter des transformations linéaires d'espaces vectoriels quelconque, on définit les directions invariantes (et leur facteur associé) par un terme qui s'éloigne un peu de la géométrie.</p>
    <definition xml:id="def-propres">
    <title>Vecteurs et valeurs propres</title>
    <statement><p>
    Soit <m>A</m> une matrice carrée représentant une transformation linéaire d'un espace <m>V</m> vers ce même espace. On dit que <m>\vec{v}\in V</m> est un vecteur propre si <m>\vec{v}\neq \vec{0}</m> et s'il existe un scalaire <m>\lambda</m> pour lesquels
    <men xml:id="eq-propres">
    A\vec{v}=\lambda \vec{v}
    </men>.
    Le scalaire <m>\lambda</m> est alors appelé la valeur propre associée à <m>\vec{v}</m>.
    </p></statement>
    </definition>
    <p>Si on regarde l'équation <xref ref="eq-propres"/> du point de vue d'un système d'équations, on se retrouve avec <m>n</m> inconnues pour le vecteur <m>\vec{v}</m> et une supplémentaire provenant de la valeur propre <m>\lambda</m>. Le fait que cette valeur propre multiplie les composantes du vecteurs fait en sorte que ce système d'équations n'est même pas linéaire. Heureusement, les notions du <xref ref="chap-det" text="type-global"/> et du <xref ref="chap-espvec" text="type-global"/> vont permettre d'avoir une méthode efficace pour trouver les vecteurs et les valeurs propres. Dans un premier temps par contre, on considère certaines questions et on tente d'y répondre en s'appuyant sur des arguments géométriques.</p>
    <p>Une première remarque est qu'un vecteur propre ne peut pas être nul, par définition. Puisque le vecteur nul n'a pas de direction et que les vecteurs propres correspondent géométriquement aux directions invariantes de la transformation linéaire, ce choix prend son sens. D'un autre point de vue, puisque n'importe quelle valeur de <m>\lambda</m> rendrait l'équation <xref ref="eq-propres"/> si <m>\vec{v}=\vec{0}</m>, on n'aurait plus unicité de la valeur propre. Par contre rien n'empêche <m>\lambda</m> d'être nul. Géométriquement, cela signifie qu'une direction est envoyée sur le vecteur nul. Si on pense à une projection orthogonale dans <m>\mathbb{R}^2</m> sur un vecteur <m>\vec{v}</m>, on sait que <m>P(\vec{v})=\vec{v}</m> et <m>P(\vec{v}_{\perp})=\vec{0}</m>. Cela donne donc directement deux vecteurs propres, <m>\vec{v},\vec{v}_{\perp}</m> et leur valeur propre associée, <m>1,0</m>.</p>
    <p>Est-ce qu'une transformation linéaire a toujours des directions invariantes? En regardant l'effet d'une rotation et le fait que le vecteur nul ne peut pas être un vecteur propre, on comprend qu'il est possible qu'il n'y ait pas de vecteurs propres pour une transformation donnée. (Si on se permet de sortir du cadre des nombres réels, alors cela sera possible. On explore cette particularité dans l'annexe <xref provisional="nombrescomplexes"/>.) Y a-t-il un nombre maximum de directions invariantes? Si on pense à la matrice identité, toutes les directions sont invariantes. En termes de vecteurs propres, dès qu'il y en a un, il y a une infinité de vecteurs propres.</p>
    <proposition>
    <title>Vecteurs propres et direction invariante</title>
    <statement><p>Soit <m>\vec{v}</m> un vecteur propre d'une matrice <m>A</m>. Alors pour tout <m>k\neq 0</m> réel, le vecteur <m>k\vec{v}</m> est aussi un vecteur propre.</p>
    <p>Pour une direction invariante, il y a donc une infinité de vecteurs propres.</p>
    </statement>
    <proof>
    <p>Voir l'exercice <xref ref="exo-direcinvariante"/>.</p>
    </proof>
    </proposition>
    <p>La figure interactive suivante permet d'explorer différentes transformations géométriques et demande de déterminer les vecteurs et valeurs propres associés.</p>
    <figure xml:id="fig-propresgeo">
    <caption>Vecteurs et valeurs propres géométriques</caption>
<interactive aspect="1:1" platform="geogebra" width="150%"
      xml:id="geog-propresgeo">
        <slate aspect="1:1" source="code/geogebra/propresgeo.ggb"
        surface="geogebra" xml:id="slate-propresgeo">
            setCoordSystem(-10,10,-10,10);
          </slate>
        <instructions>
          <p>Cliquer sur l'une des quatre transformations à étudier. Déterminer ensuite les valeurs et vecteurs propres en bougeant le vecteur <m>\vec{u}</m> et en observant sa transformation. Écrire les réponses dans les cases appropriées, en prenant soin de mettre la plus petite des valeurs propres et un vecteur propre associé dans la ligne du haut. Cliquer ensuite sur valider les réponses afin de vérifier. Un clic sur un autre type de transformation va générer un nouveau problème.
          </p>
        </instructions>
        </interactive>
    </figure>
    <p>On termine en avec des commandes Sage en lien avec la sous-section.</p>
    <computation>
    <title>Vecteurs et valeurs propres sur Sage</title>
    <statement><p>En anglais, les mots pour vecteur propre et valeur propre sont respectivement <sq>eigenvector</sq> et <sq>eigenvalue</sq>, qui sont eux-mêmes dérivés du mot allemand <sq>eigen</sq>, qui signifie propre, caractéristique ou encore particulier. Sans surprise donc, la commande sur Sage pour trouver  valeurs propres est <c>A.eigenvalues()</c>. Pour les vecteurs propres, Sage doit aussi savoir si on veut un vecteur propre à droite (ce que l'on veut) ou à gauche (car il pourrait aussi exister un vecteur <m>\vec{v}</m> et un scalaire <m>\lambda</m> tels que <m>\vec{v}A=\lambda\vec{v}</m>). La commande est donc <c>A.eigenvectors_right()</c>. Sage retourne alors un triplet composé dans l'ordre de la valeur propre, d'un vecteur propre associée à celle-ci et d'un troisième nombre, appelé la multiplicité, qui sera considéré dans la sous-section suivante. Voici les valeurs et vecteurs propres de la matrice <m>A</m> de l'introduction.</p>
    <sage>
    <input>
A=column_matrix([[10,6],[2,11]])
show(A.eigenvalues())
show(A.eigenvectors_right())
    </input>
    </sage>
    <p>On vérifie que pour une matrice de réflexion quelconque, on a toujours <m>1</m> et <m>-1</m> comme valeurs propres et que les vecteurs propres associés sont la direction de l'axe de réflexion et son perpendiculaire.</p>
    <sage>
    <input>
var("theta")
S=column_matrix([[cos(2*theta),sin(2*theta)],[sin(2*theta),-cos(2*theta)]])
show(S.eigenvalues())
show(S.eigenvectors_right())
#Pas très éclairant sans simplifier. 
#On peut simplifier les valeurs propres
show("Valeur propre #1:", S.eigenvalues()[0].simplify_full())
show("Valeur propre #2:", S.eigenvalues()[1].simplify_full())
#On peut faire de même pour les vecteurs propres, mais on doit d'abord les accéder correctement.
#Ils se retrouvent dans le deuxième élément de chaque élément de la liste du retour et comme unique élément de ce dernier, on les obtient comme cela:
show("Vecteur propre #1:", S.eigenvectors_right()[0][1][0].simplify_full())
show("Vecteur propre #2:", S.eigenvectors_right()[1][1][0].simplify_full())
    </input>
    </sage>
    <p>En multipliant le second vecteur par <m>\cos(\theta)</m> et le premier par <m>-\sin(\theta)</m>, on retrouve les directions voulues.</p>
    <p>Une particularité que l'on peut remarquer est que dans la commande du vecteur propre, le vecteur propre est donné dans une liste qui ne contient que ce vecteur. En fait, pour une valeur spécifique de <m>\lambda</m>, il est possible d'avoir plus d'une direction invariante associée à cette valeur. Il s'avère que l'espace engendré par les vecteurs propres associés à un scalaire <m>\lambda</m> est un sous-espace vectoriel et on peut donc parler d'espace invariant ou espace propre. Ce concept sera exploré dans la sous-section suivante. En attendant, voici les vecteurs et valeurs propres d'une réflexion dans <m>\mathbb{R}^3</m>.</p>
    <sage>
    <input>
S=column_matrix([[-1,0,0],[0,1,0],[0,0,1]])
show(S.eigenvalues())
show(S.eigenvectors_right())    
    </input>
    </sage>
    <p>On remarque que la valeur propre <m>1</m> apparait deux fois dans l'exécution de la commande <c>eigenvalues</c> et que le  vecteur propre qui lui est associé est en fait deux vecteurs. On remarque aussi que le troisième élément de la liste est <m>2</m>, qu'il y a deux vecteurs et que la valeur propre apparassait deux fois. Coincidence?</p>
    </statement>
    </computation>
    </subsection>
    <!-- Sous-sections à écrire, à même ce fichier -->
    <subsection>
    <title>Polynôme caractéristique et espace propre</title>
    <p>L'objectif principal de cette sous-section est de déterminer une manière d'obtenir les valeurs et les vecteurs propres. On a expliqué plus haut que le système d'équations découlant le l'équation matricielle <xref ref="eq-propres"/> n'est pas linéaire par rapport à la variable <m>\lambda</m>, ce qui peut poser problème, spécifiquement en pensant aux méthodes du <xref ref="chap-SEL" text="type-global"/>. On regarde donc à nouveau cette équation. On peut réécrire
    <md>
    <mrow>A\vec{v}&amp;=\lambda \vec{v}</mrow>
    <mrow>A\vec{v}-\lambda \vec{v}&amp;=\vec{0}</mrow>
    <mrow>(A-\lambda I)\vec{v}&amp;=\vec{0}</mrow>
    </md>. De cette nouvelle équation, on peut conclure qu'un vecteur propre doit être dans l'espace nul de la matrice <m>A-\lambda I</m>. De plus, puisque la définition d'un vecteur propre exige que le vecteur soit non nul, il faut que l'espace nul de cette matrice soit de dimension plus grande ou égale à <m>1</m>. Les valeurs propres possibles pour une matrice <m>A</m> sont donc les valeurs de <m>\lambda</m> qui font en sorte que l'espace nul de la matrice <m>A-\lambda I</m> n'est pas réduit à <m>\{\vec{0}\}</m>. Selon le <xref ref="thm-delamatriceinversev5" text="title"/>, ceci se produit lorsque la matrice <m>A-\lambda I</m> n'est pas inversible.</p>
    <p>La manière classique pour déterminer les valeurs propres d'une matrice <m>A</m> est de considérer les valeurs de <m>\lambda</m> qui font que le déterminant de la matrice <m>A-\lambda I</m> est nul. Le déterminant de cette matrice est un polynôme en <m>\lambda</m>, dont les zéros sont précisément les valeurs propres.</p>
    <definition xml:id="def-polynomecaracteristique">
    <title>Polynôme caractéristique d'une matrice</title>
    <statement>
    <p>Soit <m>A</m> une matrice carrée. On appelle le polynôme caractéristique de la matrice <m>A</m> le polynôme résultant du calcul de <m>\text{det}(A-\lambda I)</m>.</p>
    </statement>
    </definition>
    <p>En reformulant les informations ci-dessus ensemble, on obtient alors la proposition suivante.</p>
    <proposition xml:id="prop-valproprepoly">
    <title>Polynôme caractéristique et valeurs propres</title>
    <statement><p>Soit <m>A</m> une matrice carrée. Les valeurs propres de <m>A</m> correspondent aux zéros de son polynôme caractéristique.</p></statement>
    <proof>
    <p>Pour que <m>\lambda</m> soit une valeur propre de la matrice <m>A</m>, il faut qu'il existe un vecteur non nul <m>\vec{v}</m> tel que <m>A\vec{v}=\lambda \vec{v}</m>. Soit <m>\lambda</m> un zéro du polynôme caractéristique. Puisque 
    <m>\text{det}(A-\lambda I)=0</m>, l'espace nul de cette matrice est de dimension supérieure ou égale à <m>1</m>. Soit <m>\vec{v}\in \mathcal{N}(A-\lambda I)</m> un vecteur non nul de cet espace nul. On a alors
    <md>
    <mrow>(A-\lambda I)\vec{v}&amp;=\vec{0} &amp;&amp;\text{par définition de l'espace nul}</mrow>
    <mrow>A\vec{v}-\lambda \vec{v}&amp;=\vec{0} &amp;&amp;\text{par distributivité}</mrow>
    <mrow>A\vec{v}&amp;=\lambda \vec{v}</mrow>
    </md>.</p>
    <p>Ainsi, <m>\lambda</m> est une valeur propre de la matrice <m>A</m>.</p>
    </proof>
    </proposition>
    <example xml:id="ex-valpropre1">
    <title>Calcul des valeurs propres de la matrice d'introduction</title>
    <statement>
    <p>On calcule les valeurs propres de la matrice
    <me>
    A=\begin{pmatrix}
    10 &amp; 2 \\
6 &amp; 11
    \end{pmatrix}
    </me>. À l'exemple <xref ref="ex-valpropresA1"/>, on a déjà calculé ces valeurs propres, mais c'était en connaissant les vecteurs propres qui leur étaient associés.</p>
    </statement>
    <solution>
    <p>On calcule le polynôme caractéristique de la matrice <m>A</m> en calculant le déterminant de <m>A-\lambda I</m>:
    <md>
    <mrow>\lvert A-\lambda I\rvert&amp;=\left\lvert \begin{pmatrix}
    10 &amp; 2 \\
6 &amp; 11
    \end{pmatrix}-\lambda \begin{pmatrix}
    1 &amp; 0 \\
0 &amp; 1
\end{pmatrix}\right \rvert</mrow>
    <mrow>&amp;= \begin{vmatrix}
    10-\lambda &amp; 2 \\
6 &amp; 11-\lambda
    \end{vmatrix}</mrow>
    <mrow>&amp;=(10-\lambda)(11-\lambda)-12</mrow>
    <mrow>&amp;=\lambda^2-21\lambda+98</mrow>
    </md>.
    </p>
    <p>Ce polynôme se factorise en <m>(\lambda-7)(\lambda-14)</m>, ce qui donne les valeurs propres connues <m>\lambda_1=7</m> et <m>\lambda_2=14</m>.</p>
    </solution>
    </example>
    <p>On se tourne vers les transformations linéaires géométriques de l'exemple <xref ref="ex-transfor2"/>. On peut probablement déduire les valeurs propres de plusieurs de ces transformations en s'appuyant seulement sur le contexte géométrique, mais le calcul du polynôme caractéristique permettra de voir si on en oublie.</p>
    <example xml:id="ex-transfor2valpropres">
    <title>Valeurs propres des transformations géométriques de <m>\mathbb{R}^2</m>.</title>
    <statement><p>
    Pour chacune des transformations de la liste <xref ref="liste-transfor2"/>, déterminer les valeurs propres réelles. (Rappel que les matrices de ces tranformations linéaires ont été trouvées à l'exemple <xref ref="ex-mattransfor2"/>.)
    </p></statement>
    <solution>
    <p>La transformation identité garde tous les vecteurs en place. On a donc <m>I\vec{v}=\vec{v}</m> pour tous vecteurs de <m>\mathbb{R}^2</m>. Il s'en suit donc que <m>\lambda=1</m> est une valeur propre. Le polynôme caractéristique de la matrice est 
    <me>
    \begin{vmatrix}1-\lambda &amp; 0\\ 0&amp; 1-\lambda \end{vmatrix}=(1-\lambda)^2
    </me>. 
    Ce polynôme s'annule seulement lorsque <m>\lambda=1</m>, c'est donc la seule valeur propre possible.</p>
    </solution>
    <solution><p>La réflexion par rapport à l'axe des <m>x</m> laisse tout vecteur parallèle à <m>(1,0)</m> inchangé et tout vecteur perpendiculaire à <m>(1,0)</m> est envoyé sur son opposé. Les valeurs propres devraient donc être <m>\lambda_1=1</m> et <m>\lambda_2=-1</m>. En effet, le polynôme caractéristique donne
    <me>
    \begin{vmatrix}1-\lambda &amp; 0\\ 0&amp; -1-\lambda \end{vmatrix}=(1-\lambda)(-1-\lambda)
    </me>.
    Comme ce polynôme s'annule uniquement en <m>\lambda_1=-1</m> et <m>\lambda_2=1</m>, ce sont les seules valeurs propres possibles.
    </p></solution>
    <solution><p>Une rotation ne laisse aucun vecteur en place, sauf le vecteur nul qui ne peut pas être un vecteur propre. Il ne devrait donc pas y avoir de valeurs propres. Le polynôme caractéristique est
    <me>
    \begin{vmatrix}-\lambda &amp; 1\\ -1&amp; -\lambda \end{vmatrix}=\lambda^2+1
    </me>. Ce polynôme n'a en effet pas de solution dans les réels.</p></solution>
    <solution><p>Les étirements ont dans leur définition une direction qui est inchangée. Une valeur propre est très certainement le facteur d'étirement <m>r</m>. Dans l'exercice <xref ref="exo-dirinvariantesR2"/>, on a toutefois mentionné qu'un vecteur dont la direction est perpendiculaire à la direction étirée ne changeait pas. La valeur <m>\lambda=1</m> semble donc aussi être une valeur propre. Dans le cas de l'étirement horizontal, on a
    <me>
    \begin{vmatrix}r-\lambda &amp; 0\\ 0&amp; 1-\lambda \end{vmatrix}=(r-\lambda)(1-\lambda)
    </me>,
    qui s'annule bel et bien en <m>\lambda_1=r</m> et <m>\lambda_2=1</m>.
    </p></solution>
    <solution><p>
    Une homothétie étant un étirement dans les deux directions, il semble plausible de considérer que le facteur <m>r</m> sera la seule valeur propre. En effet, on a
    <me>
    \begin{vmatrix}r-\lambda &amp; 0\\ 0&amp; r-\lambda \end{vmatrix}=(r-\lambda)^2
    </me>,
    qui s'annule seulement lorsque <m>\lambda=r</m>.
    </p></solution>
    <solution><p>Il n'est peut-être pas évident de voir quels seront les valeurs propres en pensant seulement à la permutation des composantes. On peut toutefois remarquer que la permutaiton dans <m>\mathbb{R}^2</m> est aussi une réflexion de direction <m>\pi/4</m>. On peut probablement conclure que les valeurs propres seront <m>1</m> et <m>-1</m>, comme pour les valeurs propres d'une réflexion selon l'axe des abscisses. En effet,
    <me>
    \begin{vmatrix}-\lambda &amp; 1\\ 1&amp; 0-\lambda \end{vmatrix}=\lambda^2-1
    </me>
    s'annule lorsque <m>\lambda_1=-1</m> et <m>\lambda_2=1</m>. Ce sont donc les valeurs propres.</p></solution>
    <solution><p>Pour une projection orthogonale, le vecteur sur lequel on projette demeure inchangé et le vecteur perpendiculaire s'écrase à <m>\vec{0}</m>. On peut donc penser que les valeurs propres seront <m>\lambda_1=1</m> et <m>\lambda_2=0</m>. On a,
    <md>
    <mrow>\begin{vmatrix}\frac{w_1^2}{\norm{\vec{w}}^2}-\lambda &amp; \frac{w_1w_2}{\norm{\vec{w}}^2}\\ \frac{w_1w_2}{\norm{\vec{w}}^2}&amp; \frac{w_2^2}{\norm{\vec{w}}^2}-\lambda \end{vmatrix}&amp;=\left(\frac{w_1^2}{\norm{\vec{w}}^2}-\lambda\right)\left(\frac{w_2^2}{\norm{\vec{w}}^2}-\lambda \right)-w_1^2w_2^2</mrow>  
    <mrow>&amp;=\left(\lambda^2 -\frac{(w_1^2+w_2^2)}{\norm{\vec{w}}^2}\lambda\right)</mrow>
        <mrow>&amp;=\left(\lambda^2 -\frac{\norm{\vec{w}}^2}{\norm{\vec{w}}^2}\lambda\right)</mrow>
        <mrow>&amp;=\lambda^2-\lambda</mrow>
        <mrow>&amp;=\lambda(\lambda-1)</mrow>
    </md>.
    On voit que peu importe les composantes du vecteur <m>(w_1,w_2)</m>, les valeurs propres seront toujours <m>\lambda_1=0</m> et <m>\lambda_2=1</m>.
    </p></solution>
    </example>
    <p>Pour ensuite trouver les vecteurs propres d'une matrice <m>A</m>, on peut regarder l'espace nul de <m>A-\lambda I</m> pour chaque valeur de <m>\lambda</m> faisant en sorte que le polynôme caractéristique s'annule. Bien qu'ils soient relativement facile de les déterminer géométriquement, et on l'a en quelque sorte fait à l'exemple <xref ref="ex-transfor2valpropres"/>, l'exercice <xref ref="exo-transfor2vecpropres"/> déterminera en regardant l'espace propre approprié les vecteurs propres des transformations linéaires de la liste <xref ref="liste-transfor2"/>. Pour le moment, on va regarder les vecteurs propres de la matrice d'introduction.</p>
    <example>
    <title>Calcul des vecteurs propres de la matrice d'introduction</title>
    <statement>
    <p>On calcule les vecteurs propres de la matrice
    <me>
    A=\begin{pmatrix}
    10 &amp; 2 \\
6 &amp; 11
    \end{pmatrix}
    </me>. On sait que ces vecteurs propres devraient être <m>\vec{u}=(1,2),\vec{v}=(2,-3)</m>, selon la figure <xref ref="fig-vecpropre1"/> .</p>
    </statement>
    <solution>
    <p>À l'exemple <xref ref="ex-valpropre1"/>, on a réussit à trouver que les valeurs propres de la matrice sont <m>\lambda_1=7</m> et <m>\lambda_2=14</m>. En caractérisant l'espace propre des matrices <m>A-7I</m> et <m>A-14I</m>, on pourra trouver des vecteurs propres associés à chacune des valeurs propres.</p>
    <p>Dans un premier temps, on a
    <md>
    <mrow>
    A-7I&amp;=\begin{pmatrix}
    3 &amp; 2 \\
6 &amp; 4
    \end{pmatrix}
    </mrow>
    <mrow>&amp;\stackrel{L2-2L_1\to L_2}{\sim}\begin{pmatrix}
    3 &amp; 2 \\
0 &amp; 0
    \end{pmatrix}</mrow>
        <mrow>&amp;\stackrel{\frac{L_1}{3}\to L_1}{\sim}\begin{pmatrix}
    1 &amp; 2/3 \\
0 &amp; 0
    \end{pmatrix}</mrow>
    </md>.
    On peut lire une base de l'espace nul de cette matrice de sa forme échelonnée réduite qui est <m>\vec{u}_1=\left(-\frac{2}{3},1\right)</m>. En multipliant par <m>-3</m> ce vecteur, on retrouve le vecteur <m>\vec{v}=(2,-3)</m> de l'énoncé.
    </p>
    <p>De même, avec la seconde valeur propre, on a
    <md>
    <mrow>
    A-14I&amp;=\begin{pmatrix}
    -4 &amp; 2 \\
6 &amp; -3
    \end{pmatrix}
    </mrow>
    <mrow>&amp;\stackrel{\frac{L_1}{-4}\to L_1}{\sim}\begin{pmatrix}
    1 &amp; -1/2 \\
6 &amp; -3
    \end{pmatrix}</mrow>
        <mrow>&amp;\stackrel{L_2-6L_1\to L2}{\sim}\begin{pmatrix}
    1 &amp; -1/2 \\
0 &amp; 0
    \end{pmatrix}</mrow>
    </md>.
    On peut lire une base de l'espace nul de cette matrice de sa forme échelonnée réduite qui est <m>\vec{v}_1=\left(\frac{1}{2},1\right)</m>. En multipliant par <m>2</m> ce vecteur, on retrouve le vecteur <m>\vec{v}=(1,2)</m> de l'énoncé.    
    </p>
    </solution>
    </example>
    <p>On constate que pour trouver un vecteur propre, il faut regarder l'espace nul de la matrice <m>A-\lambda I</m> et que n'importe quel vecteur de ce sous-espace pourra jouer le rôle de vecteur propre. En particulier, une base de cet espace peut entièrement décrire l'espace des vecteurs propres associés à une valeur propre spécifique d'une matrice <m>A</m>. On appelle <m>\mathcal{N}(A-\lambda I)</m> <em>l'espace propre</em> de la matrice <m>A</m> associé à <m>\lambda</m>.</p>
    <p>On regarde d'autres exemples de calculs de valeurs et vecteurs propres. De ceux-ci on fera quelques observations qui guideront la suite.</p>
    <example xml:id="ex-vecvalpropres">
    <title>Des calculs de vecteurs et valeurs propres</title>
    <statement><p>
    On cherche les valeurs et vecteurs propres des matrices suivantes:
    <md>
    <mrow>A&amp;=\begin{pmatrix} 3&amp;1\\ -2&amp;0 \end{pmatrix}</mrow>
    <mrow>B&amp;=\begin{pmatrix} 1&amp;-3\\ 3&amp;-5 \end{pmatrix}</mrow>
    <mrow>C&amp;=\begin{pmatrix} -2 &amp; 2 &amp; 0 \\ 0 &amp; 2 &amp; 0 \\ 2 &amp; -1 &amp; 0 \end{pmatrix}</mrow>
    <mrow>E&amp;=\begin{pmatrix} 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{pmatrix}</mrow>
    <mrow>F&amp;=\begin{pmatrix} -1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}</mrow>
    </md>.
    </p></statement>
    <solution>
    <p>On commence par trouver les valeurs propres en calculant le déterminant de <m>A-\lambda I</m>. On obtient
    <md>
    <mrow>\lvert A-\lambda I\rvert &amp;=\lvert \begin{pmatrix} 3&amp;1\\ -2&amp;0 \end{pmatrix}-\begin{pmatrix} \lambda&amp;0\\ 0&amp;\lambda \end{pmatrix}\rvert</mrow>
    <mrow>&amp;=\begin{vmatrix} 3-\lambda&amp;1\\ -2&amp;-\lambda \end{vmatrix}</mrow>
    <mrow>&amp;=(3-\lambda)(-\lambda)+2</mrow>
    <mrow>&amp;=\lambda^2-3\lambda+2</mrow>
    <mrow>&amp;=(\lambda-1)(\lambda-2)</mrow>
    </md>.
    Les valeurs propres sont les valeurs de <m>\lambda</m> qui annulent ce déterminant, soit <m>\lambda_1=1</m> et <m>\lambda_2=2</m>.
    </p>
    <p>On trouve le premier vecteur propre en regardant l'espace nul de <m>A-I=\begin{pmatrix} 2&amp;1\\ -2&amp;-1 \end{pmatrix}</m>. En effectuant l'opération élémentaire <m>L_2+L_1\to L_2</m> et ensuite l'opération élémentaire <m>L_1/2\to L_1</m>, on obtient la matrice échelonnée réduite
    <me>
    rref(A-I)=\begin{pmatrix} 1&amp;1/2\\ 0&amp;0 \end{pmatrix}
    </me>.
    Celle-ci donne le vecteur propre <m>\vec{u}_1=(-1/2,1)</m>, ou son multiple <m>\vec{v}_1=(-1,2)</m>. Un calcul rapide montre qu'on a bel et bien <m>A\vec{v}_1=\vec{v}_1</m>.</p>
    <p>Pour le second vecteur propre, on procède de manière similaire. On regarde l'espace propre associé à <m>\lambda_2=2</m>. On a alors
    <md>
    <mrow>A-2I&amp;=\begin{pmatrix} 1&amp;1\\ -2&amp;1 \end{pmatrix}</mrow>
    <mrow>&amp;\stackrel{L2+2L_1\to L_1}{\sim}\begin{pmatrix}1&amp;1\\ 0&amp;0\end{pmatrix}</mrow>
    </md>.
    Une base de cet espace, et donc un vecteur propre, est le vecteur <m>\vec{v}_2=(-1,1)</m>. Encore une fois, un calcul rapide permet d'observer que
    <me>
    \begin{pmatrix} 3&amp;1\\ -2&amp;0 \end{pmatrix}\begin{pmatrix}-1\\ 1\end{pmatrix}=-\begin{pmatrix}3\\-2 \end{pmatrix}+\begin{pmatrix}1\\0 \end{pmatrix}=\begin{pmatrix}-2\\2\end{pmatrix}=2\begin{pmatrix}-1\\ 1\end{pmatrix}
    </me>.
    </p>
    </solution>
    <solution><p>On procède de manière similaire. On a
    <md>
    <mrow>\lvert B-\lambda I\rvert&amp;=\lvert\begin{pmatrix} 1&amp;-3\\ 3&amp;-5 \end{pmatrix}-\begin{pmatrix} \lambda&amp;0\\ 0&amp;0 \end{pmatrix} \rvert</mrow>
    <mrow>&amp;=\begin{vmatrix} 1-\lambda&amp;-3\\ 3&amp;-5-\lambda \end{vmatrix}</mrow>
    <mrow>&amp;=(1-\lambda)(-5-\lambda)+9</mrow>
    <mrow>&amp;=\lambda^2+4\lambda+4</mrow>
    <mrow>&amp;=(\lambda+2)^2</mrow>
    </md>.
    On ne trouve ici qu'une seule valeur propre égale à <m>\lambda=-2</m>. Pour l'espace propre, on regarde l'espace nul de la matrice 
    <m>B+2I=\begin{pmatrix} 3&amp;-3\\ 3&amp;-3 \end{pmatrix}</m>, dont la forme échelonnée réduite est 
    <m>rref(B+2I)=\begin{pmatrix} 1&amp;-1\\ 0&amp;0 \end{pmatrix}</m>. Un vecteur propre est donc <m>\vec{v}=(1,1)</m>. Encore une fois, un calcul permet de vérifier que 
    <me>
    \begin{pmatrix} 1&amp;-3\\ 3&amp;-5 \end{pmatrix}\begin{pmatrix} 1\\ 1\end{pmatrix}=\begin{pmatrix}1-3 \\ 3-5 \end{pmatrix}=\begin{pmatrix}-2\\ -2 \end{pmatrix}=-2\begin{pmatrix} 1\\ 1\end{pmatrix}
    </me>.</p></solution>
    <solution><p>On calcule le déterminant de la matrice <m>C-\lambda I</m> afin de déterminer les valeurs propres. On a
    <md>
    <mrow>\lvert C-\lambda I\rvert&amp;= \begin{vmatrix} -2-\lambda &amp; 2 &amp; 0 \\ 0 &amp; 2-\lambda &amp; 0 \\ 2 &amp; -1 &amp; -\lambda \end{vmatrix}</mrow>
    <intertext><p>En se basant sur la proposition <xref ref="prop-detcofact"/>, on développe selon la deuxième ligne:</p></intertext>
    <mrow>&amp;=(2-\lambda)\begin{vmatrix} -2-\lambda &amp;  0  \\ 2 &amp;  -\lambda \end{vmatrix}</mrow>
    <mrow>&amp;=(2-\lambda)(-2-\lambda)(-\lambda)</mrow>
    </md>.
    Les valeurs propres sont donc <m>\lambda_1=2,\lambda_2=-2</m> et <m>\lambda_3=0</m>.
    </p>
    <p>Pour la recherche des vecteurs propres associés, il faut regarder les espaces nuls des matrices <m>C-\lambda I</m> pour chaque valeur <m>\lambda</m> trouvée. Pour <m>\lambda=2</m>, on trouve
    <md>
    <mrow>\begin{pmatrix} -4 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 2 &amp; -1 &amp; -2 \end{pmatrix}&amp;=\stackrel{L_2\leftrightarrow L_3}{\sim}\begin{pmatrix} -4 &amp; 2 &amp; 0 \\  2 &amp; -1 &amp; -2 \\ 0 &amp; 0 &amp; 0 \\ \end{pmatrix}</mrow>
    <mrow>&amp;\stackrel{L_2+L_1/2\to L_2}{\sim}\begin{pmatrix} -4 &amp; 2 &amp; 0 \\  0 &amp; 0 &amp; -2 \\ 0 &amp; 0 &amp; 0 \\ \end{pmatrix}</mrow>
    <mrow>&amp;\stackrel{\stackrel{L_1/(-4)\to L_1}{L_2/(-2)\to L_2}}{\sim}\begin{pmatrix} 1 &amp; -1/2 &amp; 0 \\  0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0 \\ \end{pmatrix}</mrow>
    </md>.
    Il n'y a qu'une seule variable libre, la deuxième. Une base de l'espace nul est donc <m>\vec{u}_1=(1/2,1,0)</m>, qui constitue donc un vecteur propre. Plus simplement, on peut le multiplier par <m>2</m> pour avoir <m>\vec{v}_1=(1,2,0)</m>.  Un calcul simple montre qu'on a bel et bien <m>C\vec{v}_1=2\vec{v}_1</m>.
    </p>
    <p>Pour le deuxième vecteur propre associé à la valeur propre <m>\lambda_2=-2</m>, on procède de la même manière. On a
    <md>
    <mrow> C+2 I&amp;= \begin{pmatrix} 0 &amp; 2 &amp; 0 \\ 0 &amp; 4 &amp; 0 \\ 2 &amp; -1 &amp; 2 \end{pmatrix}</mrow>
    <mrow>&amp;\stackrel{\stackrel{L_2-2L_1\to L_2}{L_3+L_1/2\to L_3}}{\sim}\begin{pmatrix} 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 2 &amp; 0 &amp; 2 \end{pmatrix}</mrow>
    <mrow>&amp;\stackrel{\stackrel{L_1/2\to L_1}{L_3/2\to L_3}}{\sim}\begin{pmatrix} 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{pmatrix}</mrow>
    <mrow>&amp;\stackrel{L_3\leftrightarrow L_1}{\sim}\begin{pmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix}</mrow>
</md>.
Encore une fois ici, on ne retrouve qu'une variable libre, cette fois en <m>z</m>, qui amène comme vecteur propre le vecteur <m>\vec{v}_2=(-1,0,1)</m>. On vérifie encore qu'on a bien <m>C\vec{v}_2=-2\vec{v}_2</m> par un calcul de produit matrice vecteur.
</p>
<p>Finalement, avec la dernière valeur propre <m>\lambda_3=0</m>, on a
<md>
<mrow>C&amp;=\begin{pmatrix} -2 &amp; 2 &amp; 0 \\ 0 &amp; 2 &amp; 0 \\ 2 &amp; -1 &amp; 0 \end{pmatrix}</mrow>
<mrow>&amp;\stackrel{\stackrel{L_1/(-2)\to L_1}{L_2/2\to L_2}}{\sim}\begin{pmatrix} 1 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 2 &amp; -1 &amp; 0 \end{pmatrix}</mrow>
<mrow>&amp;\stackrel{L_3-2L_1\to L_3}{\sim}\begin{pmatrix} 1 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 \end{pmatrix}</mrow>
<mrow>&amp;\stackrel{L_3-L_2\to L_3}{\sim}\begin{pmatrix} 1 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix}</mrow>
</md>.
Le dernier vecteur propre est <m>\vec{v}_3=(0,0,1)</m>.
</p>
    </solution>
    
    <solution><p>On se permet de faire les calculs sur Sage pour les deux prochaines matrices. On n'utilisera pas les fonctions <c>eigenvalues</c> et <c>eigenvectors_right</c>, car on veut pour l'instant être en mesure de comprendre comment obtenir les valeurs et vecteurs propres à partir des définitions et résultats, mais on laisse Sage faire les calculs de ces déterminants et l'échelonnage des matrices. Pour les valeurs propres:</p>
    <sage>
    <input>
    E=matrix(QQ,[[1,1,0],[0,1,0],[0,0,2]])
    I=matrix.identity(3)
    show((E-x*I).determinant())    
    </input>
    </sage>
    <p>Cette fois, les valeurs propres sont <m>\lambda_1=1</m> et <m>\lambda_2=2</m>. On remarque qu'il n'y a encore une fois que deux valeurs propres.</p>
    <p>Un premier vecteur propre est obtenu en regardant l'espace propre associé à <m>\lambda_1</m>.</p>
    <sage>
    <input>
    show((E-I).right_kernel(basis="pivot").basis())
    </input>
    </sage>
    <p>Le vecteur propre est <m>\vec{v}_1=(1,0,0)</m>. Pour la seconde valeur propre :</p>
    <sage>
    <input>
    show((E-2*I).right_kernel(basis="pivot").basis())
    </input>
    </sage>
    <p>Le second vecteur propre est <m>\vec{v}_2=(0,0,1)</m>.</p>
    <p>On remarque dans ce cas-ci que chaque valeur propre n'a qu'une direction propre associée.</p>
    </solution>
    <solution><p> On répète la démarche précédente avec la matrice <m>F</m> et Sage.  </p>
    <p>D'abord pour les valeurs propres, on calcule le déterminant de la matrice <m>F-\lambda I</m> et on demande à Sage de factoriser le tout pour pouvoir y lire facilement les zéros. À noter qu'on utilise <m>x</m> plutôt que <m>\lambda</m>, car le mot <c>lambda</c> est protégé en python.</p>
    <sage>
    <input>
    F=matrix(QQ,[[-1,1,0],[0,1,0],[0,0,1]]) #Le QQ sera nécessaire pour utiliser les commandes reliées à l'espace nul dans le calcul des vecteurs propres
    I=matrix.identity(3)
    show((F-x*I).determinant())
    </input>
    </sage>
    <p>Il n'y a que deux valeurs propres distinctes, <m>\lambda_1=-1</m> et <m>\lambda_2=1</m>. Pour trouver les vecteurs propres, on trouve une base pour les espaces nuls des deux matrices <m>D+I</m> et <m>D-I</m>.</p>
    <sage>
    <input>
    show((F+I).right_kernel(basis="pivot").basis())
    </input>
    </sage>
    <p>Un vecteur propre associé à <m>\lambda=-1</m> est donc <m>\vec{v}_1=(1,0,0)</m>. On vérifie facilement que <m>F\vec{v}_1=-\vec{v}_1</m>, puisque ce produit donne la première colonne de la matrice <m>F</m>. Pour <m>\lambda_2=1</m>, on regarde ce que Sage donne comme espace propre.</p>
    <sage>
    <input>
    show((F-I).right_kernel(basis="pivot").basis())
    </input>
    </sage>
    <p>On constate qu'il y a ici deux vecteurs propres associés à la valeur propre <m>\lambda_2=1</m>. Le vecteur <m>\vec{v}_2=(1,2,0)</m> (après multiplication par <m>2</m>)  et le vecteur <m>\vec{v}_3=(0,0,1)</m> ont la propriété que
    <md>
    <mrow>F\vec{v}_2&amp;=\vec{v}_2 &amp; F\vec{v}_3=\vec{v}_3</mrow>
    </md>.</p>
    </solution>
    </example>
    <p>L'exemple précédent montre que plusieurs situations peuvent survenir lors du calcul des valeurs et vecteurs propres. Est-ce que le déterminant d'une matrice <m>A-\lambda I</m>, où <m>A</m> est une matrice <m>n\times n</m>, est toujours un polynôme de degré <m>n</m>? Cela semble évident, mais il faudrait le démontrer. (Voir l'exercice <xref provisional="exo-detvalpropredegre"/>). Parfois, pour une valeur propre, il peut y avoir plus d'un vecteur (direction en fait) propre. En fait, la véritable question est quelle peut être la dimension de l'espace propre associée à une valeur propre? La matrice <m>D</m> de l'exemple précédent avait deux valeurs propres, l'une dont l'espace propre était de dimension <m>1</m> et l'autre de dimension <m>2</m>. La matrice <m>E</m> quant à elle avait aussi deux valeurs propres, mais avec chacun leur espace propre respectif de dimension <m>1</m>. Ceci mène à la définition suivante.</p>
    <definition xml:id="def-multalggeo">
    <title>Multiplicité algébrique et géométrique</title>
    <statement><p>Soit <m>A</m> une matrice, <m>\lambda_0</m> une valeur propre de cette matrice et <m>k\in \mathbb{N}</m> la plus grande valeur telle que le facteur <m>(\lambda-\lambda_0)^k</m> divise le polynôme caractéristique. On dit que <m>k</m> est la multiplicité algébrique de la valeur propre.</p>
    <p>Soit <m>r=\text{dim}(\mathcal{N}(A-\lambda_0 I))</m> la dimension de l'espace nul de la matrice <m>A-\lambda_0 I</m>. On dit que <m>r</m> est la multiplicité géométrique de la valeur propre.</p>
    </statement>
    </definition>
    <p>La valeur propre <m>\lambda_2=1</m> pour la matrice <m>D</m> de l'exemple <xref ref="ex-vecvalpropres"/> a une multiplicité algébrique de <m>2</m> puisque le déterminant de <m>D-\lambda I</m> est <m>(\lambda+1)(\lambda-1)^2</m>. Puisque l'espace propre associé à cette valeur propre était donné par <m>\vspan((1,2,0),(0,0,1))</m>. Ces vecteurs étant indépendants, la dimension du sous-espace est <m>2</m>, qui est donc la dimension géométrique de <m>\lambda_2=1</m>. Par contre, avec la matrice <m>E</m>, toujours avec la valeur propre valant <m>1</m>, on avait encore une dimension algébrique égale à <m>2</m>, mais comme l'espace nul de <m>E-\lambda I</m> est engendré par <m>(1,0,0)</m>, la dimension géométrique est de <m>1</m>. Sur Sage, lorsqu'on utilise la commande <c>eigenvectors_right</c>, le troisième argument d'un triplet de réponse représente la multiplicité algébrique de la valeur propre. La multiplicité géométrique doit être déduite du nombre de vecteurs propres retournés.</p>
    <p>On termine avec des commandes Sage en lien avec la sous-section.</p>
    <computation>
    <title>Polynôme caractéristique et Sage</title>
    <statement><p>
    Bien que Sage soit capable directement de calculer les vecteurs et les valeurs propres, il peut aussi simplement donner le polynôme caractéristique d'une matrice <m>A</m>. Il suffit d'utiliser la commande <c>characteristic_polynomial()</c> ou son équivalent plus courte <c>charpoly()</c>. À noter que Sage utilise une version légèrement différente du polynôme caractéristique. Il considère plutôt le déterminant de <m>\lambda I-A</m>. Le polynôme retourné par Sage sera le même que celui de la définition <xref ref="def-polynomecaracteristique"/> lorsque <m>A</m> est une matrice de taille <m>n\times n</m> où <m>n</m> est paire et différera d'un facteur <m>-1</m> lorsque <m>n</m> est impair.
    </p>
    <sage>
    <input>
    A=matrix([[1,2],[3,4]])
    show(A.characteristic_polynomial())
    show((A-x*identity_matrix(2)).determinant().expand())
    B=matrix([[1,2,3],[4,5,6],[7,8,9]])
    show(B.characteristic_polynomial())
    show((B-x*identity_matrix(3)).determinant().expand())
    </input>
    </sage>
    <p>On peut ensuite demander à Sage de factoriser ce polynôme caractéristique, mais pour qu'il puisse accomplir le travail au complet, il faudra peut-être préciser l'espace sous-jacent aux entrées de la matrice. (Par défaut, Sage considère qu'une matrice dont toutes les entrées sont des entiers comme une matrice dans un espace d'entiers, et va factoriser en conséquence.)</p>
    <sage>
    <input>
     show(A.characteristic_polynomial().factor()) #Ne produit rien
     A=matrix(RR,[[1,2],[3,4]]) #On définit l'espace sous-jacent comme l'espace des nombres réels.
     show(A.characteristic_polynomial().factor())
    </input>
    </sage>
    <p>En terminant, un retour sur la commande <c>eigenvectors_right</c> en lien avec les multiplicités.</p>
    <sage>
    <input>
E=matrix([[1,1,0],[0,1,0],[0,0,2]])
show(E.charpoly().factor())
show(E.eigenvectors_right())
    </input>
    </sage>
    <p>Cette matrice correspond à la matrice <m>E</m> de l'exemple <xref ref="ex-vecvalpropres"/>, où l'on avait trouvé que la valeur propre <m>\lambda=1</m> n'avait qu'un vecteur propre. Elle apparait toutefois avec une multiplicité algébrique de <m>2</m>.</p>
    </statement>
    </computation>
    </subsection>
    <subsection>
    <title>Quelques résultats</title>
    <p>On obtient dans cette section quelques résultats théoriques en lien avec les valeurs et vecteurs propres. En particulier, on obtiendra la sixième version du théorème de la matrice inverse, en ajoutant une équivalence sur les valeurs propres. Que peut-on dire d'une matrice inversible et ses valeurs propres? Puisque les valeurs et vecteurs propres satisfont l'équation <m>A\vec{v}=\lambda \vec{v}</m> et qu'un vecteur propre ne peut pas être le vecteur nul, il y a quelque chose de particulier qui se passe si la valeur propre est nulle. Cela entraine qu'il y a des vecteurs non nul dans l'espace nul de la matrice. Une telle matrice ne peut donc pas être inversible.</p>
    <theorem xml:id="thm-delamatriceinversev6">
    <title>Théorème de la matrice inverse, sixième version</title>
    <statement><p>
<p>Soit <m>A</m> une matrice carrée d'ordre <m>n</m>. Les énoncés suivants sont équivalents:
    <ol>
    <li><p>La matrice <m>A</m> est inversible</p></li>
    <li><p>Pour chaque vecteur <m>\vec{v}\in \R^n</m>, il existe un seul vecteur <m>\vec{u}\in \R^n</m> tel que <m>A\vec{u}=\vec{v}</m>.</p></li>
    <li><p>Le rang de la matrice est égal à <m>n</m>.</p></li>
    <li><p>La matrice <m>A</m> possède <m>n</m> pivots.</p></li>
    <li><p>La forme échelonnée réduite de <m>A</m> est la matrice identité.</p></li>
    <li><p>Aucune ligne n'est une combinaison linéaire des autres lignes.</p></li>
    <li><p>Aucune colonne n'est une combinaison linéaire des autres colonnes.</p></li>
    <li><p>Le déterminant de la matrice <m>A</m> est non nul.</p></li>
    <li><p>L'espace colonne est de dimension <m>n</m>.</p></li>
    <li><p>L'espace ligne est de dimension <m>n</m>.</p></li>
    <li><p>L'espace nul est de dimension <m>0</m>.</p></li>
    <li><p>L'espace nul-gauche est de dimension <m>0</m>.</p></li>
    <li><p>Toutes les valeurs propres de <m>A</m> sont non nulles.</p></li>
    </ol>
    </p>    
    </p></statement>
    <proof><p>Il faut seulement démontrer l'équivalence entre le dernier énoncé et l'un des autres. Dans un premier temps, si <m>A</m> est inversible, alors son espace nul est de dimension <m>0</m>. Il n'existe donc pas d'autres vecteurs que <m>\vec{0}</m> tels que <m>A\vec{v}=\vec{0}</m>. Il découle de cela que <m>\lambda=0</m> ne peut être une valeur propre.</p>
    <p>À l'inverse, si <m>A</m> est une matrice dont toutes les valeurs propres sont différentes de zéro, cela implique qu'aucun vecteur non nul ne possède la propriété que <m>A\vec{v}=0\vec{v}=\vec{0}</m>. Donc, le seul élément dans l'espace nul de <m>A</m> est le vecteur nul, ce qui signifie que l'espace nul est de dimension <m>0</m>. Ainsi, une matrice est inversible si et seulement si aucune de ses valeurs propres n'est nulle.</p>
    </proof>
    </theorem>
    <p>Le prochain résultat sera utile dans la prochaine section. Il s'intéresse aux vecteurs propres et à leur indépendance.</p>
    <proposition xml:id="prop-propresindep">
    <title>Vecteurs propres et indépendance</title>
    <statement><p>
    <p>Soit <m>\lambda_1,\lambda_2,\ldots ,\lambda_k</m> des valeurs propres disctinctes d'une matrice <m>A</m> et <m>\vec{v}_1,\vec{v}_2,\ldots , \vec{v}_k</m> un vecteur propre associé à chacune de ces valeurs propres.
    Alors l'ensemble <m>\{\vec{v}_1,\vec{v}_2,\ldots , \vec{v}_k\}</m> est linéairement indépendant.</p>
    </p></statement>
    <proof>
    <p>Si <m>k=1</m>, il n'y a qu'un seul vecteur propre. Celui-ci étant non nul, il est par défaut indépendant. S'il y a plus de deux vecteurs, on procède par contradiction. On suppose que l'ensemble de ces vecteurs propres est dépendant. On peut alors affirmer qu'il existe un entier <m>p</m> tel que <m>\{\vec{v}_1,\vec{v}_2,\ldots , \vec{v}_p\}</m> est un ensemble indépendant, mais que <m>\{\vec{v}_1,\vec{v}_2,\ldots , \vec{v}_{p+1}\}</m> est dépendant. Bien entendu, <m>p\neq k</m> puisqu'on suppose que l'ensemble de tous ces vecteurs est dépendant. On cherche donc le premier vecteur qui, combiné aux précédents vecteurs indépendants, rend l'ensemble dépendant. Si <m>\vec{v}_1,\vec{v}_2,\ldots , \vec{v}_{p+1}</m> est dépendant, il existe alors une combinaison linéaire des <m>p</m> premiers vecteurs qui donne le vecteur <m>\vec{v}_{p+1}</m> :
    <men xml:id="eq-vecpropreindep" symbol="star">
    a_1\vec{v}_1+a_2\vec{v}_2+\ldots + a_p\vec{v}_p=\vec{v}_{p+1}
    </men>.
    On multiplie les deux côtés de cette équation par <m>A</m>. On obtient alors
    <md>
    <mrow>A(a_1\vec{v}_1+a_2\vec{v}_2+\ldots + a_p\vec{v}_p)&amp;=A\vec{v}_{p+1}</mrow>
    <mrow>Aa_1\vec{v}_1+Aa_2\vec{v}_2+\ldots + Aa_p\vec{v}_p=A\vec{v}_{p+1}</mrow>
    <mrow>a_1A\vec{v}_1+a_2A\vec{v}_2+\ldots + a_pA\vec{v}_p=A\vec{v}_{p+1}</mrow>
    <mrow>a_1\lambda_1\vec{v}_1+a_2\lambda_2\vec{v}_2+\ldots + a_p\lambda_p\vec{v}_p=\lambda_{p+1}\vec{v}_{p+1}</mrow>
    </md>.
    D'un autre côté, on multiplie l'équation <xref ref="eq-vecpropreindep"/> par <m>\lambda_{p+1}</m> pour obtenir
    <me>
        a_1\lambda_{p+1}\vec{v}_1+a_2\lambda_{p+1}\vec{v}_2+\ldots + a_p\lambda_{p+1}\vec{v}_p=\lambda_{p+1}\vec{v}_{p+1}
    </me>.
    On a alors
    <me>
    a_1\lambda_{p+1}\vec{v}_1+a_2\lambda_{p+1}\vec{v}_2+\ldots + a_p\lambda_{p+1}\vec{v}_p=a_1\lambda_1\vec{v}_1+a_2\lambda_2\vec{v}_2+\ldots + a_p\lambda_p\vec{v}_p
    </me>
    qui se réécrit sous la forme
    <me>
    a_1(\lambda_1-\lambda_{p+1})\vec{v}_1+a_2(\lambda_2-\lambda_{p+1})\vec{v}_2+\ldots + a_p(\lambda_p-\lambda_{p+1})\vec{v}_p=\vec{0}
    </me>.
    Comme les vecteurs <m>\{\vec{v}_1,\vec{v}_2,\ldots , \vec{v}_p\}</m> sont linéairement indépedants, il faut que les coefficients de cette combinaison linéaire soient nuls, mais on a également que les valeurs propres sont disctinctes. Cela entraine que <m>a_1=a_2=\cdots =a_p=0</m>. Par contre, si cela était vrai, l'équation <xref ref="eq-vecpropreindep"/> signifierait alors que <m>\vec{v}_{p+1}=\vec{0}</m>, ce qui est impossible. Il n'est donc pas possible de trouver une valeur de <m>p</m> telle que l'ensemble <m>\{\vec{v}_1,\vec{v}_2,\ldots , \vec{v}_p\}</m> est indépendant, mais l'ensemble <m>\{\vec{v}_1,\vec{v}_2,\ldots , \vec{v}_{p+1}\}</m> est dépendant. On conclut donc que <m>\{\vec{v}_1,\vec{v}_2,\ldots , \vec{v}_k\}</m> est forcément indépendant, ce qui conclut la preuve.
    </p>
    </proof>
    </proposition>
    <p>Le prochain résultat est en lien avec les valeurs propres d'une matrice <m>A</m> et celles de matrices obtenues à partir d'opérations algébriques et matricielles comme la multiplication par un scalaire ou l'inverse de <m>A</m>. </p>
    <proposition xml:id="prop-effetvalpropre">
    <title>L'effet de certaines opérations sur les valeurs propres</title>
    <statement><p>Soit <m>A</m> une matrice carrée et <m>\lambda</m> une valeur propre. Alors
    <ol>
    <li><p>le scalaire <m>r\lambda</m> est une valeur propre de la matrice <m>rA</m>;</p></li>
    <li><p>le scalaire <m>\lambda^k</m> est une valeur propre de la matrice <m>A^k</m>;</p></li>
    <li><p>lorsque <m>A</m> est inversible, <m>\frac{1}{\lambda}</m> est une valeur propre de la matrice <m>A^{-1}</m>;</p></li>
    <li><p>la valeur propre <m>\lambda</m> est aussi une valeur propre de transposée <m>A^T</m>.</p></li>
    </ol>
    </p></statement>
    <proof><p>Soit <m>\vec{v}</m> un vecteur propre associé à <m>\lambda</m> pour la matrice <m>A</m>. On a
    <md>
    <mrow>(rA)\vec{v}&amp;=r(A\vec{v}) &amp;&amp; \text{associativité du produit scalaire matrice }</mrow>
    <mrow>&amp;=r\lambda \vec{v}</mrow>
    </md>.
    Ainsi, puisque <m>(rA)\vec{v}=(r\lambda)\vec{v}</m>, une valeur propre de <m>rA</m> est <m>r\lambda</m>. De plus, le vecteur <m>\vec{v}</m> demeure un vecteur propre.</p></proof>
    <proof><p>Voir l'exercice <xref ref="exo-effetvalpropre"/></p></proof>
    <proof><p>Si <m>A</m> est une matrice inversible, alors selon le <xref ref="thm-delamatriceinversev6" text="title"/>, la valeur propre n'est pas nulle. Soit <m>\vec{v}</m> un vecteur propre associé à <m>\lambda</m>. On peut alors écrire
    <md>
    <mrow>A^{-1}\vec{v}&amp;= A^{-1}\frac{\lambda \vec{v}}{\lambda}</mrow>
    <mrow>&amp;=A^{-1}\frac{A\vec{v}}{\lambda} &amp;&amp; \text{ car } A\vec{v}=\lambda\vec{v}</mrow>
    <mrow>&amp;=A^{-1}A \frac{1}{\lambda}\vec{v}</mrow>
    <mrow>&amp;=\frac{1}{\lambda}\vec{v}</mrow>
    </md>.
    Le scalaire <m>\frac{1}{\lambda}</m> est donc bel et bien une valeur propre de <m>A^{-1}</m>.
    </p></proof>
    <proof><p>Voir l'exercice <xref ref="exo-effetvalpropre"/></p></proof>
    </proposition>
    <p>Il n'était pas question du produit de deux matrices dans la proposition précédente. Qu'en est-il des valeurs propres d'une matrice <m>AB</m>? À première vue, on pourrait penser que si <m>\alpha</m> était une valeur propre de <m>A</m> et <m>\beta</m> une valeur propre de <m>B</m>, alors <m>\alpha \beta</m> serait une valeur propre du produit puisque
    <me>
    AB\vec{v}=A\beta \vec{v}=\beta A\vec{v}=\beta \alpha \vec{v}
    </me>,
    mais il y a un problème avec ce raisonnement. Rien ne garantit que <m>A</m> et <m>B</m> ont les mêmes vecteurs propres. Si c'est toutefois le cas, c'est-à-dire si <m>\vec{v}</m> est un vecteur propre de <m>A</m> associé à la valeur propre <m>\alpha</m> et un vecteur propre de <m>B</m> associé à la valeur propre <m>\beta</m>, alors <m>\vec{v}</m> est un vecteur propre de <m>AB</m> associé à la valeur propre <m> \alpha \beta</m>. En fait on a même davantage, puisque <m>\vec{v}</m> est aussi un vecteur propre de la matrice <m>BA</m>, aussi associé à la valeur propre <m>\alpha \beta</m>. La question du partage des vecteurs propres entre deux matrices sera étudié plus spécifiquement à la prochaine section.</p>
    
    <p>Un type de matrice pour lequel il est facile de trouver les valeurs propres est lorsque la matrice est <xref ref="def-mattriang" text="custom">triangulaire</xref>. Dans ce cas, les valeurs propres se trouvent toujours sur la diagonale.</p>
    <proposition xml:id="prop-valpropretriangulaire">
    <title>Les valeurs propres de matrices triangulaires</title>
    <statement><p>Soit <m>L</m> une matrice triangulaire inférieure et <m>U</m> une matrice triangulaire supérieure. Alors les valeurs propres de <m>L,U</m> sont sur leur diagonale respective.</p></statement>
    <proof>
    <p>C'est une conséquence directe du fait que les matrices <m>L-\lambda I</m> et <m>U-\lambda I</m> sont encore triangulaire et de la proposition <xref ref="prop-detmattriang"/> qui stipule que le déterminant d'une matrice triangulaire est le produits des entrées sur la diagonale principale. Le polynôme caractéristique est alors déjà factorisé et s'annule précisemment aux entrées de la diagonale de <m>L</m> ou <m>U</m>.</p>
    </proof>
    </proposition>
    <!--  FAUX Tel Quel, il faut une hypothèse additionnelle pour la direction <=, soit que A et B soit diagonalisable simultanément
    <proposition>
    <title>Deux matrices aux mêmes vecteurs propres</title>
    <statement><p>Soit <m>A,B</m> deux matrices carrées de format <m>n\times n</m>. Alors <m>A</m> et <m>B</m> ont  <m>n</m> vecteurs propres indépendants identiques si et seulement si <m>AB=BA</m>.</p></statement>
    <proof><p>
    On montre dans un premier temps que si <m>A,B</m> se partagent <m>n</m> vecteurs propres indépendants <m>\vec{v}_1,\vec{v}_2,\ldots ,\vec{v}_n</m>, alors <m>AB=BA</m>. Puisqu'on a <m>n</m> vecteurs propres indépendants de <m>\mahtbb{R}^n</m>, ceux-ci forment une base de l'espace. Pour tout vecteur <m>\vec{u}\in \mathbb{R}^n</m>, il existe donc des coefficients tels que
    <me>
    \vec{u}=a_1\vec{v}_1+a_2\vec{v}_2+\cdots +a_n\vec{v}_n
    </me>.
    </p>
    <p>On a alors
    <md><mrow>BA\vec{u}&amp;=BA(a_1\vec{v}_1+a_2\vec{v}_2+\cdots +a_n\vec{v}_n)</mrow>
    <mrow>&amp;=B(Aa_1\vec{v}_1+Aa_2\vec{v}_2+\cdots +Aa_n\vec{v}_n)</mrow>
    <mrow>&amp;=B(a_1A\vec{v}_1+a_2A\vec{v}_2+\cdots +a_nA\vec{v}_n)</mrow>
    <mrow>&amp;=B(a_1\alpha_1\vec{v}_1+a_2\alpha_2\vec{v}_2+\cdots +a_n\alpha_n\vec{v}_n)&amp;&amp; \text{ car chaque vecteur est un vecteur propre de } A</mrow>
    <mrow>&amp;=Ba_1\alpha_1\vec{v}_1+Ba_2\alpha_2\vec{v}_2+\cdots +Ba_n\alpha_n\vec{v}_n</mrow>
    <mrow>&amp;=a_1\alpha_1B\vec{v}_1+a_2\alpha_2B\vec{v}_2+\cdots +a_n\alpha_nB\vec{v}_n</mrow>
    <mrow>&amp;=a_1\alpha_1\beta_1\vec{v}_1+a_2\alpha_2\beta_2\vec{v}_2+\cdots +a_n\alpha_n\beta_n\vec{v}_n &amp;&amp; \text{ car chaque vecteur est un vecteur propre de } B</mrow>
    <mrow>&amp;=a_1\beta_1\alpha_1\vec{v}_1+a_2\beta_2\alpha_2\vec{v}_2+\cdots +a_n\beta_n\alpha_n\vec{v}_n</mrow>
    <mrow>&amp;=a_1\beta_1A\vec{v}_1+a_2\beta_2A\vec{v}_2+\cdots +a_n\beta_nA\vec{v}_n</mrow>
    <mrow>&amp;=A(a_1\beta_1\vec{v}_1+a_2\beta_2\vec{v}_2+\cdots +a_n\beta_n\vec{v}_n)</mrow>
    <mrow>&amp;=A(a_1B\vec{v}_1+a_2B\vec{v}_2+\cdots +a_nB\vec{v}_n)</mrow>
    <mrow>&amp;=AB(a_1\vec{v}_1+a_2\vec{v}_2+\cdots +a_n\vec{v}_n)</mrow>
    <mrow>&amp;=AB\vec{u}</mrow>
    </md>.
    Comme pour tout <m>\vec{u}</m> on a <m>AB\vec{u}=BA\vec{u}</m>, on conclut que les deux produits sont égaux et que les matrices commutent.
    </p>
    <p>À l'inverse, si on a deux matrices <m>A,B</m> telles que <m>AB=BA</m></p>
    </proof>
    </proposition>
    -->
    </subsection>
    <conclusion xml:id="concl-vecvalpropres">  <!-- Ajouter le même identifiant de la section après le - du xml:id -->
    <p>Les points importants de cette section sont:
    <ul>
    <li><p>La définition des <xref ref="def-propres" text="custom">vecteurs et valeurs propres</xref>;</p></li>
    <li><p>La définition du <xref ref="def-polynomecaracteristique" text="custom">polynôme caractéristique</xref>.</p></li>
    <li><p>Le fait que les <xref ref="prop-valproprepoly" text="custom">zéros du polynôme caractéristique </xref> sont les valeurs propres;</p></li>
    <li><p>Les deux <xref ref="def-multalggeo" text="custom">multiplicité</xref> associées à une valeur propre;</p></li>
    <li><p>Le fait qu'une matrice est inversible si et seulement si aucune de ses valeurs propres est zéro, s'ajoutant au <xref ref="thm-delamatriceinversev6" text="custom">théorème de la matrice inverse</xref>;</p></li>
    <li><p>Les <xref ref="prop-effetvalpropre" text="custom">propriétés</xref> des valeurs propres en lien avec différentes opérations algébriques.</p></li>
    </ul></p>
    <p>De plus, avec Sage, on a vu les commandes <c>eigenvalues</c>, <c>eigenvectors_right</c> et <c>characteristic_polynomial</c> (ou <c>charpoly</c>) permettant respectivement de déterminer les valeurs propres, vecteurs propres et le polynôme caractéristique d'une matrice. </p>
    </conclusion>
   <!--Inclure les exercices de la section ci-dessous--> 
   <xi:include href="Exercices_propres.xml"/>
</section>

<!-- geogebra pour vecteur valeur propre de cisaillement -->
<!-- Ajouter exercice dans section cofactaeur sur degré determinant A-lambda I --> 