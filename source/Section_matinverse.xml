<?xml version="1.0" encoding="UTF-8"?>

<!-- Ce fichier constitue une section du livre                              -->
<!--                                                                        -->
<!--      Algèbre linéaire : Intuition et rigueur                           -->
<!--                                                                        -->
<!-- Copyright (C) 2019  Jean-Sébastien Turcotte, Philémon Turcotte         -->
<!-- Licence à venir                                                        -->

<!-- Les sections sont divisées en quatre parties, en plus du titre. Les parties introduction et conclusion sont facultatives. Le texte de ceux-ci apparait respectivement avant et après les sections. Les exercices sont à la fin de la section -->

<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="sec-matinverse">   <!-- Ajouter l'identifiant de la section après le - du xml:id -->
    <title> Transformations inverses </title>
    <introduction xml:id= "intro-matinverse">  <!-- Ajouter le même identifiant de la section après le - du xml:id -->
    <p>Soit <m>T_1</m> une transformation linéaire, de <m>\R^2</m> vers <m>\R^2</m>. Est-il possible de trouver une transformation <m>T_2</m> telle que <m>T_2(T_1)=I</m>? En d'autres mots, est-il possible d'annuler
    l'effet de <m>T_1</m> sur les vecteurs de <m>\R^2</m> par une transformation? Une sorte d'inverse, au même sens où pour défaire une multiplication par <m>5</m> on peut diviser par <m>5</m>. Pourrait-on ainsi,
    définir la division de transformations linéaires, et par le fait même, de matrices?</p>
    <p>Est-il toujours possible de trouver cette transformation inverse? Dans <m>\R^2</m>? Qu'en est-il de la situation plus générale d'une transformation de <m>\R^n</m> vers <m>\R^m</m>?</p>
    <p>Dans cette section, on se concentre sur l'inverse d'une transformation de <m>\R^2</m> vers <m>\R^2</m>. On verra les conditions d'existence de l'inverse, de même que certaines propriétés. Quelques-une de ces propriétés se 
    généraliseront éventuellement au cas général, mais celui-ci demandera une attention particulière.</p>
    </introduction>
    <subsection xml:id="sssec-invgeo">
    <title>L'inverse de transformations géométriques</title>
    <p>On débute cette section par une partie intuitive, déterminer l'inverse des transformations de nature géométrique simple, celles de la liste <xref ref="liste-transfor2"/>. Avant, une définition de ce que représente l'inverse d'une transformation.</p>
    <definition xml:id="def-transfoinverseprelim">
    <title> La transformation inverse, définition préliminaire</title>
    <statement><p>Soit <m>T_1</m> une transformation linéaire. La transformation inverse de <m>T_1</m>, notée <m>T_1^{-1}</m>, est une transformation telle que
    <men xml:id="eq-invprelim">
    T_1^{-1}(T_1(\vec{u}))=\vec{u}
    </men> pour tout <m>\vec{u}</m> dans le domaine de <m>T_1</m>.</p></statement>
    </definition>
    <remark xml:id="rem-invprelim">
    <title>Une précision sur l'inverse</title>
    <p>Dans la définition, il n'est pas mentionné que la transformation inverse est une transformation linéaire. Comme on peut interpréter l'équation <xref ref="eq-invprelim"/> de manière matricielle, on cherche l'existence
    d'une matrice <m>T_1^{-1}</m> telle que <m>T_1^{-1}T_1\vec{u}=\vec{u}</m> pour tout vecteur <m>\vec{u}</m> dans le domaine de <m>T_1</m>, c'est-à-dire une transformation telle que
    <me>
    T_1^{-1}T_1=I
    </me>.
    En vertu de la proposition <xref ref="prop-matsonttransfos"/>, si une telle matrice existe, alors <m>T_1^{-1}</m> est une transformation linéaire.</p>
    </remark>
    <example xml:id="ex-transfor2inv">
    <title>L'inverse de transformations géométriques</title>
    <statement><p>On considère les transformations de la liste <xref ref="liste-transfor2"/> et on cherche à les inverser, si possible.</p></statement>
    <solution>
    <p>La transformation identité n'a aucun effet sur les vecteurs de <m>\R^2</m>. Ainsi, si on la compose avec elle-même, on restera avec l'identité. L'inverse de <m>I</m> est donc <m>I</m>.</p>
    </solution>
    <solution>
    <p>Afin de défaire une réflexion par rapport à l'axe des <m>x</m>, il semble suffisant d'appliquer à nouveau la réflexion. On aurait donc <m>S_x^{-1}=S_x</m>. On vérifie algébriquement que c'est le cas.
    <md>
    <mrow>S_xS_x&amp;=\begin{pmatrix}
    1 &amp;0 \\ 0 &amp; -1
    \end{pmatrix}\begin{pmatrix}
    1 &amp;0 \\ 0 &amp; -1
    \end{pmatrix}</mrow>
    <mrow>&amp;=\begin{pmatrix} \lvert &amp; \lvert \\
                                \begin{pmatrix}
    1 &amp;0 \\ 0 &amp; -1
    \end{pmatrix}\vecd{1}{0} &amp; \begin{pmatrix}
    1 &amp;0 \\ 0 &amp; -1
    \end{pmatrix}\vecd{0}{-1}\\
     \lvert &amp; \lvert
    \end{pmatrix} &amp;&amp; \text{ selon la définition de la multiplication matricielle}</mrow>
<mrow> \begin{pmatrix}
1&amp;0\\ 0 &amp;1
\end{pmatrix}
</mrow>
</md>.
    L'inverse de cette réflexion est donc en effet la réflexion même.</p>
    </solution>
    <solution>
    <p>Pour défaire une rotation de <m>90^{\circ}</m>, il semble logique de faire une rotation de <m>-90^{\circ}</m>, correspondant en fait à une rotation dans le sens horaire. Selon l'équation <xref ref="eq-rotr2"/>,
    cette matrice serait <m>R_{-\frac{\pi}{2}}=R_{\frac{\pi}{2}}^{-1}=\begin{pmatrix}0 &amp; 1\\ -1 &amp; 0\end{pmatrix}</m>. Algébriquement, on a
    <md>
    <mrow>R_{-\frac{\pi}{2}}R_{\frac{\pi}{2}}&amp;=\begin{pmatrix}0 &amp; 1\\ -1 &amp; 0\end{pmatrix}\begin{pmatrix}0 &amp; -1\\ 1 &amp; 0\end{pmatrix}</mrow>
    <mrow>&amp;=\begin{pmatrix}(0,1)\cdot (0,1) &amp; (0,1)\cdot (1,0)\\ (-1,0)\cdot (0,1) &amp; (-1,0)\cdot(-1,0)\end{pmatrix} &amp;&amp; \text{ en vertu de la multiplication composante par composante}</mrow>
    <mrow>&amp;=\begin{pmatrix}1 &amp; 0\\ 0 &amp; 1\end{pmatrix}</mrow>
    </md>.
    La transformation inverse de la rotation de <m>90^{\circ}</m> est donc bel et bien une rotation dans le sens opposé. Il est intéressant de remarquer que <m>R_{-\frac{\pi}{2}}=R_{\frac{\pi}{2}}^{-1}</m> et qu'on a aussi
    <m>R_{\frac{\pi}{2}}=R_{\frac{-\pi}{2}}^{-1}</m>. On vérifiera éventuellement si cela se produit toujours.
    </p>
    </solution>
    <solution>
    <p>Un étirement horizontal multiplie la première composante d'un vecteur par <m>r</m>. Pour défaire cette transformation, on devrait diviser cette composante par <m>r</m>, ce qui donnerait un étirement de facteur
    <m>\frac{1}{r}</m>. Ici se présente un premier problème dans la recherche d'inverse. En effet, si <m>r=0</m>, il est impossible de diviser par <m>0</m> et donc l'inverse, s'il existe, ne serait pas un étirement 
    de facteur <m>\frac{1}{r}</m>. On débute par considérer le cas <m>r\neq 0</m> avant de réfléchir à ce que représente un étirement de facteur <m>0</m> et son inverse. Ainsi, si <m>r\neq 0</m>, on a
    <md>
    <mrow> Eh_{1/r}Eh_{r}&amp;=\begin{pmatrix} \frac{1}{r} &amp; 0\\ 0&amp; 1\end{pmatrix}\begin{pmatrix} r &amp; 0\\ 0&amp; 1\end{pmatrix}</mrow>
    <mrow>&amp;\begin{pmatrix} 1 &amp; 0\\ 0&amp; 1\end{pmatrix} &amp;&amp; \text{on arrive à ceci avec l'une ou l'autre des interprétations du produit matriciel.}</mrow>
    </md>.
    Ainsi, <m>Eh_r^{-1}=Eh_{\frac{1}{r}}</m> si <m>r\neq 0</m>.
    </p>
    <p>On considère maintenant le cas <m>r=0</m>. La matrice de l'étirement est 
    <me>
    Eh_0=\begin{pmatrix} 0&amp; 0\\ 0 &amp;1\end{pmatrix}
    </me>. 
    On prétend qu'il ne peut exister d'inverse pour cette transformation. En effet, soit <m>A</m> la matrice représentant l'inverse. Selon la définition <xref ref="def-transfoinverseprelim"/>, il faut que <m>AEh_0=I</m>. 
    En particulier, il faudrait que <m>A\vecd{0}{0}=\vecd{1}{0}</m> selon la définition du <xref ref="def-matmatprod">produit matriciel</xref>, puisque <m>\vecd{0}{0}</m> est la première colonne de <m>Eh_0</m> et <m>\vecd{1}{0}</m> 
    la première colonne de la matrice identité. Selon la proposition <xref ref="prop-transfolinprop"/>, l'image du vecteur nul sera toujours le vecteur nul et donc, il est impossible d'avoir <m>\vecd{1}{0}</m> comme résultat.</p>
    <p>Il est cependant possible que l'inverse existe, mais ne soit pas une transformation linéaire. Quand on précisera la définition <xref ref="def-transfoinverseprelim"/>, cette posibilité sera invalidée.</p>
    </solution>
    <solution>
    <p>La solution pour l'inverse de <m>Ev_r</m> est donnée à l'exercice <xref provisional="exo-Evrinv"/>.</p>
    </solution>
    <solution>
    <p>Il est évident que si les composantes d'un vecteurs ont été permutées, cette même permutation redonnera le vecteur initial. Ainsi, <m>P^{-1}=P</m>. La vérification algébrique est laissée au lecteur.</p>
    </solution>
    <solution>
    <p>On est forcé une fois de plus à réfléchir à la signification de l'inverse d'une transformation avec le cas de la projection orthogonale. Si on pense aux fonctions réelles traditionnelles et à leur inverse, quelque chose de commun ressort.
    L'inverse de la fonction <m>y=ax+b</m> est <m>y=\frac{(x-b)}{a}</m>, pour autant que <m>a\neq 0</m>. La fonction <m>f(x)=e^x</m> a pour inverse le logarithme naturel, <m>g(x)=\ln(x)</m>. Dans chacun de ces cas, on remarque
    que pour chaque <m>x</m> dans le domaine, il correspond un seul <m>y</m> dans l'image tel que <m>f(x)=y</m>. Si on regarde la fonction <m>f(x)=x^2</m>, celle-ci ne respecte pas cette condition. Par exemple, <m>f(-2)=f(2)=4</m>.
    </p>
    <p>Lorsqu'on veut inverser la fonction <m>x^2</m>, on parle souvent de la racine carrée. Cet inverse ne fonctionne toutefois que pour des valeurs de <m>x\geq 0</m> (Un choix qui a été fait de considérer cette branche plutôt que l'autre).
    De même, pour inverser la fonction <m>\sin(x)</m>, on ne considère que les valeurs de <m>x\in\left[-\frac{\pi}{2},\frac{\pi}{2}\right]</m>. Sur chacune de ces branches, il n'existe qu'une seule paire de nombres <m>x,y</m> 
    tels que <m>f(x)=y</m>. Ainsi, on peut inverser la fonction sans problème.</p>
    <p>On revient à la projection. Pour un vecteur <m>\vec{v}</m> parallèle à <m>\vec{w}</m>, il existe une infinité de vecteurs <m>\vec{u}</m> tels que <m>\text{prof}_{\vec{w}}(\vec{u})=\vec{v} </m>. Comment choisir
    l'inverse de <m>\vec{v}</m> dans ce cas, parmi l'infinité de possibilités? On préfère ne pas choisir dans ce cas et dire que la projection orthogonale ne possède pas d'inverse.</p>
    <p>En appronfondissant notre intuition géométrique dans les prochaines sections, on comprendra davantage la raison de l'inexistance de l'inverse de la projection. On peut s'imaginer en quelque sorte qu'il y a une 
    perte d'information lorsque la projection est appliquée et qu'il n'est pas possible de revenir en arrière.</p>
    </solution>
    </example>
    <p>Le dernier exemple est riche en intuition géométrique et en questionnement. On a pu trouver des inverses sans faire de calculs, sauf pour vérifier que l'intuition était bonne. On a également constaté un cas où lorsque 
    <m>B</m> est l'inverse de <m>A</m>, <m>A</m> est aussi l'inverse de <m>B</m>. Ce constat, vrai en général pour les matrices carrées, n'est pas aussi évident que cela puisse le paraitre. Également, on a réalisé que 
    l'inverse d'une matrice n'existe pas toujours. On verra bientôt des premiers critères géométriques et algébriques nécessaires pour avoir  l'existence d'une transformation inverse.</p>
    </subsection>
    <subsection xml:id="sssec-inv2x2">
    <title>L'inverse d'une transformation linéaire du plan</title>
    <p>Dans cette sous-section, on cherche à établir une formule, mais surtout des critères pour déterminer si une transformation possède une transformation inverse. Avant, on donne un exemple qui motive la recherche
    de l'inverse.</p>
    <p>On sait que les colonnes de la matrice d'une transformation linéaire correspondent aux images des vecteurs <m>(1,0)</m> et <m>(0,1)</m> (dans <m>\R^2</m>, le cas général étant semblable, voir la proposition <xref ref="prop-matsonttransfos"/> et
    le texte qui la suit). Si par contre on connait l'effet d'une transformation <m>T</m>, non pas sur ces vecteurs, mais sur deux vecteurs quelconques. <fn>Il y a une condition à respecter, mais on ne veut pas trop en dire pour le moment.</fn></p>
    <p>Concrètement, si <m>T(\vec{u}_1)=\vec{v}_1</m> et <m>T(\vec{u}_2)=\vec{v}_2</m>, alors selon la définition de la multiplication, on peut écrire
    <me>
    T\begin{pmatrix} \lvert &amp; \lvert \\ \vec{u}_1 &amp; \vec{u}_2 \\ \lvert &amp; \lvert \end{pmatrix}=\begin{pmatrix} \lvert &amp; \lvert \\ \vec{v}_1 &amp; \vec{v}_2 \\ \lvert &amp; \lvert \end{pmatrix}
    </me>.
    Pour simplifier, on écrit <m>TU=V</m>. Si on était capable d'inverser la matrice <m>U</m>, on pourrait isoler la matrice de la transformation linéaire <m>T</m>: 
    <md>
    <mrow>TU&amp;=V</mrow>
    <mrow>TUU^{-1}&amp;=VU^{-1} &amp; &amp; \text{multiplication à droite de part et d'autre par } U^{-1}</mrow>
    <mrow number="yes" xml:id="eq-UUm1">TI&amp;=VU^{-1} &amp;&amp; \text{ car } UU^{-1}=I \text{ par définition de l'inverse}</mrow>
    <mrow number="yes" xml:id="eq-matparvecteursuv"> T&amp;=VU^{-1}</mrow>
    </md>.</p>
    <p>Techniquement, l'équation <xref ref="eq-UUm1"/> suppose que l'inverse <m>U^{-1}</m> sera le même à droite (la définition <xref ref="def-transfoinverseprelim"/> parle seulement d'inverse à gauche). La définition 
    <xref provisional="def-transfoinverse"/> qui suivra permettra de lever l'ambiguité. On accepte l'incohérence temporaire puisque le but n'est que de donner un exemple de l'utilisation de la matrice inverse.</p>
    <p>On débute l'exploration de l'inverse d'une matrice quelconque par le calcul d'un tel inverse pour une matrice qui n'a pas d'interprétation géométrique claire.</p>
    <example xml:id="ex-matinverse1">
    <title>Un premier calcul de matrice inverse</title>
    <statement>
    <p>On considère la matrice <m>A=\begin{pmatrix} 1&amp; 1\\ 4&amp; 5 \end{pmatrix}</m>. On cherche l'inverse de cette transformation, sous hypothèse que l'inverse existe.</p>
    </statement>
    <solution>
    <p>On cherche une matrice <m>A^{-1}</m> telle que <m>A^{-1}\vecd{1}{4}=\vecd{1}{0}</m> et <m>A^{-1}\vecd{1}{5}=\vecd{0}{1}</m>. Si on pose <m>A^{-1}=\begin{pmatrix} x&amp;z\\ y&amp;w \end{pmatrix}</m>, on obtient une 
    paire de systèmes à deux équations deux inconnues. D'une part,
    <md>
    <mrow>A^{-1}\vecd{1}{4}&amp;=\vecd{1}{0} </mrow>
    <mrow xml:id="eq-matinverse1-1">\vecd{x+4z}{y+4w}&amp;=\vecd{1}{0} </mrow>
    </md>
     et d'autre part
      <md>
    <mrow>A^{-1}\vecd{1}{5}&amp;=\vecd{0}{2} </mrow>
    <mrow xml:id="eq-matinverse1-2">\vecd{x+5z}{y+5w}&amp;=\vecd{0}{1} </mrow>
    </md>.
    </p>
   <p> En prenant la première composante des équations <xref ref="eq-matinverse1-1"/> et <xref ref="eq-matinverse1-2"/>, qui ne contiennent que les variables <m>x</m>
     et <m>z</m>, on obtient
     <md>
     <mrow>x+4z&amp;=1 </mrow>
     <mrow>x+5z&amp;=0</mrow>
     </md>.
     La seconde de ces équations permet d'obtenir <m>x=-5z</m>, puis en substituant dans la première on détermine <m>z=-1</m>. On a alors <m>x=5</m>.
     </p>
      <p> En prenant la deuxième composante des équations <xref ref="eq-matinverse1-1"/> et <xref ref="eq-matinverse1-2"/>, qui ne contiennent que les variables <m>y</m>
     et <m>w</m>, on obtient
     <md>
     <mrow>y+4w&amp;=0 </mrow>
     <mrow>y+5w&amp;=1</mrow>
     </md>.
     La première de ces équations permet d'obtenir <m>y=-4z</m>, puis en substituant dans la seconde on détermine <m>w=1</m>. On a alors <m>y=-4</m>.
     </p>
     <p>Ainsi, la matrice inverse doit être <m>A^{-1}=\begin{pmatrix} 5 &amp; -1 \\ -4 &amp; 1 \end{pmatrix}</m>. Une vérification ne fait jamais de tort:
     <md>
     <mrow>A^{-1}A&amp;=\begin{pmatrix} 5 &amp; -1 \\ -4 &amp; 1 \end{pmatrix}\begin{pmatrix} 1 &amp; 1 \\ 4 &amp; 5 \end{pmatrix}</mrow>
     <mrow>&amp;=\begin{pmatrix} (5,-1)\cdot (1,4) &amp; (5,-1)\cdot (1,5) \\ (-4,1)\cdot (1,4) &amp; (-4,1)\cdot (1,5) \end{pmatrix}</mrow>
     <mrow>&amp;=\begin{pmatrix} 1&amp; 0\\ 0&amp;1\end{pmatrix}</mrow>
     <mrow>&amp;=I</mrow>
     </md>.
     </p>
    </solution>
    </example>
    <p>On considère maintenant une matrice <m>A=\begin{pmatrix} a&amp; c\\ b &amp; d \end{pmatrix}</m> quelconque et on cherche l'inverse. En se basant sur les calculs faits à
    l'exemple <xref ref="ex-matinverse1"/>, on peut retrouver deux système d'équations analogues. D'abord,
    <md>
   <mrow> A^{-1}\vecd{a}{b}&amp;= \vecd{1}{0}</mrow>
   <mrow> \vecd{ax+bz}{ay+bw}&amp;= \vecd{1}{0}</mrow>
    </md>
    et ensuite
    <md>
   <mrow> A^{-1}\vecd{c}{d}&amp;= \vecd{0}{1}</mrow>
   <mrow> \vecd{cx+dz}{cy+dw}&amp;= \vecd{0}{1}</mrow>
    </md>.
    </p>
    <p>Rappelons ici que <m>a,b,c,d</m> sont les coefficients de la matrice <m>A</m> et qu'ils sont en général connus. On cherche donc à résoudre ces systèmes en fonction
    des inconnues <m>x,y,z,w</m>. En couplant les premières composantes ensemble et les secondes composantes ensemble, on obtient deux systèmes à deux équations et deux inconnues.
    Le premier de ces systèmes est
    <md>
    <mrow>ax+bz&amp;=1 </mrow>
    <mrow> cx+dz&amp;=0</mrow>
    </md>
    et le second
    <md>
    <mrow>ay+bw&amp;=0 </mrow>
    <mrow> cy+dw&amp;=1</mrow>
    </md>.
    </p>
    <p>Comme on veut éviter de restreindre le plus possible les valeurs de <m>a,b,c,d</m>, on ne peut pas isoler <m>x</m> ou <m>z</m> dans la seconde équation du premier système.
    Cela supposerait que <m>c\neq 0</m> ou <m>d\neq 0</m>. Pour contourner ce problème, on multiplie la première équation par <m>c</m> et la seconde par <m>a</m>. On obtient
    <mdn>
    <mrow xml:id="eq-matinverse2x2-1">acx+bcz&amp;=c </mrow>
    <mrow xml:id="eq-matinverse2x2-2"> acx+adz&amp;=0</mrow>
    </mdn>.
    En soustrayant l'équation <xref ref="eq-matinverse2x2-1"/> de l'équation <xref ref="eq-matinverse2x2-2"/>, on obtient
    <me>
    (ad-bc)z=-c
    </me>.
    On ne peut plus éviter la division et donc, en faisant la supposition que <m>ad-bc\neq 0</m>, on obtient <m>z=\frac{-c}{ad-bc}</m>. 
    </p>
    <p>En multipliant maintenant la première équation du premier système par <m>d</m> et la seconde par <m>b</m>, on obtient
    <mdn>
    <mrow xml:id="eq-matinverse2x2-3">adx+bdz&amp;=d </mrow>
    <mrow xml:id="eq-matinverse2x2-4"> abx+bdz&amp;=0</mrow>
    </mdn>.
    En soustrayant  cette fois l'équation <xref ref="eq-matinverse2x2-4"/> de l'équation <xref ref="eq-matinverse2x2-3"/>, on obtient
    <me>
    (ad-bc)x=d
    </me>.
    La supposition que <m>ad-bc\neq 0</m> ayant déjà été faite, on trouve <m>x=\frac{d}{ad-bc}</m>.
    </p>
    <p>Dans l'exercice <xref provisional="exercice deuxieme système matrice inverse"/>, il est montré que <m>y=\frac{-b}{ad-bc}</m> et <m>w=\frac{a}{ad-bc}</m>.</p>
    <p>En sortant le facteur <m>ad-bc</m> commun à chacun des termes, on obtient finalement la matrice inverse 
    <men xml:id="eq-matinverse2x2">
    \frac{1}{ad-bc}\begin{pmatrix} d \amp -b \\ -c \amp a \end{pmatrix}
    </men>.</p>
    <example xml:id="ex-matinverse2">
    <title>Calcul de l'inverse avec la formule</title>
    <p>On reprend la matrice de l'exemple <xref ref="ex-matinverse1"/> et on détermine son inverse avec la formule <xref ref="eq-matinverse2x2"/>. On obtient
    <md>
    <mrow>A^{-1}&amp;=\frac{1}{1*5-4*1}\begin{pmatrix} 5 \amp -1 \\ -4 \amp 1 \end{pmatrix} </mrow>
    <mrow>&amp;=\begin{pmatrix} 5 \amp -1 \\ -4 \amp 1 \end{pmatrix}</mrow>
    </md>,
    qui correspond ave ce qui a été obtenu plus tôt.
    </p>
    </example>
    <p>Il semble qu'un critère pour déterminer si une matrice (ou une transformation) est inversible soit que <m>ad-bc\neq 0</m>. On réfléchit maintenant à ce que
    cela signifie si <m>ad-bc=0</m> et pourquoi une transformation ayant cette propriété ne peut être inversible.</p>
    <p>
    Dans un premier temps, si<m>ad-bc=0</m>, alors <m>ad=bc</m>. On a aussi 
    <md>
    <mrow> c(a,b)&amp;=(ac,bc)</mrow>
    <mrow>&amp;=(ac,ad)</mrow>
    <mrow>&amp;=a(c,d)</mrow>
    </md>.
    Les vecteurs <m>(a,b)</m> et <m>(c,d)</m> semblent donc parallèles. En fait, il faut distinguer certains cas problématiques, ce que l'on fera un peu plus tard. D'abord, on montre que des vecteurs parallèles sont suffisants
    pour faire en sorte qu'une matrice <m>2\times 2</m> ne soit pas inversible.
    </p>
    <p>En effet, si <m>(a,b)=k(c,d)</m>, alors 
    <md><mrow> a&amp;=kc</mrow>
    <mrow>ad&amp;=kcd</mrow>
    <mrow>ad&amp;=kdc</mrow>
    <mrow>ad&amp;=bc</mrow>
    <mrow>ad-bc&amp;=0</mrow>
    </md>.
    Le seul problème avec l'argument est que si <m>(a,b)=(0,0)</m> ou <m>(c,d)=(0,0)</m>, les vecteurs ne sont pas considérés comme parallèles (voir la définition <xref ref="def-vecpara"/> et la note de bas de page l'accompagnant).
    Or de manière évidente, si <m>(a,b)=(0,0)</m> ou <m>(c,d)=(0,0)</m> (ou les deux), alors <m>ad-bc=0</m>. Dans tous les cas, la situation géométrique est analogue à une projection. La transformation associée à une matrice 
    <m>2\times 2</m> telle que <m>ad-bc=0</m> envoie le plan <m>\R^2</m> sur une droite (ou un point si <m>a=b=c=d=0</m>). Tel que mentionné à l'exemple <xref ref="ex-transfor2inv"/>, il y a perte d'information qui fait qu'on
    ne peut défaire la transformation.
    </p>
    <p>On termine avec des commandes Sage en lien avec la sous-section.</p>
    <computation xml:id="sageex-inv2x2">
    <title>Les matrices inverses sur Sage</title>
    <p>Sur Sage, on peut calculer facilement l'inverse d'une matrice <m>2\times 2</m> <m> A</m> en utilisant la commande <c>A.inverse()</c>. On pourra bien entendu vérifer avec la multiplication matricielle
    que le calcul est bon.</p>
    <sage>
    <input>
A=column_matrix([[2,4],[-2,1]])
Ainv=A.inverse()
show("A^{-1}=",Ainv)
show("A^{-1}A=",Ainv*A)    
    </input>
    </sage>
    <p>Si l'inverse n'existe pas, Sage retourne une erreur, comme le montre le code suivant</p>
    <sage>
A=column_matrix([[2,4],[-2,-4]])
Ainv=A.inverse()    
    </sage>
    <p>On fait maintenant un exemple en référence à l'équation <xref ref="eq-matparvecteursuv"/>, qui stipule que si on connait l'effet d'une transformation <m>T</m> sur deux vecteurs, alors on peut déterminer 
    la matrice associée à la transformation sans calculer directement l'image des vecteurs <m>(1,0)</m> et <m>(0,1)</m> (pour le cas <m>2\times 2</m>).</p>
    <p>On considère donc une transformation linéaire <m>T</m> telle que <m>T(2,4)=(-3,1)</m> et <m>T(1,-4)=(-2,-5)</m>. On cherche la matrice représentant <m>T</m>. Avec Sage et l'équation <xref ref="eq-matparvecteursuv"/>, on a</p>
    <sage>
    <input>
   U=column_matrix([[2,4],[1,-4]])
V=column_matrix([[-3,1],[-2,-5]])
Uinv=U.inverse()
T=V*Uinv
show("T=",T) 
    </input>
    </sage>
    <p>Afin de vérifier, on peut calculer les images des vecteurs <m>(2,4)</m> et <m>(1,-4)</m>.</p>
    <sage>
    <input>
u1=U.column(0)
u2=U.column(1)
show("Tu1=",T*u1)
show("Tu2=",T*u2)    
    </input>
    </sage>
    <p>Évidemment, on aurait pu calculer <m>TU</m> et vérifier que l'on obtient <m>V</m>, mais on s'est permis ici de rappeler la commande <c>U.column()</c> qui permet d'accéder aux colonnes d'une matrice.
    Il est pratique de travailler de cette manière plutôt que de par exemple, aller définir manuellement <c>u1=vector([2,4])</c>. L'idée est que si, pour une raison quelconque, on décidait de changer la matrice <m>U</m>,
    il suffirait de changer seulement la ligne où <m>U</m> est définie. Ceci est résumé dans le conseil <xref ref="con-defefficace"/> ci-dessous.
    </p>
    </computation>
    <insight xml:id="con-defefficace">
    <title>Utilisation efficace de l'informatique</title>
    <p>L'un des  but d'une utilisation de l'informatique est de lui déléguer certains calculs, afin de se concentrer davantage sur les concepts. Il faut toutefois être efficace dans son utilisation afin de 
    bénéficier de sa flexibilité et de toute sa puissance. On illustre avec un exemple.</p>
    <p>Soit <m>A=\begin{pmatrix} 1\amp 3\\ -2\amp 6 \end{pmatrix}</m>. On cherche à calculer la quantité <m>ad-bc</m> pour cette matrice. Si ce nombre n'est pas zéro, on veut calculer <m>A^{-1}\vecd{2}{3}</m>. Voici une première option.</p>
    <sage>
    <input>
    #On vérifie si ad-bc=0
    a=1
    b=-2
    c=3
    d=6
    show(a*d-b*c)
    #On définit la matrice
    A=column_matrix([[1,-2],[3,6]])
    #On inverse la matrice
    Ainv=A.inverse()
    #On calcule l'inverse de (2,3)
    u=vector([2,3])
    show(Ainv*u)
    </input>
    </sage>
    <p>Simple, la séquence d'instruction ressemble à ce qui serait fait sur une feille de papier, l'ordre en fonction des questions demandées. Si maintenant on demandait les mêmes questions, mais pour la matrice <m>B=\begin{pmatrix} -1\amp 2\\ 4\amp -3 \end{pmatrix}</m>.
    On s'imagine alors recopier le code et corriger les lignes <c>2-5</c> et la ligne <c>8</c>. 
    </p>
    <p>Voici une deuxième option pour répondre aux questions initiales demandées.</p>
    <sage>
    <input>
    #On définit la matrice et le vecteur 
    A=column_matrix([[1,-2],[3,6]])
    u=vector([2,3])
    #On vérifie si ad-bc=0
    a=A[0][0]
    b=A[1][0]
    c=A[0][1]
    d=A[1][1]
    show(a*d-b*c)
    #on calcule l'inverse de A et l'image inverse du vecteur (2,3)
    Ainv=A.inverse()
    show(Ainv*u)
    </input>
    </sage>
    <p>Maintenant, pour répondre aux questions avec la matrice <m>B</m>, seulement la ligne <c>1</c> doit être éditée.</p>
    </insight>
    </subsection>
    <subsection xml:id="sssec-matinv">
    <title>Définition formelle de l'inverse d'une matrice carrée</title>
    <p>Dans cette sous-section, on étudie les propriétés de la matrice inverse. On précise également la définition de l'inverse. Bien que pour le moment, on ne connaisse que l'inverse d'une transformation de 
    <m>\R^2</m> vers <m>\R^2</m>, les propriétés étudiées ici sont valides pour toute matrice <em>carrée</em> possédant un inverse. On verra dans le chapitre <xref provisional="Chapitre SEL"/> comment obtenir l'inverse
    d'une matrice carrée de taille plus grande que <m>2</m>.</p>
    <p>La première étape de notre démarche est de valider l'hypothèse que si <m>A^{-1}A=I</m>, alors <m>AA^{-1}=I</m> également. Cela parait sans doute plus simple que ce ne l'est vraiment. En effet,
    rien ne garantit à priori que si <m>A</m> possède un inverse, alors cet inverse possède lui-même un inverse.</p>
    <p>Il est par contre assez intuitif que l'inverse de l'inverse de <m>A</m> devrait être <m>A</m>. Les prochains résultats visent à mettre un peu d'ordre et de rigueur derrière l'intuition. </p>
    <remark xml:id="rem-transfodomim">
    <title>Le domaine et l'image d'une transformation linéaire</title>
    <p>Les prochains résultats parlent du domaine et de l'image d'une transformation linéaire. On rappelle que selon la proposition <xref ref="prop-matsonttransfos"/>, une matrice <m>m\times n</m> est une transformation
     linéaire de <m>\R^n</m> vers <m>\R^m</m>. Le chapitre <xref provisional="chap-SEL"/> explore en profondeur domaine, image et zéros d'une transformation linéaire.</p>
     <p> Pour ce qui suit, on s'intéresse seulement aux matrices carrées <m>n\times n</m>.</p>
    </remark>
    <p>D'abord, on cherche à démontrer que si <m>A</m> est une matrice pour laquelle il existe <m>A^{-1}</m> telle que <m>A^{-1}A=I</m>, alors il existe également une matrice <m>B</m> telle que <m>AB=I</m>. En d'autres mots,
    si <m>A</m> possède un inverse selon la définiton <xref ref="def-transfoinverseprelim"/> et la remarque <xref ref="rem-invprelim"/>, alors <m>A</m> est aussi l'inverse d'une matrice. </p>
    <p>Par la suite, on montre que <m>B=A^{-1}</m>.</p>
    <proposition xml:id="prop-investsurj">
    <title>Les transformations inversibles atteignent tous les vecteurs de leur image</title>
    <statement>
    <p>Si <m>A</m> est une matrice possédant un inverse au sens de la définition <xref ref="def-transfoinverseprelim"/> et de la remarque <xref ref="rem-invprelim"/>, alors pour tout <m>\vec{v}\in\R^n</m>, il existe <m>\vec{u}\in\R^n</m> tel que <m>A\vec{u}=\vec{v}</m>.
    Cela signifie que chaque vecteur de <m>\R^n</m> (l'image) est atteint par la transformation d'un (au moins) vecteur de <m>\R^n</m> (domaine). </p>
    </statement>
    <proof>
    <p>Soit <m>\vec{v}\in \R^n</m> un vecteur de l'image de <m>A</m>. On cherche <m>\vec{u}</m> tel que <m>A\vec{u}=\vec{v}</m>. Puisque <m>A</m> possède un inverse <m>A^{-1}</m>, on a
    <md>
    <mrow>A\vec{u}&amp;=\vec{v}</mrow>
    <mrow>A^{-1}A\vec{u}&amp;=A^{-1}\vec{v} &amp;&amp; \text{ En multipliant à droite de chaque côté par } A^{-1}\text{. Le côté droit de l'égalité est bien défini, ca l'inverse est une transformation dont le domaine est } \R^n\text{.}</mrow>
    <mrow>I\vec{u}&amp;=A^{-1}\vec{v} &amp;&amp; \text{ car } A^{-1} \text{ est l'inverse de } A\text{.}</mrow>
    <mrow>\vec{u}&amp;=A^{-1}\vec{v} &amp;&amp; \text{par <xref ref="def-matid">définition</xref> de la transformation identité.}</mrow>
    </md>.
    </p>
    <p>Ainsi, il existe <m>\vec{u}</m> tel que <m>A\vec{u}=\vec{v}</m> et on peut explicitement le calculer à l'aide de l'inverse.</p>
    </proof>
    </proposition>
    <proposition xml:id="prop-surjestinv">
    <title>Les transformations atteignant chaque vecteur de leur image sont l'inverse d'une transformation</title>
    <statement><p>Soit <m>A</m> une matrice telle que pour tout <m>\vec{v}\in \R^n</m>, il existe <m>\vec{u}\in\R^n</m> telle que <m>A\vec{u}=\vec{v}</m>, alors <m>A</m> est l'inverse d'une matrice <m>B</m>, au
    sens de la définition <xref ref="def-transfoinverseprelim"/> et de la remarque <xref ref="rem-invprelim"/>. Cela signifie qu'il existe une matrice <m>B</m> telle que <m>AB=I</m>.</p></statement>
    <proof>
    <p>Soit <m>\vec{e}_1=(1,0,\cdots , 0),\vec{e}_2=(0,1,0,\ldots , 0),\ldots, \vec{e}_n=(0,\ldots ,0 ,1)</m> les vecteurs colonnes de la matrice identité. Par hypothèse sur <m>A</m>, il existe des vecteurs
    <m>\vec{b}_1,\vec{b}_2,\ldots , \vec{b}_n</m> tels que <m>A\vec{b}_k=\vec{e}_k</m> pour chaque <m>k=1,\ldots , n</m>.</p>
    <p>On pose 
    <me>
    B=\begin{pmatrix}
    \lvert \amp \lvert \amp \cdots \amp \lvert \\
     \vec{b}_1\amp \vec{b}_2\amp \cdots \amp \vec{b}_n \\
    \lvert \amp \lvert \amp \cdots \amp \lvert \\
    \end{pmatrix}
    </me>.
    On a alors <m>AB=I</m> puisque la colonne <m>k</m> de <m>AB</m> est donnée par <m>A\vec{b_k}=\vec{e}_k</m>.
    </p>
    </proof>
    </proposition>
    <aside>
    <title> En passant</title>
    <p>Une fonction <m>f</m> d'un ensemble <m>X</m> vers un ensemble <m>Y</m> est dite surjective (en anglais on dit parfois "onto") si pour chaque <m>y\in Y</m> il existe au moins un élément <m>x\in X</m> tel que <m>f(x)=y</m>. Cela signifie que chaque valeur
    de l'image est atteinte par au moins une valeur du domaine.
    </p>
    <p>
    La proposition <xref ref="prop-investsurj"/> montre donc qu'une transformation linéaire inversible est surjective alors que
    la proposition <xref ref="prop-surjestinv"/> montre elle qu'une transformation surjective est inversible.</p>
    <p>Lorsqu'une fonction est telle que si <m>f(x)=f(y)</m>, alors <m>x=y</m>, on dit qu'elle est injective (en anglais on dit parfois "one-to-one"). Cela signifie que chaque valeur atteinte dans <m>Y</m> ne l'est que par un et un seul <m>x</m>.</p>
    <p>Une fonction qui est à la fois injective et surjective est dite bijective.</p>
    </aside>
    <p>On peut finalement montrer, en utilisant les deux propositions précédentes, que l'inverse d'une matrice fonctionne des deux côtés. Cela va permettre de préciser 
    la définition <xref ref="def-transfoinverseprelim"/>.</p>
    <theorem xml:id="thm-invgauchedroite">
    <title>L'inverse d'une matrice carrée est bilatéral</title>
    <statement>
    <p>Soit <m>A</m> une matrice carrée pour laquelle il existe <m>A^{-1}</m> telle que <m>A^{-1}A=I</m>. Alors <m>AA^{-1}=I</m>.</p>
    </statement>
    <proof>
    <p>Puisque <m>A</m> possède un inverse au sens de la définition <xref ref="def-transfoinverseprelim"/>, la proposition <xref ref="prop-investsurj"/> dit que chaque
    valeur de  son image est atteinte par une valeur de son domaine.</p>
    <p>De plus, la proposition <xref ref="prop-surjestinv"/> affirme qu'il existe une matrice <m>B</m> telle que <m>AB=I</m>, c'est-à-dire que <m>A</m> est l'inverse d'une
    matrice <m>B</m> au sens de la définition <xref ref="def-transfoinverseprelim"/>. On a alors
    <md>
    <mrow> B&amp;= A^{-1}AB &amp;&amp; \text{car } A^{-1}A=I</mrow>
    <mrow>&amp;= A^{-1}(AB) &amp;&amp; \text{ par associativité de la multiplication matricielle}</mrow>
    <mrow>&amp;=A^{-1} &amp;&amp; \text{ car } AB=I</mrow>
    </md>.
    Ainsi l'inverse de l'inverse de <m>A</m> est <m>A</m>. On dira aussi que l'inverse à gauche de <m>A</m> est le même que son inverse à droite.</p>
    </proof>
    </theorem>
    <p>Ce résultat, peut-être pas très surprenant, est néanmoins primordial dans l'étude de l'algèbre linéaire. D'autant plus qu'on sait qu'en général, le produit matriciel
    ne commute pas. Lorsqu'il est question de l'inverse toutefois, c'est le cas. La définition suivante raffine la définition <xref ref="def-transfoinverseprelim"/>. Elle
    servira de référence à partir de maintenant pour l'inverse d'une transformation linéaire de <m>\R^n</m> vers <m>\R^n</m> ou de façon équivalente, d'une matrice carrée.</p>
    <definition xml:id="def-matcarreeinverse">
    <title>L'inverse d'une matrice carrée</title>
    <p>Soit <m>A</m> une matrice carrée. On dit que <m>A</m> est inversible s'il existe une matrice <m>A^{-1}</m> telle que 
    <me>
    A^{-1}A=AA^{-1}=I
    </me>.</p>
    <p>En termes d'une transformation linéaire, on dit que la transformation <m>T</m> de <m>\R^n</m> vers <m>\R^n</m> est inversible s'il existe une transformation
    linéaire <m>T^{-1}</m> de <m>\R^n</m> vers <m>\R^n</m> telle que 
    <me>
    T^{-1}(T(\vec{u}))=T(T^{-1}(\vec{u}))=\vec{u}
    </me>
    pour tout vecteur <m>\vec{u}\in\R^n</m>.</p>
    </definition>
    <remark xml:id="rem-obtenirinverse">
    <title>Obtenir l'inverse d'une matrice</title>
    <p>Ensemble, les propositions <xref ref="prop-investsurj"/> et <xref ref="prop-surjestinv"/>, combinées à la définition <xref ref="def-matcarreeinverse"/> donnent un critère pour déterminer si une matrice carrée <m>A</m> possède un inverse. Il faut que
    l'équation <m>A\vec{u}=\vec{v}</m> possède une solution pour chaque <m>\vec{v}\in \R^n</m>. L'exercice <xref provisional="exo-vecteursesuffisants"/> montre qu'en fait,
    il est suffisant d'avoir une solution pour les vecteurs
    <md>
    <mrow>\vec{e}_1&amp;=  (1,0,\ldots ,0)</mrow>
    <mrow>\vec{e}_2&amp;=  (0,1,0,\ldots,0) </mrow>
    <mrow>\vdots\&amp;= vdots  </mrow>
    <mrow>\vec{e}_n&amp;=  (0,\ldots ,0,1) </mrow>
    </md>.
    </p>
    <p>De plus, la proposition <xref ref="prop-surjestinv"/> donne une manière explicite pour obtenir l'inverse. Pour chaque vecteur <m>\vec{e}_k</m>, on cherche un
    vecteur <m>\vec{b}_k</m> tel que <m>A\vec{b}_k=\vec{e}_k</m>. La matrice inverse est alors
    <me>
    A^{-1}=\begin{pmatrix}
    \lvert \amp \lvert \amp \cdots \amp \lvert \\
     \vec{b}_1\amp \vec{b}_2\amp \cdots \amp \vec{b}_n \\
    \lvert \amp \lvert \amp \cdots \amp \lvert \\
    \end{pmatrix}</me>.
    </p>
    </remark>
    <p>On utilise cette méthode pour calculer à nouveau l'inverse de la matrice de l'exemple <xref ref="ex-matinverse1"/>.</p>
    <example xml:id="ex-matinverse3">
    <title>Calcul d'une matrice inverse</title>
    <statement><p>
    On considère la matrice <m>A=\begin{pmatrix} 1\amp 1 \\ 4\amp 5 \end{pmatrix}</m> de l'exemple <xref ref="ex-matinverse1"/> et on calcule son inverse à l'aide de la 
    méthode de la proposition <xref ref="prop-surjestinv"/> et détaillée à la remarque <xref ref="rem-obtenirinverse"/>.
    </p>
    <p>Il est intéressant de s'attarder à la différence entre la méthode explicitée à l'exemple <xref ref="ex-matinverse1"/> et dans le paragraphe qui suit cet exemple et la 
    méthode effectuée ci-dessous basée sur la remarque <xref ref="rem-obtenirinverse"/>. </p>
    <p>Évidemment, pour une matrice <m>2\times 2</m>, la formule <xref ref="eq-matinverse2x2"/> permet d'obtenir l'inverse sans faire beaucoup de calculs, mais on verra
    qu'avec une matrice de taille plus grande, il est plus difficile de trouver une formule générale et il est plus simple de rechercher les vecteurs colonnes de l'inverse.</p>
    </statement>
    <solution>
    <p>On cherche un vecteur <m>\vec{b}_1=\vecd{b_{1,1}{b_{2,1}}</m> tel que <m>A\vec{b}_1=\vecd{1}{0}</m> et un vecteur <m>\vec{b}_2=\vecd{b_{1,2}{b_{2,2}}</m> tel que <m>A\vec{b}_2=\vecd{0}{1}</m>.
    On a
    <md>
   <mrow> A\vec{b}_1&amp;=\vecd{1}{0}</mrow>
   <mrow number="yes" xml:id="eq-matinverse3-1-1">\begin{pmatrix} 1\amp 1 \\ 4\amp 5 \end{pmatrix}\vecd{b_{1,1}{b_{2,1}}&amp;=\vecd{1}{0}</mrow>
   <mrow number="yes" xml:id="eq-matinverse3-1-2">\vecd{b_{1,1}+b_{2,1}}{4b_{1,1}+5b_{2,1}}&amp;=\vecd{1}{0}</mrow>
    </md>
    et
    <md>
   <mrow> A\vec{b}_2&amp;=\vecd{0}{1}</mrow>
   <mrow>\begin{pmatrix} 1\amp 1 \\ 4\amp 5 \end{pmatrix}\vecd{b_{1,2}{b_{2,2}}&amp;=\vecd{0}{1}</mrow>
   <mrow>\vecd{b_{1,2}+b_{2,2}}{4b_{1,2}+5b_{2,2}}&amp;=\vecd{0}{1}</mrow>
    </md>.
    </p><p>
    Pour le premier système, on trouve en comparant les deuxièmes composantes des vecteurs de l'équation <xref ref="eq-matinverse3-1-2"/> que <m>b_{2,1}=-\frac{4}{5}b_{1,1}</m>.
    En remplaçant dans l'équation <xref ref="eq-matinverse3-1-1"/>, on trouve <m>b_{1,1}-\frac{4}{5}b_{1,1}=1</m>, ce qui donne <m>b_{1,1}=5</m>. On déduit alors <m>b_{2,1}=-4</m>
    </p>
    <p>Pour le second système, en suivant une méthode similaire, on trouver <m>b_{1,2}=-1</m> et <m>b_{2,2}=1</m>. Cela correspond évidemment à ce qui avait été trouvé
    aux exemples <xref ref="ex-matinverse1"/> et <xref ref="ex-matinverse2"/>.</p>
    </solution>
    </example>
    <p>Dans le chapitre <xref provisional="chap-SEL"/>, on aura une méthode efficace pour trouver les vecteurs colonnes de l'inverse d'une matrice carrée de taille <m>n</m>. Pour le moment,
    on utilisera Sage ou on donnera simplement l'inverse en cas de besoin.</p>
    <p>On termine avec un exemple Sage en lien avec la sous-section.</p>
    <computation xml:id="sageex-matinv">
    <title>Retour sur les matrices inverses sur Sage</title>
    <p>Armé de la définition <xref ref="def-matcarreeinverse"/>, on regarde à nouveau des calculs de matrices inverses sur Sage. En particulier, on compare on multiplie à gauche
    et à droite par l'inverse afin de vérifier l'égalité avec la matrice identité.</p>
    <sage>
    <input>
A=column_matrix([[1,2,3],[-1,3,2],[-3,2,1]])
Ainv=A.inverse()
show("AA^(-1)=",A*Ainv)
show("A^(-1)A=",Ainv*A)
    </input>
    </sage>
    <p>Il peut être utile aussi de savoir que pour calculer l'inverse, on peut utiliser <c>A^(-1)</c>, qui se rapproche davantage de la notation mathématique usuelle.</p>
    <sage>
    <input>
    show("A^(-1)=",A^(-1))
    </input>
    </sage>
    </computation>
    </subsection>
    <subsection xml:id="sssec-propinv">
    <title>Propriétés de la matrice inverse</title>
    <p>Dans cette sous-section, on s'intéresse aux propriétés de la matrice inverse. En particulier, on énonce une première version du <xref provisional="thm-delamatriceinverse">théorème de la matrice inverse</xref>,
    qui donne plusieurs critères équivalents pour déterminer si une matrice carrée possède un inverse.</p>
     <p>Depuis le début de cette section, on parle de l'inverse d'une matrice. On ne sait toutefois jamais posé la question à savoir si cet inverse est unique. On a déjà fait 
     une remarque similaire dans la section <xref ref="sec-droitesplans"/> au sujet des vecteurs directeurs des droites et des plans. Ceux-ci ne sont pas uniques. Dans le cas
     d'une matrice inverse par contre, l'unicité se vérifie.</p>
     <proposition xml:id="prop-inverseunique">
     <title>L'inverse d'une matrice carrée est unique</title>
     <statement><p>Soit <m>A</m> une matrice carrée et <m>B,C</m> deux matrices telles que <m>BA=I</m> et <m>CA=I</m>. Alors <m>B=C</m>.</p></statement>
     <proof>
     <p>L'égalité suit des propriétés de la multiplication matricielle et du fait que si <m>CA=I</m>, alors <m>AC=I</m> (théorème <xref ref="thm-invgauchedroite"/>).
     <md>
     <mrow>B&amp;=BAC &amp;&amp; \text{ car } AC=I</mrow>
     <mrow>&amp;=(BA)C&amp;&amp; \text{ associativité de la multiplication matricielle}</mrow>
     <mrow>&amp;=C &amp;&amp; \text{car} BA=I</mrow>
     </md>.
     </p>
     </proof>
     </proposition>
     <p>En plus d'être unique, l'inverse possède la propriété importante suivante.</p>
     <proposition xml:id="prop-inverseproduit">
     <title>L'inverse d'un produit</title>
     <statement><p>Soit <m>A</m> et <m>B</m> deux matrices carrées d'ordre <m>n</m> qui sont inversibles. Alors le produit <m>AB</m> est inversible et son inverse est <m>B^{-1}A^{-1}</m>.</p></statement>
     <proof>
     <p>Avant de s'embarquer dans des manipulations algébriques abstraites, on réfléchit un instant sur la signification de la proposition. Si deux transformations sont inversibles et 
     qu'on les compose, c'est-à-dire qu'on les fait l'une après l'autre, alors il n'y a pas de raison apparente qui ferait en sorte que cette composition ne soit pas inversible.
     De plus, il semble que pour revenir sur le vecteur original, il suffit de faire les transformations inverses dans l'ordre inverses. Il semble donc assez intuitif que
     <me>
     (AB)^{-1}=B^{-1}A^{-1}
     </me>
     D'un point de vue algébrique, on vérifie facilement que <m>(B^{-1}A^{-1})(AB)=B^{-1}A^{-1}AB=B^{-1}IB=B^{-1}B=I</m>. Il ne reste donc qu'à vérifier l'existence de l'inverse.
     </p>
     <p>Soit <m>\vec{v}\in \R^n</m> un vecteur de l'image de <m>AB</m>. On cherche <m>\vec{u}</m> telle que <m>AB\vec{u}=\vec{v}</m>. En vertu de la proposition 
     <xref ref="prop-surjestinv"/>, si <m>\vec{u}</m> existe, la matrice <m>AB</m> est inversible. On construit <m>\vec{u}</m> en inversant successivement les matrices <m>A</m>
     et <m>B</m>. Tel qu'attendu
     <md>
     <mrow>AB\vec{u}&amp;=\vec{v}</mrow>
     <mrow>B\vec{u}&amp;=A^{-1}\vec{v}</mrow>
     <mrow>\vec{u}&amp;=B^{-1}A^{-1}\vec{v}</mrow>
     </md>.
     Comme les matrices <m>A^{-1}</m> et <m>B^{-1}</m> existent, le vecteur <m>\vec{u}</m> existe. Ainsi, la matrice <m>AB</m> possède toujours un inverse.
     </p>
     </proof>
     </proposition>
     <aside>
     <title>En passant</title>
     <p>Parfois, la proposition <xref ref="prop-inverseproduit"/> est appelé le théorème du bas et du soulier. L'analogie va comme suit:</p>
     <p>Lorsqu'on se lève le matin pour quitter la maison, on commence par mettre des bas à ses pieds, pour ensuite mettre des souliers. Le soir venu, on enlève d'abord
     les souliers, puis ensuite les bas. L'inverse de mettre ses bas puis ses souliers est donc enlever ses souliers puis ses bas:
     <me>
     (\text{bas}\circ \text{souliers})^{-1}=\text{soulier}^{-1}\circ\text{bas}^{-1}
     </me>.</p>
     </aside>
     <p>Parfois, une discussion sur les propriétés en est aussi une sur les "non propriétés". Par exemple, l'inverse de la somme de deux matrices n'est pas la somme des
     inverses de ces matrices. En fait, il est même possible que la somme de deux matrices inversibles ne soit pas inversible (contrairement au produit). Pour un exemple simple,
     on prend une matrice inversible <m>A</m> et on considère la matrice <m>A+(-A)</m>. On obtient la matrice nulle, qui n'est pas inversible. En effet, si <m>\vec{v}\neq \vec{0}</m> est un 
     vecteur de l'image de <m>A-A</m>, aucun vecteur <m>\vec{u}</m> n'est envoyé sur <m>\vec{v}</m>, car <m>(A-A)\vec{u}=\vec{0}</m>. En vertu de la proposition <xref ref="prop-investsurj"/>,
     la matrice nulle ne peut être inversible.</p>
     <p>Pour la multiplication par un scalaire, on a toutefois la propriété suivante, si <m>k\neq 0</m>:
     <me>
     (kA)^{-1}=\frac{1}{k}A^{-1}
     </me>.</p>
    </subsection>
    <subsection xml:id="sssec-algmat">
    <title>Algèbre matricielle</title>
   <p>Dans cette section, on illustre l'utilisation de l'algèbre matricielle, qui diffère de l'algèbre usuelle, principalement par le fait que la multiplication n'est
    pas commutatif.</p>
    <example xml:id="ex-algmat1">
    <title>Algèbre matricielle</title>
    <statement><p>Soit <m>A,B,C</m> et <m>X</m> des matrices. On cherche à isoler <m>X</m> dans l'équation
    <me>
    AX+B=C
    </me>.</p>
    <p>Quelles sont les conditions sur les matrices <m>A,B,C</m> pour qu'il soit possible d'isoler <m>X</m>.</p></statement> 
    <solution>
    <p>On isole <m>X</m>, sans se soucier des conditions sur les matrices. On les déterminera après avoir effectué les opérations algébriques. On a donc
    <md>
    <mrow> AX+B&amp;=C</mrow>
    <mrow>AX&amp;=C-B</mrow>
    <mrow>X&amp;=A^{-1}(C-B)</mrow>
    </md>.
    </p>
    <p>Il faut donc que <m>B</m> et <m>C</m> soit de même format, puisqu'on les soustrait. De plus, on utilise l'inverse de <m>A</m>. Il faut donc que <m>A</m> soit une matrice carrée.
    <fn>Techniquement, ce n'est pas vrai, <m>A</m> pourrait posséder un inverse à droite si elle est rectangulaire. Voir la section <xref provisional="sec-matmn"/>.</fn>
    Si <m>A</m> est une matrice <m>n\times n</m>, il faut que <m>C-B</m> ait <m>n</m> lignes. Sans obligation sur le nombre de colonnes, on peut supposer qu'il y en a <m>m</m>.
    Les matrices <m>B,C</m> sont donc de format <m>n\times m</m>.
    </p>
    <p>Le produit <m>A^{-1}(C-B)</m> sera une matrice <m>n\times m</m> et donc, le format de <m>X</m> sera <m>n\times m</m>. C'est également compatible avec l'équation initial
    puisque <m>AX</m> est de format <m>n\times m</m> et qu'on additionne à ce produit la matrice <m>n\times m</m> B pour avoir la matrice <m>C</m>, aussi <m>n\times m</m>. </p>
    </solution>
    </example>
    <p>Dans tous les exemples qui suivent, on suppose que les matrices ont un format approprié pour que les opérations soient définies et que les matrices qui ont à
    être inversée sont carrées. L'exercice <xref provisional="exo-algmatpropex"/> servira à déterminer dans chaque cas les conditions sur les matrices.
    </p>
    <example xml:id="ex-algmat2">
    <title>Algèbre matricielle, deuxième partie</title>
    <statement>
    <p>Pour chaque équation ci-dessous, isoler la matrice <m>X</m>, en supposant que les formats sont appropriés et que la matrices ayant à être inversée sont inversible.
    <ol>
    <li><m>XA+B=C</m></li>
    <li><m>AX+B=X</m></li>
    <li><m>AXB=D</m></li>
    <li><m>(A+X)B+(B+X)A=I</m></li>
    <li><m>AX^{-1}+B=C</m></li>
    </ol>
    </p>
    </statement>
    <solution>
    <p>Similairement à l'exemple <xref ref="ex-algmat1"/>, on a
    <md>
    <mrow> XA+B&amp;=C</mrow>
    <mrow>XA&amp;=C-B</mrow>
    <mrow>X&amp;=(C-B)A^{-1}</mrow>
    </md>.
    </p>
    </solution>
    <solution>
    <p>On commence par regrouper les termes ayant la matrice <m>X</m> d'un côté. On obtient alors
    <md>
    <mrow> AX-X&amp;=-B</mrow>
    <mrow>(A-I)X&amp;=-B</mrow>
    <mrow>X&amp;=(A-I)^{-1}(-B)</mrow>
    <mrow>X&amp;=-(A-I)^{-1}B</mrow>
    </md>.
    </p>
    </solution>
    <solution>
    <p>On a
    <md>
    <mrow>AXB&amp;=D</mrow>
    <mrow>AX&amp;=DB^{-1}</mrow>
    <mrow>X&amp;=A^{-1}DB^{-1}</mrow>
    </md>.
    </p>
    </solution>
    <solution>
    <p>On commence par poser <m>Y=X^{-1}</m>. L'expression devient <m>AY+B=C</m>, équivalente à celle de l'exemple <xref ref="ex-algmat1"/>. On a donc
    <me>
    Y=A^{-1}(C-B)
    </me>.
    Puisque <m>X=Y^{-1}=(X^{-1})^{-1}</m>, on a
    <md>
    <mrow>X&amp;=\left(A^{-1}(C-B)\right)^{-1}</mrow>
    <mrow>&amp;=(C-B)^{-1}(A^{-1})^{-1}</mrow>
    <mrow>&amp;=(C-B)^{-1}A</mrow>
    </md>.
    </p>
    </solution>
    <solution>
    <p>On commence par développer l'expression au carré, en portant une attention particulière aux termes <m>AB</m> et <m>BA</m> (voir l'exercice <xref provisional="AplusBaucarré"/>). On a donc
    <md>
    <mrow>A^2+AX+XA</mrow>
    </md>
    </p>
    </solution>
    </example>
    <p>TERMINER avec vérification sage des réponses? Style plug X= dans l'équation et vérifier bool si égalité? </p>
    <computation xml:id="sageex-algmat">
    <title>L'algèbre matricielle avec Sage</title>
    <p>Parfois, il est pratique de travailler avec des matrices génériques, de manière purement algébrique, sans se soucier des entrées spécifiques des matrices. 
    Malheureusement, Sage ne permet pas (encore) l'utilisation de variables pour des matrices comme on l'a fait dans les exemples <xref first="ex-algmat1" last="ex-algmat2"/>.
    On fait un compromis en se créant une matrice arbitraire de taille spécifique. Les calculs faits ainsi ne pourront donc pas êtres considérés comme des preuves des
    identités, mais au moins une vérification plus générale qu'un exemple spécifique.
    </p>
    <p>On doit dans un premier temps créer une matrice dont les entrées seront arbitraires, par exemple
    <m>A=\begin{pmatrix} a_{1,1} &amp; a_{1,2} &amp; a_{1,3} \\ 
    a_{2,1} &amp; a_{2,2} &amp; a_{2,3} \\
    a_{3,1} &amp; a_{3,2} &amp; a_{3,3} 
    \end{pmatrix}</m>.</p>
    <p>Pour cela, on se sert du constructeur de matrices à partir d'une liste. Par exemple, pour créer une matrice dont les entrées seraient les nombres de <m>1</m> à <m>9</m>,
    en ligne, on peut utiliser la suite d'instructions suivante.</p>
    <sage>
    <input>
    liste=[i+1 for i in range(9)]   #range(9) va de 0 à 8, donc on fait 1+1
    show(liste)
    A=matrix(3,3,liste)    #3,3 est la taille de la matrice
    show(A)
    </input>
    </sage>
    <p>On veut donc créer une fonction Sage qui, étant donné des dimensions <m>m,n</m> et un paramètre alphabétique, retournera une matrice de taille <m>m\times n</m> dont les entrées
    seront dénotées par la lettre choisie indicée de la position, comme à l'équation <xref ref="eq-matindice"/>. Voici une manière d'y arriver, avec commentaire dans le code.</p>
    <sage>
    <input>
    def matquelc(m,n,lettre):
    a=str(lettre)   #On s'assure que lettre est un caractère de la forme "a"
    liste=[] #Création d'une liste vide
    for i in range(m):
        for j in range(n):     #On itère sur les lignes (i de 0 à m-1) et les colonnes (j de 0 à n-1)
            liste.append('%s_%d%d'%(a,i+1,j+1))   #On ajoute à la fin de la liste (append) la chaine a_i+1j+1 , les +1 paliant au fait que range(k) va de 0 à k-1
    M=matrix(SR,m,n,liste)  #On crée une matrice mxn à partir de liste (Le SR dit à sage que la matrice est symbolique. Il n'est pas nécessaire de comprendre son rôle)
    return M  #La fonction retourne la matrice M
    </input>
    </sage>
    <p>On appelle maintenant le code pour créer deux matrices <m>2\times 2</m> avec des indices respectifs <m>a</m> et <m>b</m>.</p>
    <sage>
    <input>
    A=matquelc(2,2,'a')
    B=matquelc(2,2,'b')
    show(A+B)
    </input>
    </sage>
    <p>Remarquons que les fonctions sage utilisées ne permettent pas d'écrire <m>a_{1,2}</m>. Les entrées pourraient porter à confusion si la taille était supérieure à <m>9</m>.</p>
    <sage>
    <input>
    C=matquelc(12,15,'c')
    show(C)
    </input>
    </sage>
    <p>On peut ensuite "vérifier" l'équation de l'exemple <xref ref="ex-algmat1"/>   de la manière suivante. </p>
    <sage>
    <input>
A=matquelc(4,4,'a')
B=matquelc(4,4,'b')
C=matquelc(4,4,'c')
X=A^(-1)*(C-B)
A*X+B==C
    </input>
    </sage>
    <p>On a choisit <m>4\times 4</m> pour la taille des matrices, mais on aurait pu choisir d'autre valeurs, respectant les conditions énumérées dans l'exemple <xref ref="ex-algmat1"/>.
    Un rappel s'impose aussi sur sage. Si l'expression avec le <c>==</c> renvoie <c>true</c>, alors on peut être certain que c'est vrai. Par contre, un <c>false</c> ne signifie pas
    que l'expression est fausse, mais que sage est incapable de la vérifier.</p>
    </computation>
    </subsection>
    <conclusion xml:id="concl-matinverse">  <!-- Ajouter le même identifiant de la section après le - du xml:id -->
    <p>Les points importants de cette section sont
    <ul>
    <li><p> L'inverse d'une matrice ou d'une transformation n'existe pas toujours </p></li>
    <li><p>Lorsqu'il existe, l'inverse d'une matrice <m>2\times 2</m> est donnée par l'équation <xref ref="eq-matinverse2x2"/>.</p></li>
    <li><p> La <xref ref="def-matcarreeinverse"> définition </xref> de l'inverse d'une matrice carrée et le fait qu'un inverse à gauche est un inverse à droite.</p></li>
    <li><p><xref ref="prop-inverseproduit">L'inverse d'un produit</xref> est le produit des inverses, mais l'ordre est renversé </p></li>
    </ul> 
    De plus avec Sage, on peut calculer l'inverse d'une matrice <m>A</m> avec la commande <c>A.inverse()</c> ou <c>A^(-1)</c>.
    </p>
    </conclusion>
   <!--Inclure les exercices de la section ci-dessous--> 
</section>
<!-- Geogebra pour l'exemple transforr2inv -->
<!-- Exercice pour résoudre le 2e SEL 2x2 pour la matrice inverse. Voir le texte -->
<!-- Exercice en lien avec remarque <xref ref="rem-obtenirinverse"/>  Il suffit que Au=v ait une solution pour tous les vecteurs de la forme e1=(1,0,...,0),...,en=(0,0,...,1) -->
<!-- inverse d'un produit de trois matrices -->
<!-- Expliquer pourquoi la projection orthogonale ne peut pas avoir d'inverse, selon la proposition <xref ref="prop-investsurj"/> -->
<!-- Montrer que A0=0 et que 0A=0 où 0 est la matrice nulle de format approprié -->
<!-- Déterminer les caractéristiques pour chaque matrices des exemples de la sssec-algmat -->
<!-- Exercice Sage en lien avec sageex-algmat pour vérifier certaines égalité de ex-algmat2.

