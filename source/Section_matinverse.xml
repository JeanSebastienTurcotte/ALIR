<?xml version="1.0" encoding="UTF-8"?>

<!-- Ce fichier constitue une section du livre                              -->
<!--                                                                        -->
<!--      Algèbre linéaire : Intuition et rigueur                           -->
<!--                                                                        -->
<!-- Copyright (C) 2019  Jean-Sébastien Turcotte, Philémon Turcotte         -->
<!-- Licence à venir                                                        -->

<!-- Les sections sont divisées en quatre parties, en plus du titre. Les parties introduction et conclusion sont facultatives. Le texte de ceux-ci apparait respectivement avant et après les sections. Les exercices sont à la fin de la section -->

<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="sec-matinverse">   <!-- Ajouter l'identifiant de la section après le - du xml:id -->
    <title> Transformations inverses </title>
    <introduction xml:id= "intro-matinverse">  <!-- Ajouter le même identifiant de la section après le - du xml:id -->
    <p>Soit <m>T_1</m> une transformation linéaire, de <m>\R^2</m> vers <m>\R^2</m>. Est-il possible de trouver une transformation <m>T_2</m> telle que <m>T_2(T_1)=I</m>? En d'autres mots, est-il possible d'annuler
    l'effet de <m>T_1</m> sur les vecteurs de <m>\R^2</m> par une transformation? Une sorte d'inverse, au même sens où pour défaire une multiplication par <m>5</m> on peut diviser par <m>5</m>. Pourrait-on ainsi,
    définir la division de transformations linéaires, et par le fait même, de matrices?</p>
    <p>Est-il toujours possible de trouver cette transformation inverse? Dans <m>\R^2</m>? Qu'en est-il de la situation plus générale d'une transformation de <m>\R^n</m> vers <m>\R^m</m>?</p>
    <p>Dans cette section, on se concentre sur l'inverse d'une transformation de <m>\R^2</m> vers <m>\R^2</m>. On verra les conditions d'existence de l'inverse, de même que certaines propriétés. Quelques-une de ces propriétés se 
    généraliseront éventuellement au cas général, mais celui-ci demandera une attention particulière.</p>
    </introduction>
    <subsection xml:id="sssec-invgeo">
    <title>L'inverse de transformations géométriques</title>
    <p>On débute cette section par une partie intuitive, déterminer l'inverse des transformations de nature géométrique simple, celles de la liste <xref ref="liste-transfor2"/>. Avant, une définition de ce que représente l'inverse d'une transformation.</p>
    <definition xml:id="def-transfoinverseprelim">
    <title> La transformation inverse, définition préliminaire</title>
    <statement><p>Soit <m>T_1</m> une transformation linéaire. La transformation inverse de <m>T_1</m>, notée <m>T_1^{-1}</m>, est une transformation telle que
    <men xml:id="eq-invprelim">
    T_1^{-1}(T_1(\vec{u}))=\vec{u}
    </men> pour tout <m>\vec{u}</m> dans le domaine de <m>T_1</m>.</p></statement>
    </definition>
    <remark xml:id="rem-invprelim">
    <title>Une précision sur l'inverse</title>
    <p>Dans la définition, il n'est pas mentionné que la transformation inverse est une transformation linéaire. Comme on peut interpréter l'équation <xref ref="eq-invprelim"/> de manière matricielle, on cherche l'existence
    d'une matrice <m>T_1^{-1}</m> telle que <m>T_1^{-1}T_1\vec{u}=\vec{u}</m> pour tout vecteur <m>\vec{u}</m> dans le domaine de <m>T_1</m>, c'est-à-dire une transformation telle que
    <me>
    T_1^{-1}T_1=I
    </me>.
    En vertu de la proposition <xref ref="prop-matsonttransfos"/>, si une telle matrice existe, alors <m>T_1^{-1}</m> est une transformation linéaire.</p>
    </remark>
    <example xml:id="ex-transfor2inv">
    <title>L'inverse de transformations géométriques</title>
    <statement><p>On considère les transformations de la liste <xref ref="liste-transfor2"/> et on cherche à les inverser, si possible.</p></statement>
    <solution>
    <p>La transformation identité n'a aucun effet sur les vecteurs de <m>\R^2</m>. Ainsi, si on la compose avec elle-même, on restera avec l'identité. L'inverse de <m>I</m> est donc <m>I</m>.</p>
    </solution>
    <solution>
    <p>Afin de défaire une réflexion par rapport à l'axe des <m>x</m>, il semble suffisant d'appliquer à nouveau la réflexion. On aurait donc <m>S_x^{-1}=S_x</m>. On vérifie algébriquement que c'est le cas.
    <md>
    <mrow>S_xS_x&amp;=\begin{pmatrix}
    1 &amp;0 \\ 0 &amp; -1
    \end{pmatrix}\begin{pmatrix}
    1 &amp;0 \\ 0 &amp; -1
    \end{pmatrix}</mrow>
    <mrow>&amp;=\begin{pmatrix} \lvert &amp; \lvert \\
                                \begin{pmatrix}
    1 &amp;0 \\ 0 &amp; -1
    \end{pmatrix}\vecd{1}{0} &amp; \begin{pmatrix}
    1 &amp;0 \\ 0 &amp; -1
    \end{pmatrix}\vecd{0}{-1}\\
     \lvert &amp; \lvert
    \end{pmatrix} &amp;&amp; \text{ selon la définition de la multiplication matricielle}</mrow>
<mrow> \begin{pmatrix}
1&amp;0\\ 0 &amp;1
\end{pmatrix}
</mrow>
</md>.
    L'inverse de cette réflexion est donc en effet la réflexion même.</p>
    </solution>
    <solution>
    <p>Pour défaire une rotation de <m>90^{\circ}</m>, il semble logique de faire une rotation de <m>-90^{\circ}</m>, correspondant en fait à une rotation dans le sens horaire. Selon l'équation <xref ref="eq-rotr2"/>,
    cette matrice serait <m>R_{-\frac{\pi}{2}}=R_{\frac{\pi}{2}}^{-1}=\begin{pmatrix}0 &amp; 1\\ -1 &amp; 0\end{pmatrix}</m>. Algébriquement, on a
    <md>
    <mrow>R_{-\frac{\pi}{2}}R_{\frac{\pi}{2}}&amp;=\begin{pmatrix}0 &amp; 1\\ -1 &amp; 0\end{pmatrix}\begin{pmatrix}0 &amp; -1\\ 1 &amp; 0\end{pmatrix}</mrow>
    <mrow>&amp;=\begin{pmatrix}(0,1)\cdot (0,1) &amp; (0,1)\cdot (1,0)\\ (-1,0)\cdot (0,1) &amp; (-1,0)\cdot(-1,0)\end{pmatrix} &amp;&amp; \text{ en vertu de la multiplication composante par composante}</mrow>
    <mrow>&amp;=\begin{pmatrix}1 &amp; 0\\ 0 &amp; 1\end{pmatrix}</mrow>
    </md>.
    La transformation inverse de la rotation de <m>90^{\circ}</m> est donc bel et bien une rotation dans le sens opposé. Il est intéressant de remarquer que <m>R_{-\frac{\pi}{2}}=R_{\frac{\pi}{2}}^{-1}</m> et qu'on a aussi
    <m>R_{\frac{\pi}{2}}=R_{\frac{-\pi}{2}}^{-1}</m>. On vérifiera éventuellement si cela se produit toujours.
    </p>
    </solution>
    <solution>
    <p>Un étirement horizontal multiplie la première composante d'un vecteur par <m>r</m>. Pour défaire cette transformation, on devrait diviser cette composante par <m>r</m>, ce qui donnerait un étirement de facteur
    <m>\frac{1}{r}</m>. Ici se présente un premier problème dans la recherche d'inverse. En effet, si <m>r=0</m>, il est impossible de diviser par <m>0</m> et donc l'inverse, s'il existe, ne serait pas un étirement 
    de facteur <m>\frac{1}{r}</m>. On débute par considérer le cas <m>r\neq 0</m> avant de réfléchir à ce que représente un étirement de facteur <m>0</m> et son inverse. Ainsi, si <m>r\neq 0</m>, on a
    <md>
    <mrow> Eh_{1/r}Eh_{r}&amp;=\begin{pmatrix} \frac{1}{r} &amp; 0\\ 0&amp; 1\end{pmatrix}\begin{pmatrix} r &amp; 0\\ 0&amp; 1\end{pmatrix}</mrow>
    <mrow>&amp;\begin{pmatrix} 1 &amp; 0\\ 0&amp; 1\end{pmatrix} &amp;&amp; \text{on arrive à ceci avec l'une ou l'autre des interprétations du produit matriciel.}</mrow>
    </md>.
    Ainsi, <m>Eh_r^{-1}=Eh_{\frac{1}{r}}</m> si <m>r\neq 0</m>.
    </p>
    <p>On considère maintenant le cas <m>r=0</m>. La matrice de l'étirement est 
    <me>
    Eh_0=\begin{pmatrix} 0&amp; 0\\ 0 &amp;1\end{pmatrix}
    </me>. 
    On prétend qu'il ne peut exister d'inverse pour cette transformation. En effet, soit <m>A</m> la matrice représentant l'inverse. Selon la définition <xref ref="def-transfoinverseprelim"/>, il faut que <m>AEh_0=I</m>. 
    En particulier, il faudrait que <m>A\vecd{0}{0}=\vecd{1}{0}</m> selon la définition du <xref ref="def-matmatprod">produit matriciel</xref>, puisque <m>\vecd{0}{0}</m> est la première colonne de <m>Eh_0</m> et <m>\vecd{1}{0}</m> 
    la première colonne de la matrice identité. Selon la proposition <xref ref="prop-transfolinprop"/>, l'image du vecteur nul sera toujours le vecteur nul et donc, il est impossible d'avoir <m>\vecd{1}{0}</m> comme résultat.</p>
    <p>Il est cependant possible que l'inverse existe, mais ne soit pas une transformation linéaire. Quand on précisera la définition <xref ref="def-transfoinverseprelim"/>, cette posibilité sera invalidée.</p>
    </solution>
    <solution>
    <p>La solution pour l'inverse de <m>Ev_r</m> est donnée à l'exercice <xref provisional="exo-Evrinv"/>.</p>
    </solution>
    <solution>
    <p>Il est évident que si les composantes d'un vecteurs ont été permutées, cette même permutation redonnera le vecteur initial. Ainsi, <m>P^{-1}=P</m>. La vérification algébrique est laissée au lecteur.</p>
    </solution>
    <solution>
    <p>On est forcé une fois de plus à réfléchir à la signification de l'inverse d'une transformation avec le cas de la projection orthogonale. Si on pense aux fonctions réelles traditionnelles et à leur inverse, quelque chose de commun ressort.
    L'inverse de la fonction <m>y=ax+b</m> est <m>y=\frac{(x-b)}{a}</m>, pour autant que <m>a\neq 0</m>. La fonction <m>f(x)=e^x</m> a pour inverse le logarithme naturel, <m>g(x)=\ln(x)</m>. Dans chacun de ces cas, on remarque
    que pour chaque <m>x</m> dans le domaine, il correspond un seul <m>y</m> dans l'image tel que <m>f(x)=y</m>. Si on regarde la fonction <m>f(x)=x^2</m>, celle-ci ne respecte pas cette condition. Par exemple, <m>f(-2)=f(2)=4</m>.
    </p>
    <p>Lorsqu'on veut inverser la fonction <m>x^2</m>, on parle souvent de la racine carrée. Cet inverse ne fonctionne toutefois que pour des valeurs de <m>x\geq 0</m> (Un choix qui a été fait de considérer cette branche plutôt que l'autre).
    De même, pour inverser la fonction <m>\sin(x)</m>, on ne considère que les valeurs de <m>x\in\left[-\frac{\pi}{2},\frac{\pi}{2}\right]</m>. Sur chacune de ces branches, il n'existe qu'une seule paire de nombres <m>x,y</m> 
    tels que <m>f(x)=y</m>. Ainsi, on peut inverser la fonction sans problème.</p>
    <p>On revient à la projection. Pour un vecteur <m>\vec{v}</m> parallèle à <m>\vec{w}</m>, il existe une infinité de vecteurs <m>\vec{u}</m> tels que <m>\text{prof}_{\vec{w}}(\vec{u})=\vec{v} </m>. Comment choisir
    l'inverse de <m>\vec{v}</m> dans ce cas, parmi l'infinité de possibilités? On préfère ne pas choisir dans ce cas et dire que la projection orthogonale ne possède pas d'inverse.</p>
    <p>En appronfondissant notre intuition géométrique dans les prochaines sections, on comprendra davantage la raison de l'inexistance de l'inverse de la projection. On peut s'imaginer en quelque sorte qu'il y a une 
    perte d'information lorsque la projection est appliquée et qu'il n'est pas possible de revenir en arrière.</p>
    </solution>
    </example>
    <p>Le dernier exemple est riche en intuition géométrique et en questionnement. On a pu trouver des inverses sans faire de calculs, sauf pour vérifier que l'intuition était bonne. On a également constaté un cas où lorsque 
    <m>B</m> est l'inverse de <m>A</m>, <m>A</m> est aussi l'inverse de <m>B</m>. Ce constat, vrai en général pour les matrices carrées, n'est pas aussi évident que cela puisse le paraitre. Également, on a réalisé que 
    l'inverse d'une matrice n'existe pas toujours. On verra bientôt des premiers critères géométriques et algébriques nécessaires pour avoir  l'existence d'une transformation inverse.</p>
    </subsection>
    <subsection xml:id="sssec-inv2x2">
    <title>L'inverse d'une transformation linéaire du plan</title>
    <p>Dans cette sous-section, on cherche à établir une formule, mais surtout des critères pour déterminer si une transformation possède une transformation inverse. Avant, on donne un exemple qui motive la recherche
    de l'inverse.</p>
    <p>On sait que les colonnes de la matrice d'une transformation linéaire correspondent aux images des vecteurs <m>(1,0)</m> et <m>(0,1)</m> (dans <m>\R^2</m>, le cas général étant semblable, voir la proposition <xref ref="prop-matsonttransfos"/> et
    le texte qui la suit). Si par contre on connait l'effet d'une transformation <m>T</m>, non pas sur ces vecteurs, mais sur deux vecteurs quelconques. <fn>Il y a une condition à respecter, mais on ne veut pas trop en dire pour le moment.</fn></p>
    <p>Concrètement, si <m>T(\vec{u}_1)=\vec{v}_1</m> et <m>T(\vec{u}_2)=\vec{v}_2</m>, alors selon la définition de la multiplication, on peut écrire
    <me>
    T\begin{pmatrix} \lvert &amp; \lvert \\ \vec{u}_1 &amp; \vec{u}_2 \\ \lvert &amp; \lvert \end{pmatrix}=\begin{pmatrix} \lvert &amp; \lvert \\ \vec{v}_1 &amp; \vec{v}_2 \\ \lvert &amp; \lvert \end{pmatrix}
    </me>.
    Pour simplifier, on écrit <m>TU=V</m>. Si on était capable d'inverser la matrice <m>U</m>, on pourrait isoler la matrice de la transformation linéaire <m>T</m>: 
    <md>
    <mrow>TU&amp;=V</mrow>
    <mrow>TUU^{-1}&amp;=VU^{-1} &amp; &amp; \text{multiplication à droite de part et d'autre par } U^{-1}</mrow>
    <mrow number="yes" xml:id="eq-UUm1">TI&amp;=VU^{-1} &amp;&amp; \text{ car } UU^{-1}=I \text{ par définition de l'inverse}</mrow>
    <mrow number="yes" xml:id="eq-matparvecteursuv"> T&amp;=VU^{-1}</mrow>
    </md>.</p>
    <p>Techniquement, l'équation <xref ref="eq-UUm1"/> suppose que l'inverse <m>U^{-1}</m> sera le même à droite (la définition <xref ref="def-transfoinverseprelim"/> parle seulement d'inverse à gauche). La définition 
    <xref provisional="def-transfoinverse"/> qui suivra permettra de lever l'ambiguité. On accepte l'incohérence temporaire puisque le but n'est que de donner un exemple de l'utilisation de la matrice inverse.</p>
    <p>On débute l'exploration de l'inverse d'une matrice quelconque par le calcul d'un tel inverse pour une matrice qui n'a pas d'interprétation géométrique claire.</p>
    <example xml:id="ex-matinverse1">
    <title>Un premier calcul de matrice inverse</title>
    <statement>
    <p>On considère la matrice <m>A=\begin{pmatrix} 1&amp; 1\\ 4&amp; 5 \end{pmatrix}</m>. On cherche l'inverse de cette transformation, sous hypothèse que l'inverse existe.</p>
    </statement>
    <solution>
    <p>On cherche une matrice <m>A^{-1}</m> telle que <m>A^{-1}\vecd{1}{4}=\vecd{1}{0}</m> et <m>A^{-1}\vecd{1}{5}=\vecd{0}{1}</m>. Si on pose <m>A^{-1}=\begin{pmatrix} x&amp;z\\ y&amp;w \end{pmatrix}</m>, on obtient une 
    paire de systèmes à deux équations deux inconnues. D'une part,
    <md>
    <mrow>A^{-1}\vecd{1}{4}&amp;=\vecd{1}{0} </mrow>
    <mrow xml:id="eq-matinverse1-1">\vecd{x+4z}{y+4w}&amp;=\vecd{1}{0} </mrow>
    </md>
     et d'autre part
      <md>
    <mrow>A^{-1}\vecd{1}{5}&amp;=\vecd{0}{2} </mrow>
    <mrow xml:id="eq-matinverse1-2">\vecd{x+5z}{y+5w}&amp;=\vecd{0}{1} </mrow>
    </md>.
    </p>
   <p> En prenant la première composante des équations <xref ref="eq-matinverse1-1"/> et <xref ref="eq-matinverse1-2"/>, qui ne contiennent que les variables <m>x</m>
     et <m>z</m>, on obtient
     <md>
     <mrow>x+4z&amp;=1 </mrow>
     <mrow>x+5z&amp;=0</mrow>
     </md>.
     La seconde de ces équations permet d'obtenir <m>x=-5z</m>, puis en substituant dans la première on détermine <m>z=-1</m>. On a alors <m>x=5</m>.
     </p>
      <p> En prenant la deuxième composante des équations <xref ref="eq-matinverse1-1"/> et <xref ref="eq-matinverse1-2"/>, qui ne contiennent que les variables <m>y</m>
     et <m>w</m>, on obtient
     <md>
     <mrow>y+4w&amp;=0 </mrow>
     <mrow>y+5w&amp;=1</mrow>
     </md>.
     La première de ces équations permet d'obtenir <m>y=-4z</m>, puis en substituant dans la seconde on détermine <m>w=1</m>. On a alors <m>y=-4</m>.
     </p>
     <p>Ainsi, la matrice inverse doit être <m>A^{-1}=\begin{pmatrix} 5 &amp; -1 \\ -4 &amp; 1 \end{pmatrix}</m>. Une vérification ne fait jamais de tort:
     <md>
     <mrow>A^{-1}A&amp;=\begin{pmatrix} 5 &amp; -1 \\ -4 &amp; 1 \end{pmatrix}\begin{pmatrix} 1 &amp; 1 \\ 4 &amp; 5 \end{pmatrix}</mrow>
     <mrow>&amp;=\begin{pmatrix} (5,-1)\cdot (1,4) &amp; (5,-1)\cdot (1,5) \\ (-4,1)\cdot (1,4) &amp; (-4,1)\cdot (1,5) \end{pmatrix}</mrow>
     <mrow>&amp;=\begin{pmatrix} 1&amp; 0\\ 0&amp;1\end{pmatrix}</mrow>
     <mrow>&amp;=I</mrow>
     </md>.
     </p>
    </solution>
    </example>
    <p>On considère maintenant une matrice <m>A=\begin{pmatrix} a&amp; c\\ b &amp; d \end{pmatrix}</m> quelconque et on cherche l'inverse. En se basant sur les calculs faits à
    l'exemple <xref ref="ex-matinverse1"/>, on peut retrouver deux système d'équations analogues. D'abord,
    <md>
   <mrow> A^{-1}\vecd{a}{b}&amp;= \vecd{1}{0}</mrow>
   <mrow> \vecd{ax+bz}{ay+bw}&amp;= \vecd{1}{0}</mrow>
    </md>
    et ensuite
    <md>
   <mrow> A^{-1}\vecd{c}{d}&amp;= \vecd{0}{1}</mrow>
   <mrow> \vecd{cx+dz}{cy+dw}&amp;= \vecd{0}{1}</mrow>
    </md>.
    </p>
    <p>Rappelons ici que <m>a,b,c,d</m> sont les coefficients de la matrice <m>A</m> et qu'ils sont en général connus. On cherche donc à résoudre ces systèmes en fonction
    des inconnues <m>x,y,z,w</m>. En couplant les premières composantes ensemble et les secondes composantes ensemble, on obtient deux systèmes à deux équations et deux inconnues.
    Le premier de ces systèmes est
    <md>
    <mrow>ax+bz&amp;=1 </mrow>
    <mrow> cx+dz&amp;=0</mrow>
    </md>
    et le second
    <md>
    <mrow>ay+bw&amp;=0 </mrow>
    <mrow> cy+dw&amp;=1</mrow>
    </md>.
    </p>
    <p>Comme on veut éviter de restreindre le plus possible les valeurs de <m>a,b,c,d</m>, on ne peut pas isoler <m>x</m> ou <m>z</m> dans la seconde équation du premier système.
    Cela supposerait que <m>c\neq 0</m> ou <m>d\neq 0</m>. Pour contourner ce problème, on multiplie la première équation par <m>c</m> et la seconde par <m>a</m>. On obtient
    <mdn>
    <mrow xml:id="eq-matinverse2x2-1">acx+bcz&amp;=c </mrow>
    <mrow xml:id="eq-matinverse2x2-2"> acx+adz&amp;=0</mrow>
    </mdn>.
    En soustrayant l'équation <xref ref="eq-matinverse2x2-1"/> de l'équation <xref ref="eq-matinverse2x2-2"/>, on obtient
    <me>
    (ad-bc)z=-c
    </me>.
    On ne peut plus éviter la division et donc, en faisant la supposition que <m>ad-bc\neq 0</m>, on obtient <m>z=\frac{-c}{ad-bc}</m>. 
    </p>
    <p>En multipliant maintenant la première équation du premier système par <m>d</m> et la seconde par <m>b</m>, on obtient
    <mdn>
    <mrow xml:id="eq-matinverse2x2-3">adx+bdz&amp;=d </mrow>
    <mrow xml:id="eq-matinverse2x2-4"> abx+bdz&amp;=0</mrow>
    </mdn>.
    En soustrayant  cette fois l'équation <xref ref="eq-matinverse2x2-4"/> de l'équation <xref ref="eq-matinverse2x2-3"/>, on obtient
    <me>
    (ad-bc)x=d
    </me>.
    La supposition que <m>ad-bc\neq 0</m> ayant déjà été faite, on trouve <m>x=\frac{d}{ad-bc}</m>.
    </p>
    <p>Dans l'exercice <xref provisional="exercice deuxieme système matrice inverse"/>, il est montré que <m>y=\frac{-b}{ad-bc}</m> et <m>w=\frac{a}{ad-bc}</m>.</p>
    <p>En sortant le facteur <m>ad-bc</m> commun à chacun des termes, on obtient finalement la matrice inverse 
    <men xml:id="eq-matinverse2x2">
    \frac{1}{ad-bc}\begin{pmatrix} d \amp -b \\ -c \amp a \end{pmatrix}
    </men>.</p>
    <example xml:id="ex-matinverse2">
    <title>Calcul de l'inverse avec la formule</title>
    <p>On reprend la matrice de l'exemple <xref ref="ex-matinverse1"/> et on détermine son inverse avec la formule <xref ref="eq-matinverse2x2"/>. On obtient
    <md>
    <mrow>A^{-1}&amp;=\frac{1}{1*5-4*1}\begin{pmatrix} 5 \amp -1 \\ -4 \amp 1 \end{pmatrix} </mrow>
    <mrow>&amp;=\begin{pmatrix} 5 \amp -1 \\ -4 \amp 1 \end{pmatrix}</mrow>
    </md>,
    qui correspond ave ce qui a été obtenu plus tôt.
    </p>
    </example>
    <p>Il semble qu'un critère pour déterminer si une matrice (ou une transformation) est inversible soit que <m>ad-bc\neq 0</m>. On réfléchit maintenant à ce que
    cela signifie si <m>ad-bc=0</m> et pourquoi une transformation ayant cette propriété ne peut être inversible.</p>
    <p>
    Dans un premier temps, si<m>ad-bc=0</m>, alors <m>ad=bc</m>. On a aussi 
    <md>
    <mrow> c(a,b)&amp;=(ac,bc)</mrow>
    <mrow>&amp;=(ac,ad)</mrow>
    <mrow>&amp;=a(c,d)</mrow>
    </md>.
    Les vecteurs <m>(a,b)</m> et <m>(c,d)</m> semblent donc parallèles. En fait, il faut distinguer certains cas problématiques, ce que l'on fera un peu plus tard. D'abord, on montre que des vecteurs parallèles sont suffisants
    pour faire en sorte qu'une matrice <m>2\times 2</m> ne soit pas inversible.
    </p>
    <p>En effet, si <m>(a,b)=k(c,d)</m>, alors 
    <md><mrow> a&amp;=kc</mrow>
    <mrow>ad&amp;=kcd</mrow>
    <mrow>ad&amp;=kdc</mrow>
    <mrow>ad&amp;=bc</mrow>
    <mrow>ad-bc&amp;=0</mrow>
    </md>.
    Le seul problème avec l'argument est que si <m>(a,b)=(0,0)</m> ou <m>(c,d)=(0,0)</m>, les vecteurs ne sont pas considérés comme parallèles (voir la définition <xref ref="def-vecpara"/> et la note de bas de page l'accompagnant).
    Or de manière évidente, si <m>(a,b)=(0,0)</m> ou <m>(c,d)=(0,0)</m> (ou les deux), alors <m>ad-bc=0</m>. Dans tous les cas, la situation géométrique est analogue à une projection. La transformation associée à une matrice 
    <m>2\times 2</m> telle que <m>ad-bc=0</m> envoie le plan <m>\R^2</m> sur une droite (ou un point si <m>a=b=c=d=0</m>). Tel que mentionné à l'exemple <xref ref="ex-transfor2inv"/>, il y a perte d'information qui fait qu'on
    ne peut défaire la transformation.
    </p>
    <p>On termine avec des commandes Sage en lien avec la sous-section.</p>
    <computation xml:id="sageex-inv2x2">
    <title>Les matrices inverse sur Sage</title>
    <p>Sur Sage, on peut calculer facilement l'inverse d'une matrice <m>2\times 2</m> <m> A</m> en utilisant la commande <c>A.inverse()</c>. On pourra bien entendu vérifer avec la multiplication matricielle
    que le calcul est bon.</p>
    <sage>
    <input>
A=column_matrix([[2,4],[-2,1]])
Ainv=A.inverse()
show("A^{-1}=",Ainv)
show("A^{-1}A=",Ainv*A)    
    </input>
    </sage>
    <p>Si l'inverse n'existe pas, Sage retourne une erreur, comme le montre le code suivant</p>
    <sage>
A=column_matrix([[2,4],[-2,-4]])
Ainv=A.inverse()    
    </sage>
    <p>On fait maintenant un exemple en référence à l'équation <xref ref="eq-matparvecteursuv"/>, qui stipule que si on connait l'effet d'une transformation <m>T</m> sur deux vecteurs, alors on peut déterminer 
    la matrice associée à la transformation sans calculer directement l'image des vecteurs <m>(1,0)</m> et <m>(0,1)</m> (pour le cas <m>2\times 2</m>).</p>
    <p>On considère donc une transformation linéaire <m>T</m> telle que <m>T(2,4)=(-3,1)</m> et <m>T(1,-4)=(-2,-5)</m>. On cherche la matrice représentant <m>T</m>. Avec Sage et l'équation <xref ref="eq-matparvecteursuv"/>, on a</p>
    <sage>
    <input>
   U=column_matrix([[2,4],[1,-4]])
V=column_matrix([[-3,1],[-2,-5]])
Uinv=U.inverse()
T=V*Uinv
show("T=",T) 
    </input>
    </sage>
    <p>Afin de vérifier, on peut calculer les images des vecteurs <m>(2,4)</m> et <m>(1,-4)</m>.</p>
    <sage>
    <input>
u1=U.column(0)
u2=U.column(1)
show("Tu1=",T*u1)
show("Tu2=",T*u2)    
    </input>
    </sage>
    <p>Évidemment, on aurait pu calculer <m>TU</m> et vérifier que l'on obtient <m>V</m>, mais on s'est permis ici de rappeler la commande <c>U.column()</c> qui permet d'accéder aux colonnes d'une matrice.
    Il est pratique de travailler de cette manière plutôt que de par exemple, aller définir manuellement <c>u1=vector([2,4])</c>. L'idée est que si, pour une raison quelconque, on décidait de changer la matrice <m>U</m>,
    il suffirait de changer seulement la ligne où <m>U</m> est définie. Ceci est résumé dans le conseil <xref ref="con-defefficace"/> ci-dessous.
    </p>
    </computation>
    <insight xml:id="con-defefficace">
    <title>Utilisation efficace de l'informatique</title>
    <p>L'un des  but d'une utilisation de l'informatique est de lui déléguer certains calculs, afin de se concentrer davantage sur les concepts. Il faut toutefois être efficace dans son utilisation afin de 
    bénéficier de sa flexibilité et de toute sa puissance. On illustre avec un exemple.</p>
    <p>Soit <m>A=\begin{pmatrix} 1\amp 3\\ -2\amp 6 \end{pmatrix}</m>. On cherche à calculer la quantité <m>ad-bc</m> pour cette matrice. Si ce nombre n'est pas zéro, on veut calculer <m>A^{-1}\vecd{2}{3}</m>. Voici une première option.</p>
    <sage>
    <input>
    #On vérifie si ad-bc=0
    a=1
    b=-2
    c=3
    d=6
    show(a*d-b*c)
    #On définit la matrice
    A=column_matrix([[1,-2],[3,6]])
    #On inverse la matrice
    Ainv=A.inverse()
    #On calcule l'inverse de (2,3)
    u=vector([2,3])
    show(Ainv*u)
    </input>
    </sage>
    <p>Simple, la séquence d'instruction ressemble à ce qui serait fait sur une feille de papier, l'ordre en fonction des questions demandées. Si maintenant on demandait les mêmes questions, mais pour la matrice <m>B=\begin{pmatrix} -1\amp 2\\ 4\amp -3 \end{pmatrix}</m>.
    On s'imagine alors recopier le code et corriger les lignes <c>2-5</c> et la ligne <c>8</c>. 
    </p>
    <p>Voici une deuxième option pour répondre aux questions initiales demandées.</p>
    <sage>
    <input>
    #On définit la matrice et le vecteur 
    A=column_matrix([[1,-2],[3,6]])
    u=vector([2,3])
    #On vérifie si ad-bc=0
    a=A[0][0]
    b=A[1][0]
    c=A[0][1]
    d=A[1][1]
    show(a*d-b*c)
    #on calcule l'inverse de A et l'image inverse du vecteur (2,3)
    Ainv=A.inverse()
    show(Ainv*u)
    </input>
    </sage>
    <p>Maintenant, pour répondre aux questions avec la matrice <m>B</m>, seulement la ligne <c>1</c> doit être éditée.</p>
    </insight>
    </subsection>
    <subsection xml:id="sssec-matinv">
    <title>Définition formelle de l'inverse d'une matrice carrée</title>
    <p>Dans cette sous-section, on étudie les propriétés de la matrice inverse. On précise également la définition de l'inverse. Bien que pour le moment, on ne connaisse que l'inverse d'une transformation de 
    <m>\R^2</m> vers <m>\R^2</m>, les propriétés étudiées ici sont valides pour toute matrice <em>carrée</em> possédant un inverse. On verra dans le chapitre <xref provisional="Chapitre SEL"/> comment obtenir l'inverse
    d'une matrice carrée de taille plus grande que <m>2</m>.</p>
    <p>La première étape de notre démarche est de valider l'hypothèse que si <m>A^{-1}A=I</m>, alors <m>AA^{-1}=I</m> également. Cela parait sans doute plus simple que ce ne l'est vraiment. En effet,
    rien ne garantit à priori que si <m>A</m> possède un inverse, alors cet inverse possède lui-même un inverse.</p>
    <p>Il est par contre assez intuitif que l'inverse de l'inverse de <m>A</m> devrait être <m>A</m>. Les prochains résultats visent à mettre un peu d'ordre et de rigueur derrière l'intuition. </p>
    <remark xml:id="rem-transfodomim">
    <title>Le domaine et l'image d'une transformation linéaire</title>
    <p>Les prochains résultats parlent du domaine et de l'image d'une transformation linéaire. On rappelle que selon la proposition <xref ref="prop-matsonttransfos"/>, une matrice <m>m\times n</m> est une transformation
     linéaire de <m>\R^n</m> vers <m>\R^m</m>. Le chapitre <xref provisional="chap-SEL"/> explore en profondeur domaine, image et zéros d'une transformation linéaire.</p>
     <p> Pour ce qui suit, on s'intéresse seulement aux matrices carrées <m>n\times n</m>.</p>
    </remark>
    <p>D'abord, on cherche à démontrer que si <m>A</m> est une matrice pour laquelle il existe <m>A^{-1}</m> telle que <m>A^{-1}A=I</m>, alors il existe également une matrice <m>B</m> telle que <m>AB=I</m>. En d'autres mots,
    si <m>A</m> possède un inverse selon la définiton <xref ref="def-transfoinverseprelim"/> et la remarque <xref ref="rem-invprelim"/>, alors <m>A</m> est aussi l'inverse d'une matrice. </p>
    <p>Par la suite, on montre que <m>B=A^{-1}</m>.</p>
    <proposition xml:id="prop-investsurj">
    <title>Les transformations inversibles atteignent tous les vecteurs de leur image</title>
    <statement>
    <p>Si <m>A</m> est une matrice possédant un inverse au sens de la définition <xref ref="def-transfoinverseprelim"/> et de la remarque <xref ref="rem-invprelim"/>, alors pour tout <m>\vec{v}\in\R^n</m>, il existe <m>\vec{u}\in\R^n</m> tel que <m>A\vec{u}=\vec{v}</m>.
    Cela signifie que chaque vecteur de <m>\R^n</m> (l'image) est atteint par la transformation d'un (au moins) vecteur de <m>\R^n</m> (domaine). </p>
    </statement>
    <proof>
    <p>Soit <m>\vec{v}\in \R^n</m> un vecteur de l'image de <m>A</m>. On cherche <m>\vec{u}</m> tel que <m>A\vec{u}=\vec{v}</m>. Puisque <m>A</m> possède un inverse <m>A^{-1}</m>, on a
    <md>
    <mrow>A\vec{u}&amp;=\vec{v}</mrow>
    <mrow>A^{-1}A\vec{u}&amp;=A^{-1}\vec{v} &amp;&amp; \text{ En multipliant à droite de chaque côté par } A^{-1}\text{. Le côté droit de l'égalité est bien défini, ca l'inverse est une transformation dont le domaine est } \R^n\text{.}</mrow>
    <mrow>I\vec{u}&amp;=A^{-1}\vec{v} &amp;&amp; \text{ car } A^{-1} \text{ est l'inverse de } A\text{.}</mrow>
    <mrow>\vec{u}&amp;=A^{-1}\vec{v} &amp;&amp; \text{par <xref ref="def-matid">définition</xref> de la transformation identité.}</mrow>
    </md>.
    </p>
    <p>Ainsi, il existe <m>\vec{u}</m> tel que <m>A\vec{u}=\vec{v}</m> et on peut explicitement le calculer à l'aide de l'inverse.</p>
    </proof>
    </proposition>
    <proposition xml:id="prop-surjestinv">
    <title>Les transformations atteignant chaque vecteur de leur image sont l'inverse d'une transformation</title>
    <statement><p>Soit <m>A</m> une matrice telle que pour tout <m>\vec{v}\in \R^n</m>, il existe <m>\vec{u}\in\R^n</m> telle que <m>A\vec{u}=\vec{v}</m>, alors <m>A</m> est l'inverse d'une matrice <m>B</m>, au
    sens de la définition <xref ref="def-transfoinverseprelim"/> et de la remarque <xref ref="rem-invprelim"/>. Cela signifie qu'il existe une matrice <m>B</m> telle que <m>AB=I</m>.</p></statement>
    <proof>
    <p>Soit <m>\vec{e}_1=(1,0,\cdots , 0),\vec{e}_2=(0,1,0,\ldots , 0),\ldots, \vec{e}_n=(0,\ldots ,0 ,1)</m> les vecteurs colonnes de la matrice identité. Par hypothèse sur <m>A</m>, il existe des vecteurs
    <m>\vec{b}_1,\vec{b}_2,\ldots , \vec{b}_n</m> tels que <m>A\vec{b}_k=\vec{e}_k</m> pour chaque <m>k=1,\ldots , n</m>.</p>
    <p>On pose 
    <me>
    B=\begin{pmatrix}
    \lvert \amp \lvert \amp \cdots \amp \lvert \\
     \vec{b}_1\amp \vec{b}_2\amp \cdots \amp \vec{b}_n \\
    \lvert \amp \lvert \amp \cdots \amp \lvert \\
    \end{pmatrix}
    </me>.
    On a alors <m>AB=I</m> puisque la colonne <m>k</m> de <m>AB</m> est donnée par <m>A\vec{b_k}=\vec{e}_k</m>.
    </p>
    </proof>
    </proposition>
    <aside>
    <title> En passant</title>
    <p>Une fonction <m>f</m> d'un ensemble <m>X</m> vers un ensemble <m>Y</m> est dite surjective (en anglais on dit parfois "onto") si pour chaque <m>y\in Y</m> il existe au moins un élément <m>x\in X</m> tel que <m>f(x)=y</m>. Cela signifie que chaque valeur
    de l'image est atteinte par au moins une valeur du domaine.
    </p>
    <p>
    La proposition <xref ref="prop-investsurj"/> montre donc qu'une transformation linéaire inversible est surjective alors que
    la proposition <xref ref="prop-surjestinv"/> montre elle qu'une transformation surjective est inversible.</p>
    <p>Lorsqu'une fonction est telle que si <m>f(x)=f(y)</m>, alors <m>x=y</m>, on dit qu'elle est injective (en anglais on dit parfois "one-to-one"). Cela signifie que chaque valeur atteinte dans <m>Y</m> ne l'est que par un et un seul <m>x</m>.</p>
    <p>Une fonction qui est à la fois injective et surjective est dite bijective.</p>
    </aside>
    <p>On peut finalement montrer, en utilisant les deux propositions précédentes, que l'inverse d'une matrice fonctionne des deux côtés. Cela va permettre de préciser 
    la définition <xref ref="def-transfoinverseprelim"/>.</p>
    <theorem xml:id="thm-invgauchedroite">
    <title>L'inverse d'une matrice carrée est bilatéral</title>
    <statement>
    <p>Soit <m>A</m> une matrice carrée pour laquelle il existe <m>A^{-1}</m> telle que <m>A^{-1}A=I</m>. Alors <m>AA^{-1}=I</m>.</p>
    </statement>
    <proof>
    <p>Puisque <m>A</m> possède un inverse au sens de la définition <xref ref="def-transfoinverseprelim"/>, la proposition <xref ref="prop-investsurj"/> dit que chaque
    valeur de  son image est atteinte par une valeur de son domaine.</p>
    <p>De plus, la proposition <xref ref="prop-surjestinv"/> affirme qu'il existe une matrice <m>B</m> telle que <m>AB=I</m>, c'est-à-dire que <m>A</m> est l'inverse d'une
    matrice <m>B</m> au sens de la définition <xref ref="def-transfoinverseprelim"/>. On a alors
    <md>
    <mrow> B&amp;= A^{-1}AB &amp;&amp; \text{car } A^{-1}A=I</mrow>
    <mrow>&amp;= A^{-1}(AB) &amp;&amp; \text{ par associativité de la multiplication matricielle}</mrow>
    <mrow>&amp;=A^{-1} &amp;&amp; \text{ car } AB=I</mrow>
    </md>.
    Ainsi l'inverse de l'inverse de <m>A</m> est <m>A</m>. On dira aussi que l'inverse à gauche de <m>A</m> est le même que son inverse à droite.</p>
    </proof>
    </theorem>
    <p>Ce résultat, peut-être pas très surprenant, est néanmoins primordial dans l'étude de l'algèbre linéaire. D'autant plus qu'on sait qu'en général, le produit matriciel
    ne commute pas. Lorsqu'il est question de l'inverse toutefois, c'est le cas. La définition suivante raffine la définition <xref ref="def-transfoinverseprelim"/>. Elle
    servira de référence à partir de maintenant pour l'inverse d'une transformation linéaire de <m>\R^n</m> vers <m>\R^n</m> ou de façon équivalente, d'une matrice carrée.</p>
    <definition xml:id="def-matcarreeinverse">
    <title>L'inverse d'une matrice carrée</title>
    <p>Soit <m>A</m> une matrice carrée. On dit que <m>A</m> est inversible s'il existe une matrice <m>A^{-1}</m> telle que 
    <me>
    A^{-1}A=AA^{-1}=I
    </me>.</p>
    <p>En termes d'une transformation linéaire, on dit que la transformation <m>T</m> de <m>\R^n</m> vers <m>\R^n</m> est inversible s'il existe une transformation
    linéaire <m>T^{-1}</m> de <m>\R^n</m> vers <m>\R^n</m> telle que 
    <me>
    T^{-1}(T(\vec{u}))=T(T^{-1}(\vec{u}))=\vec{u}
    </me>
    pour tout vecteur <m>\vec{u}\in\R^n</m>.</p>
    </definition>
    <remark xml:id="rem-obtenirinverse">
    <title>Obtenir l'inverse d'une matrice</title>
    <p>Ensemble, les propositions <xref ref="prop-investsurj"/> et <xref ref="prop-surjestinv"/>, combinées à la définition <xref ref="def-matcarreeinverse"/> donnent un critère pour déterminer si une matrice carrée <m>A</m> possède un inverse. Il faut que
    l'équation <m>A\vec{u}=\vec{v}</m> possède une solution pour chaque <m>\vec{v}\in \R^n</m>. L'exercice <xref provisional="exo-vecteursesuffisants"/> montre qu'en fait,
    il est suffisant d'avoir une solution pour les vecteurs
    <md>
    <mrow>\vec{e}_1&amp;=  (1,0,\ldots ,0)</mrow>
    <mrow>\vec{e}_2&amp;=  (0,1,0,\ldots,0) </mrow>
    <mrow>\vdots\&amp;= vdots  </mrow>
    <mrow>\vec{e}_n&amp;=  (0,\ldots ,0,1) </mrow>
    </md>.
    </p>
    <p>De plus, la proposition <xref ref="prop-surjestinv"/> donne une manière explicite pour obtenir l'inverse.</p>
    </remark>
    </subsection>
    <subsection xml:id="sssec-propinv">
    <title>Propriétés de la matrice inverse</title>
    </subsection>
    <conclusion xml:id="concl-matinverse">  <!-- Ajouter le même identifiant de la section après le - du xml:id -->
    <p>Les points importants de cette section sont
    <ul>
    <li><p> L'inverse d'une matrice ou d'une transformation n'existe pas toujours </p></li>
    <li><p>Lorsqu'il existe, l'inverse d'une matrice <m>2\times 2</m> est donnée par l'équation <xref ref="eq-matinverse2x2"/>.</p></li>
    </ul> 
    De plus avec Sage, 
    </p>
    </conclusion>
   <!--Inclure les exercices de la section ci-dessous--> 
</section>
<!-- Geogebra pour l'exemple transforr2inv -->
<!-- Exercice pour trouver l'inverse de A, chercher quels vecteurs c1 et c2 s'en vont sur (1,0) et (0,1). Ces vecteurs sont les colonnes de A^-1 -->
<!-- Exercice pour résoudre le 2e SEL 2x2 pour la matrice inverse. Voir le texte -->
<!-- Exercice en lien avec remarque <xref ref="rem-obtenirinverse"/>  Il suffit que Au=v ait une solution pour tous les vecteurs de la forme e1=(1,0,...,0),...,en=(0,0,...,1) -->
