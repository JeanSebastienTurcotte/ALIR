<?xml version="1.0" encoding="UTF-8"?>

<!-- Ce fichier constitue une section du livre                              -->
<!--                                                                        -->
<!--      Algèbre linéaire : Intuition et rigueur                           -->
<!--                                                                        -->
<!-- Creative Commons Attribution Share Alike 4.0 International             -->
<!-- CC-BY-SA 4.0                                                               -->
<!-- Jean-Sébastien Turcotte, Philémon Turcotte                             -->

<!-- Les sections sont divisées en quatre parties, en plus du titre. Les parties introduction et conclusion sont facultatives. Le texte de ceux-ci apparait respectivement avant et après les sections. Les exercices sont à la fin de la section -->

<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="sec-diagonalisation">   <!-- Ajouter l'identifiant de la section après le - du xml:id -->
    <title> Diagonalisation  </title>
    <introduction xml:id= "intro-diagonalisation">  <!-- Ajouter le même identifiant de la section après le - du xml:id -->
    <p>Aller aux <xref ref="exo-diagonalisation">exercices</xref> de la section.</p>
    <p>Dans la section précédente, on a vu que certaines matrices avaient des directions invariantes associées à des valeurs propres. Si les bonnes conditions sont respectées, il y a suffisamment de vecteurs propres pour obtenir une base de l'espace. Dans ce cas, ces vecteurs permettront d'offrir une perspective géométrique intéressante sur la transformation linéaire associée à la matrice. Il y aura aussi d'importantes conséquences algébriques qui découleront de cette perspective.</p>
    <p>Dans cette section, on introduit la diagonalisation d'une matrice, la notion de changement de base</p>
    </introduction>
    <subsection><title>Diagonalisation</title>
    <p>À la section <xref ref="sec-bases"/>, on a définit le concept de base ordonnée à partir d'un ensemble de vecteurs et des composantes d'un vecteur selon cette base. Ainsi, le vecteur <m>(1,9)</m> telle que représenté dans la base standard s'écrit plutôt <m>(3,-1)_{\mathcal{B}}</m> dans la base <m>\mathcal{B}=\langle (1,2),(2,-3)\rangle</m> puisque
    <me>
    3(1,2)-(2,-3)=(1,9)
    </me>. De plus, selon les notions de la sous-section <xref ref="sssec-transfomatrice" text="title"/>, une matrice contient dans chaque colonne l'image des vecteurs <m>\vec{e}_1,\vec{e}_2,\ldots , \vec{e}_n</m>. Une question naturelle qui se pose est, peut-on exprimer une matrice dans une autre base? À titre d'exemple, la matrice <m>A</m> de l'équation <xref ref="eq-Apropre"/> à l'introduction de la section précédente  peut aussi s'écrire selon la base <m>\mathcal{B}</m> comme <m>A=\begin{pmatrix} 14&amp; 0\\ 0&amp; 7\end{pmatrix}_{\mathcal{B}}</m>, puisque l'exemple <xref ref="ex-valpropresA1"/> a montré que c'était un étirement de facteur <m>14</m> dans la direction du vecteur <m>(1,2)</m> et un étirement de facteur <m>7</m> dans la direction du vecteur <m>(2,-3)</m>.</p>
    <p>Il y a une lien intéressant entre la matrice <m>A_{\mathcal{B}}</m> et la matrice <m>D=\begin{pmatrix} 14&amp; 0\\ 0&amp; 7\end{pmatrix}</m>. En regardant seulement ses entrées, il semble que ce soit la même matrice, mais comme <m>A_{\mathcal{B}}</m> est exprimée dans une autre base, les deux transformations représentent des transformations différentes. La matrice <m>A_{\mathcal{B}}</m> est un étirement de facteur <m>14</m> dans la direction du vecteur <m>(1,2)</m> et un étirement de facteur <m>7</m> dans la direction du vecteur <m>(2,-3)</m>, alors que <m>D</m> est un étirement horizontal et vertical de ces mêmes facteurs. On note <m>P=\begin{pmatrix} 1&amp; 2\\ 2&amp; -3 \end{pmatrix}</m> la matrice contenant les vecteurs propres en colonne. Justement, parce que ce sont des vecteurs propres, l'effet de <m>A</m> sur <m>P</m> est
    <md>
    <mrow>AP&amp;=\begin{pmatrix} 14\cdot 1&amp; 14\cdot 2\\ 7\cdot 2&amp; 7\cdot (-3) \end{pmatrix}</mrow>
    <mrow>&amp;=\begin{pmatrix} 1&amp; 2\\ 2&amp; -3 \end{pmatrix}\begin{pmatrix} 14&amp; 0\\ 0&amp; 7 \end{pmatrix}</mrow>
    <mrow>&amp;=PD</mrow>
    </md>.</p>
    <p>En particulier, on peut isoler <m>A</m> pour obtenir 
    <me>
    A=PDP^{-1}
    </me>.
    La proposition <xref ref="prop-propresindep"/> garantit ici que la matrice <m>P</m> est inversible puisque ses colonnes sont indépendantes. En général, ce pourrait ne pas être le cas. On introduit le concept de matrice diagonalisable.</p>
    <definition xml:id="def-diagonalisable">
    <title>Matrice diagonalisable</title>
    <statement><p>Soit <m>A</m> une matrice carrée. Si la matrice <m>A</m> possède <m>n</m> vecteurs propres indépendants, alors il existe des matrices <m>P,D</m> avec <m>D</m> une matrice diagonale telles que
    <me>
    A=PDP^{-1}
    </me>.
    On dit alors que <m>A</m> est diagonalisable.
    </p>
    <p>La matrice <m>P</m> contient dans ses colonnes les <m>n</m> vecteurs propres et la matrice <m>D</m> a pour éléments de sa diagonale les valeurs propres associées aux vecteurs propres, dans le même ordre que ceux-ci apparaissent dans <m>P</m>.</p>
    </statement></definition>
    <p>La matrice <m>A=\begin{pmatrix} 10&amp; 2\\ 6&amp; 11 \end{pmatrix}</m> est diagonalisable avec <m>P=\begin{pmatrix} 1&amp; 2\\ 2&amp; -3 \end{pmatrix}</m> et <m>D=\begin{pmatrix} 14&amp; 0\\ 0&amp; 7 \end{pmatrix}</m> comme l'a montré le calcul ci-dessus. On regarde d'autre exemple.</p>
    <example xml:id="ex-vecvalpropresdiago">
    <title>Diagonalisation de matrices</title>
    <statement><p>On reprend les matrices de l'exemple <xref ref="ex-vecvalpropres"/>. Parmi celles-ci, les matrices <m>A,C,F</m> avaient un nombre suffisant de vecteurs propres pour former une base. On s'intéresse donc à diagonaliser ces matrices. </p></statement>
    <solution><p>La matrice 
    <m>A=\begin{pmatrix} 3&amp;1\\ -2&amp;0 \end{pmatrix}</m> avait pour valeurs propres <m>\lambda_1=1</m> et <m>\lambda_2=2</m> et comme vecteurs propres associés à ces valeurs propres <m>\vec{v}_1=(-1,2)</m> et<m>\vec{v}_2=(-1,1)</m>.
    On pose donc <m>P=\begin{pmatrix} -1&amp;-1\\ 2&amp;1 \end{pmatrix}</m> et <m>D=\begin{pmatrix} 1&amp;0\\ 0&amp;2 \end{pmatrix}</m>. De cela, on devrait avoir
    <me>
    A=PDP^{-1}
    </me>.
    On vérifie avec Sage.
    </p>
    <sage>
    <input>
    P=column_matrix([[-1,2],[-1,1]])
    Pinv=P.inverse()
    D=column_matrix([[1,0],[0,2]])
    A=P*D*Pinv
    show(A)
    </input>
    </sage>
    </solution>
    <solution>
    <p>La matrice
    <m>C=\begin{vmatrix} -2 &amp; 2 &amp; 0 \\ 0 &amp; 2 &amp; 0 \\ 2 &amp; -1 &amp; 0 \end{vmatrix}</m> avait pour valeurs propres <m>\lambda_1=2,\lambda_2=-2</m> et <m>\lambda_3=0</m> et comme vecteurs propres associés à ces valeurs propres <m>\vec{v}_1=(1,2,0)</m>,<m>\vec{v}_2=(-1,0,1))</m> et <m>\vec{v}_3=(0,0,1)</m>.
    On pose donc <m>P=\begin{pmatrix} 1&amp;-1&amp;0\\ 2&amp;0&amp;0\\ 0&amp;1&amp; 1 \end{pmatrix}</m> et <m>D=\begin{pmatrix} 2&amp;0&amp; 0\\ 0&amp;-2&amp; 0\\ 0&amp; 0&amp; 0 \end{pmatrix}</m>. De cela, on devrait avoir
    <me>
    C=PDP^{-1}
    </me>.
    On vérifie avec Sage.
    </p>
    <sage>
    <input>
    P=column_matrix([[1,2,0],[-1,0,1],[0,0,1]])
    Pinv=P.inverse()
    D=column_matrix([[2,0,0],[0,-2,0],[0,0,0]])
    C=P*D*Pinv
    show(C)
    </input>
    </sage>
    </solution>
    <solution>
    <p>La matrice
    <m>F=\begin{pmatrix} -1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}</m> avait pour valeurs propres <m>\lambda_1=-1</m> et <m>\lambda_2=1</m>. Pour la valeur propre <m>\lambda_1</m>, on avait <m>\vec{v}_1=(1,0,0)</m>, alors que pour la valeur propre <m>\lambda_2</m> on avait deux vecteurs propres indépendants en <m>\vec{v}_2=(1,2,0)</m> et <m>\vec{v}_3=(0,0,1)</m>.
    On pose donc <m>P=\begin{pmatrix} 1&amp;1&amp;0\\ 0&amp;2&amp;0\\ 0&amp;0&amp; 1 \end{pmatrix}</m> et <m>D=\begin{pmatrix} -1&amp;0&amp; 0\\ 0&amp;1&amp; 0\\ 0&amp; 0&amp; 1 \end{pmatrix}</m>. De cela, on devrait avoir
    <me>
    F=PDP^{-1}
    </me>.
    On vérifie avec Sage.
    </p>
    <sage>
    <input>
    P=column_matrix([[1,0,0],[1,2,0],[0,0,1]])
    Pinv=P.inverse()
    D=column_matrix([[-1,0,0],[0,1,0],[0,0,1]])
    F=P*D*Pinv
    show(F)
    </input>
    </sage>
    </solution>
    </example>
    <p>La diagonalisation permet de voir la matrice sous un autre oeil, en permettant de la décrire par des transformations simples comme les étirements et les réflexions. Par exemple, la matrice <m>A</m> de l'exemple <xref ref="ex-vecvalpropres"/> semble être un étirement de facteur <m>2</m> dans la direction <m>(-1,1)</m> avec une direction inchangée en <m>(-1,2)</m>. La figure suivante illustre un parallélograme engendré par ces vecteurs et son image.</p>
        <figure xml:id="fig-vecpropre3">
    <caption>La transformation <m>A</m></caption>
    <image xml:id="img-vecpropre3">
    <sageplot>
v1=vector([-1,2])
v2=vector([-1,1])
P=column_matrix([v1,v2])
Pinv=P.inverse()
D=column_matrix([[1,0],[0,2]])
B=P*D*Pinv
Pointspara=[vector([0,0]),v1,v1+v2,v2]
Pointsparatrans=[B*pt for pt in Pointspara]
graph2=polygon(Pointsparatrans,color="red",alpha=0.25,aspect_ratio=0.75)+polygon(Pointspara,color="blue",alpha=0.25,aspect_ratio=0.75)
graph2
    </sageplot>
    <description><p>Un parallélogramme engendré par les deux vecteurs propres de la matrices A est illustré avec son image, qui est un étirement de facteur deux dans la direction de l'un des vecteurs propres, l'autre direction étant inchangée.</p></description>
    </image>
    </figure>
    <p>De même, la matrice <m>C</m> peut être vue comme un étirement de facteur <m>2</m> dans la direction <m>(1,2,0)</m>, un étirement aussi de facteur <m>2</m> ainsi qu'une réflexion dans la direction <m>(-1,0,1)</m>. Il y a aussi une projection sur le plan engendré par ces deux premiers vecteurs, mais c'est plus difficile à décrire puisque les projections générales n'ont pas encore été abordées. On les verra à la section <xref provisional="sec-projections"/>. On peut toutefois en observer l'effet sur la figure suivante. </p>
        <figure xml:id="fig-vecpropre4">
    <caption>La transformation <m>A</m></caption>
    <image xml:id="img-vecpropre4">
    <sageplot variant="3d">
v1=vector([1,2,0])
v2=vector([-1,0,1])
v3=vector([0,0,1])
P=column_matrix([v1,v2,v3])
Pinv=P.inverse()
D=column_matrix([[2,0,0],[0,-2,0],[0,0,0]])
C=P*D*Pinv
Pointspara=[vector([0,0,0]),v1,v1+v2,v2,vector([0,0,0]),v3,v3+v1,v1,v1+v3,v3+v1+v2,v1+v2,v3+v1+v2,v2+v3,v2,v2+v3,v3,vector([0,0,0])]
Pointsparatrans=[C*pt for pt in Pointspara]
graph2=line(Pointsparatrans,color="red",alpha=0.25,aspect_ratio=1,axes=True)+line(Pointspara,color="blue",alpha=0.25,aspect_ratio=1,)+plot(v1,color="red")+plot(v2,color="darkgreen")+plot(v3,color="blue")
graph2
    </sageplot>
    <description><p>Un parallélépipède engendré par les vecteurs propres de la matrices C est illustré avec son image par cette matrice. Dans ce cas, l'image est en fait un parallélograme dont les côtés sont parallèles aux deux premiers vecteurs propres. </p></description>
    </image>
    </figure>
    <p>Finalement, la matrice <m>F</m> peut être vue comme une réflexion selon le plan perpendiculaire à <m>x=0</m> et laissant les directions <m>(1,2,1)</m> et <m>(0,0,1)</m> fixes. La figure ci-dessous illustre la transformation sur le parallélépipède engendré par ces trois directions.</p>
        <figure xml:id="fig-vecpropre5">
    <caption>La transformation <m>A</m></caption>
    <image xml:id="img-vecpropre5">
    <sageplot variant="3d">
v1=vector([1,0,0])
v2=vector([1,2,0])
v3=vector([0,0,1])
P=column_matrix([v1,v2,v3])
Pinv=P.inverse()
D=column_matrix([[-1,0,0],[0,1,0],[0,0,1]])
C=P*D*Pinv
Pointspara=[vector([0,0,0]),v1,v1+v2,v2,vector([0,0,0]),v3,v3+v1,v1,v1+v3,v3+v1+v2,v1+v2,v3+v1+v2,v2+v3,v2,v2+v3,v3,vector([0,0,0])]
Pointsparatrans=[C*pt for pt in Pointspara]
graph2=line(Pointsparatrans,color="red",alpha=0.25,aspect_ratio=1,axes=True)+line(Pointspara,color="blue",alpha=0.25,aspect_ratio=1,)+plot(v1,color="red")+plot(v2,color="darkgreen")+plot(v3,color="blue")
graph2
    </sageplot>
    <description><p>Un parallélépipède engendré par les vecteurs propres de la matrices F est illustré avec son image par cette matrice. L'image est un parallélépipiède se trouvant à être la réflexion du parallélépipède initial selon le premier vecteur.</p></description>
    </image>
    </figure>
    <p>L'un des principales avantages de la diagonalisation est lors du calcul des puissances de la matrices <m>A</m>. En général, calculer <m>A^n</m> pour une grande valeur de <m>n</m> peut être un exercice laborieux ou encore un procédé coûteux pour un ordinateur, mais si <m>A</m> est diagonalisable, alors on peut écrire
    <md>
    <mrow>A^n&amp;=(PDP^{-1})^n</mrow>
    <mrow>&amp;=\underbrace{PD\overbrace{P^{-1}P}^ID\overbrace{P^{-1}P}^IDP^{-1}\cdots PDP^{-1}}_{n \text{ fois}}</mrow>
    <mrow>&amp;=PD^nP^{-1}</mrow>
    </md>.
    Lorsqu'une matrice est diagonale, le calcul de ses puissances est beaucoup plus simple, il suffit en effet de mettre chaque entrée sur la diagonale à la puissance <m>n</m>. Pour obtenir <m>A^n</m>, il suffit donc de multiplier <m>D^n</m> à gauche par <m>P</m> et à droite par <m>P^{-1}</m>.
    </p>
    <example>
    <title>Puissances de matrices</title>
    <statement>
    <p>Pour chacune des matrices diagonalisables de l'exemple <xref ref="ex-vecvalpropres"/>, on souhaite calculer la dixième puissance. On rappelle que la diagonalisation a été trouvée à l'exemple <xref ref="ex-vecvalpropresdiago"/>. </p></statement>
    <solution>
    <p>Pour première matrice, on avait
    <me>
    A=\begin{pmatrix} -1&amp;-1\\ 2&amp;1 \end{pmatrix}\begin{pmatrix} 1&amp;0\\ 0&amp;2 \end{pmatrix}\begin{pmatrix} 1&amp;1\\ -2&amp;-1 \end{pmatrix}
    </me> et donc
    <md>
    <mrow>A^{10}&amp;=\begin{pmatrix} -1&amp;-1\\ 2&amp;1 \end{pmatrix}\begin{pmatrix} 1^{10}&amp;0\\ 0&amp;2^{10} \end{pmatrix}\begin{pmatrix} 1&amp;1\\ -2&amp;-1 \end{pmatrix}</mrow>
    <mrow>&amp;=\begin{pmatrix} -1&amp;-2^{10}\\ 2&amp;2^{10} \end{pmatrix}\begin{pmatrix} 1&amp;1\\ -2&amp;-1 \end{pmatrix}</mrow>
    <mrow>&amp;=\begin{pmatrix} -1+2^{11}&amp;-1+2^{10}\\ 2-2^{11}&amp;2-2^{10} \end{pmatrix}</mrow>
    <mrow>&amp;=\begin{pmatrix}2047 &amp; 1023 \\
-2046 &amp; -1022 \end{pmatrix}</mrow>
    </md></p>
    </solution>
    <solution><p>Pour la deuxième matrice, on avait
    <me>
    C=\begin{pmatrix} 1&amp;-1&amp;0\\ 2&amp;0&amp;0\\ 0&amp;1&amp; 1 \end{pmatrix}\begin{pmatrix} 2&amp;0&amp; 0\\ 0&amp;-2&amp; 0\\ 0&amp; 0&amp; 0 \end{pmatrix}\begin{pmatrix}0 &amp; \frac{1}{2} &amp; 0 \\ -1 &amp; \frac{1}{2} &amp; 0 \\ 1 &amp; -\frac{1}{2} &amp; 1\end{pmatrix}
    </me>
    où l'inverse a été calculée par Sage. On a alors
    <md>
    <mrow>C^{10}&amp;=\begin{pmatrix} 1&amp;-1&amp;0\\ 2&amp;0&amp;0\\ 0&amp;1&amp; 1 \end{pmatrix}\begin{pmatrix} 2^{10}&amp;0&amp; 0\\ 0&amp;(-2)^{10}&amp; 0\\ 0&amp; 0&amp; 0 \end{pmatrix}\begin{pmatrix}0 &amp; \frac{1}{2} &amp; 0 \\ -1 &amp; \frac{1}{2} &amp; 0 \\ 1 &amp; -\frac{1}{2} &amp; 1\end{pmatrix}</mrow>
    <mrow>&amp;=\begin{pmatrix} 2^{10} &amp; -2^{10} &amp; 0 \\ 2^{11} &amp; 0 &amp; 0 \\ 0 &amp; 2^{10} &amp; 0\end{pmatrix}\begin{pmatrix}0 &amp; \frac{1}{2} &amp; 0 \\ -1 &amp; \frac{1}{2} &amp; 0 \\ 1 &amp; -\frac{1}{2} &amp; 1\end{pmatrix}</mrow>
    <mrow>&amp;=\begin{pmatrix}2^{10} &amp; 0 &amp; 0 \\ 0 &amp; 2^{10} &amp; 0 \\ -2^{10} &amp; 2^9 &amp; 0\end{pmatrix}</mrow>
<mrow>&amp;=\begin{pmatrix}1024 &amp; 0 &amp; 0 \\ 0 &amp; 1024 &amp; 0 \\ -1024 &amp; 512 &amp; 0\end{pmatrix}</mrow>
    </md>.
    </p></solution>
    <solution><p>Enfin, pour la dernière matrice, on avait
    <me>
    F=\begin{pmatrix} 1&amp;1&amp;0\\ 0&amp;2&amp;0\\ 0&amp;0&amp; 1 \end{pmatrix}\begin{pmatrix} -1&amp;0&amp; 0\\ 0&amp;1&amp; 0\\ 0&amp; 0&amp; 1 \end{pmatrix}\begin{pmatrix} 1 &amp; -\frac{1}{2} &amp; 0 \\ 0 &amp; \frac{1}{2} &amp; 0 \\ 0 &amp; 0 &amp; 1\end{pmatrix}
    </me>
    où l'inverse a été calculée par Sage. On a alors
<md>
    <mrow>F^{10}&amp;=\begin{pmatrix} 1&amp;1&amp;0\\ 0&amp;2&amp;0\\ 0&amp;0&amp; 1 \end{pmatrix}\begin{pmatrix} (-1)^n&amp;0&amp; 0\\ 0&amp;1^n&amp; 0\\ 0&amp; 0&amp; 1^n \end{pmatrix}\begin{pmatrix} 1 &amp; -\frac{1}{2} &amp; 0 \\ 0 &amp; \frac{1}{2} &amp; 0 \\ 0 &amp; 0 &amp; 1\end{pmatrix}</mrow>
    <mrow>&amp;=\\begin{pmatrix} 1&amp;1&amp;0\\ 0&amp;2&amp;0\\ 0&amp;0&amp; 1 \end{pmatrix}\begin{pmatrix} 1 &amp; -\frac{1}{2} &amp; 0 \\ 0 &amp; \frac{1}{2} &amp; 0 \\ 0 &amp; 0 &amp; 1\end{pmatrix}</mrow>
    <mrow>&amp;=\begin{pmatrix}1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1\end{pmatrix}</mrow>
    </md>.
    Sur en réfléchissant sur la nature géométrique de la matrice <m>F</m>, on aurait pu trouver encore plus facilement la dixième puissance. En effet, puisqu'elle consiste en une réflexion selon le plan perpendiculaire à <m>x=0</m> et qui laisse les directions <m>(1,2,1)</m> et <m>(0,0,1)</m> fixes, les puissances de <m>F</m> devraient alterner entre <m>F</m> et l'identité <m>I</m>.
    </p></solution>
    </example>
    <p>On termine avec des commandes Sage en lien avec la sous-section.</p>
    <computation>
    <title>La diagonalisation sur Sage</title>
    <statement><p>Sage a une commande appelé <c>eigenmatrix_right</c> qui retourne une paire de matrice, les matrices <m>D</m> et <m>P</m> lorsque la matrice est diagonalisable. On peut aussi vérifier si une matrice est diagonalisable avec la commande <m>is_diagonalizable()</m>, qui fonctionne si on ajoute l'option <c>QQ</c> à la matrice (ou <c>RR</c>,<c>RDF</c>). On regarde les matrices de l'exemple <xref ref="ex-vecvalpropres"/> et lorsque diagonalisable, on fait sortir les deux matrices afin de comparer avec celles obtenues à l'exemple <xref ref="ex-vecvalpropresdiago"/>.</p>
    <sage>
    <input>
A=matrix([[3,1],[-2,0]])
B=matrix([[1,-3],[3,-5]])
C=matrix([[-2,2,0],[0,2,0],[2,-1,0]])
E=matrix([[1,1,0],[0,1,0],[0,0,2]])
F=matrix([[-1,1,0],[0,1,0],[0,0,1]])    
    </input>
    </sage>
    <sage>
    <input>
show(A.is_diagonalizable())
show(A.eigenmatrix_right())
    </input>
    </sage>
    <sage>
    <input>
show(B.is_diagonalizable())
show(B.eigenmatrix_right())
    </input>
    </sage>
    <sage>
    <input>
show(C.is_diagonalizable())
show(C.eigenmatrix_right())
    </input>
    </sage>
    <sage>
    <input>
show(E.is_diagonalizable())
show(E.eigenmatrix_right())
    </input>
    </sage>
    <sage>
    <input>
show(F.is_diagonalizable())
show(F.eigenmatrix_right())
    </input>
    </sage>
    <p>On remarque que lorsque la matrice n'est pas diagonalisable, la commande <c>eigenmatrix_right</c> retourne quand même la matrice <m>D</m> des valeurs propres et une matrice contenant des vecteurs propres indépendants et des colonnes de zéros pour la compléter.</p>
    </statement>
    </computation>
    </subsection>
    <subsection><title>Changement de base</title>
    <p>L'écriture d'une matrice <m>A=PBP^{-1}</m> n'est pas réservée au cas où la matrice <m>B=D</m> est diagonale. En fait, deux matrices qui sont reliées par une telle correspondance sont dites similaires.</p>
    <definition><title>Matrices similaires</title>
    <statement><p>Soit <m>A</m> et <m>B</m> deux matrices pour lesquelles il existe une matrice inversible <m>P</m> telle que
    <m>
    A=PBP^{-1}
    </m>.
    On dit que <m>A</m> et <m>B</m> sont semblables.
    </p></statement>
    </definition>
    <example>
    <title>Déterminer si des matrices sont similaires</title>
    <statement><p>On cherche à savoir si la matrice <m>A=\begin{pmatrix} 1&amp; 2\\ 3&amp; 4\end{pmatrix}</m> est similaire à l'une des matrices <m>B=\begin{pmatrix}5 &amp; 1 \\
2 &amp; 0\end{pmatrix}</m> ou <m>C=\begin{pmatrix} -1&amp; 0\\ 3&amp;2\end{pmatrix}</m>.</p></statement>
<solution><p>Pour la matrice <m>B</m>, on cherche une matrice inversible <m>P</m> telle que <m>A=PBP^{-1}</m>. On pose <m>P=\begin{pmatrix}a&amp;c\\ b&amp;d\end{pmatrix}</m> et on réécrit
<md>
<mrow>A&amp;=PBP^{-1}</mrow>
<mrow>AP=PB</mrow>
<mrow>\begin{pmatrix} 1&amp; 2\\ 3&amp; 4\end{pmatrix}\begin{pmatrix}a&amp;c\\ b&amp;d\end{pmatrix}&amp;=\begin{pmatrix}a&amp;c\\ b&amp;d\end{pmatrix}\begin{pmatrix}5 &amp; 1 \\
2 &amp; 0\end{pmatrix}</mrow>
<mrow>\begin{pmatrix}a + 2 \, b &amp; c + 2 \, d \\
3 \, a + 4 \, b &amp; 3 \, c + 4 \, d \end{pmatrix}&amp;=\begin{pmatrix} 5 \, a + 2 \, c &amp; a \\
5 \, b + 2 \, d &amp; b\end{pmatrix}</mrow>
</md>.
On trouve de cette égalité matricielle un système à quatre équations et quatre inconnues:
<md>
<mrow>4a-2b+2c&amp;=0</mrow>
<mrow>3a-b-2d&amp;=0</mrow>}
<mrow>a-c-2d&amp;=0</mrow>
<mrow>b-3c-4d&amp;=0</mrow>
</md>.
On fait la résolution de ce système avec Sage.
</p>
<sage>
<input>
M=matrix([[4,-2,2,0],[3,-1,0,-2],[1,0,-1,-2],[0,1,-3,-4]])
show(M.rref())
</input>
</sage>
<p>
Ce système contient deux variables libres en <m>c,d</m>. il y a donc une infinité de solutions. On doit seulement s'assurer d'avoir  <m>P</m> inversible, alors il faut que <m>ad-bc\neq 0</m>.
En prenant <m>c,d=1</m>, on trouve
<m>P=\begin{pmatrix} 3&amp;1 \\ 7&amp; 1 \end{pmatrix}</m>, qui est effectivement inversible et donc,<m>A</m> et <m>B</m> sont similaires.</p>
</solution>
<solution>
<p>Pour la matrice <m>C</m>, on cherche aussi une matrice inversible <m>P</m> telle que <m>A=PCP^{-1}</m>. On pose <m>P=\begin{pmatrix}a&amp;c\\ b&amp;d\end{pmatrix}</m> et on réécrit
<md>
<mrow>A&amp;PCP^{-1}</mrow>
<mrow>AP=PC</mrow>
<mrow>\begin{pmatrix} 1&amp; 2\\ 3&amp; 4\end{pmatrix}\begin{pmatrix}a&amp;c\\ b&amp;d\end{pmatrix}&amp;=\begin{pmatrix}a&amp;c\\ b&amp;d\end{pmatrix}\begin{pmatrix} -1&amp; 0\\ 3&amp;2\end{pmatrix}</mrow>
<mrow>\begin{pmatrix}a + 2 \, b &amp; c + 2 \, d \\
3 \, a + 4 \, b &amp; 3 \, c + 4 \, d \end{pmatrix}&amp;=-a + 3 \, c &amp; 2 \, c \\
-b + 3 \, d &amp; 2 \, d</mrow>
</md>.
On trouve de cette égalité matricielle un système à quatre équations et quatre inconnues:
<md>
<mrow>2a+2b+-3c&amp;=0</mrow>
<mrow>3a+5b-3d&amp;=0</mrow>
<mrow>-c+2d&amp;=0</mrow>
<mrow>3c+2d&amp;=0</mrow>
</md>.
On fait la résolution de ce système avec Sage.
</p>
<sage>
<input>
M=matrix([[2,2,-3,0],[3,5,0,-3],[0,0,-1,2],[0,0,3,2]])
show(M.rref())
</input>
</sage>
<p>Il n'y a qu'une solution à ce système, et c'est lorsque <m>a=b=c=d=0</m>. Cette solution ne peut toutefois pas être valide puisque la matrice <m>P</m> ne serait pas inversile. Les matrices ne sont donc pas similaires.</p>
</solution>
    </example>
    <p>Un interprétation possible de cette définition est dans le contexte du changement de base pour l'écriture d'une matrice. En effet, si <m>A</m> représente une matrice dans la base usuelle et <m>A_{\mathcal{B}}</m> sa représentation dans la base ordonnée <m>\mathcal{B}</m>, alors les deux matrices sont similaires. Ceci n'est peut-être pas surprenant, mais c'est néammoins pratique d'un point de vue calculatoire. La proposition suivante résume ce propos.</p>
    <proposition xml:id="prop-changementdebase">
    <title>Formule de changement de base</title>
    <statement><p>Soit <m>A</m> une matrice et <m>A_{\mathcal{B}}</m> sa représentation dans une base ordonnée <m>\mathcal{B}</m> et soit <m>P</m> la matrice contenant les vecteurs de <m>\mathcal{B}</m> dans ses colonnes. Alors
    <me>
    AP=PA_{\mathcal{B}}
    </me>.</p>
    <p>Puisque <m>\mathcal{B}</m> est une base, la matrice <m>P</m> est inversible et on obtient alors des formules pour les représentations de <m>A</m>:
    <md>
    <mrow>A&amp;=PA_{\mathcal{B}}P^{-1}</mrow>
    <mrow>A_{\mathcal{B}}&amp;=P^{-1}AP</mrow>
    </md>.
    La matrice <m>P</m> est appelé la matrice de changement de base ou encore la matrice de passage.
    </p>
    </statement>
    <proof><p>On analyse les colonnes de chacun des produits <m>AP</m> et <m>PA_{\mathcal{B}}</m>. Pour <m>AP</m>, chaque colonne <m>j</m> est donnée par le produit de la matrice <m>A</m> avec les vecteurs de la base <m>\mathcal{B}</m>, soit les colonnes de <m>P</m>. Ces colonnes sont donc les images des vecteurs de la base <m>\mathcal{B}</m> par <m>A</m> représentées dans la base usuelle. On peut certainement convertir ces vecteurs dans la base <m>\mathcal{B}</m>, c'est-à-dire qu'il existe <m>c_1,c_2,\ldots , c_n</m> tels que
    <me>
    A\vec{v}_{j}=c_1\vec{v}_{1}+c_2\vec{v}_2+\cdots +c_n\vec{v}_n=(c_1,c_2,\ldots , c_n)_\mathcal{B}
    </me>.
    </p>
    <p>D'un autre côté, la colonne <m>j</m> de la matrice <m>A_{\mathcal{B}}</m> est précisemment l'image du vecteur <m>\vec{v}_j</m> et est donc <m>(c_1,c_2,\ldots , c_n)</m> (dans la base usuelle cette-fois). On a alors
    <me>
    P\begin{pmatrix} c_1\\ c_2\\ \vdots \\ c_n\end{pmatrix}=c_1\vec{v}_{1}+c_2\vec{v}_2+\cdots +c_n\vec{v}_n
    </me>.
    Les colonnes de <m>AP</m> sont donc les mêmes que les colonnes de <m>PA_{\mathcal{B}}</m>.
    </p></proof>
    </proposition>
    <p>On regarde des exemples artificiels de changement de base avant de considérer un exemple plus concret.</p>
    <example>
    <title>De la base standard à une base <m>\mathcal{B}</m></title>
    <statement><p>
    On veut convertir la matrice <m>A=\begin{pmatrix} 1&amp; 4&amp; 7\\ 2&amp; 5&amp; 8\\ 3&amp; 6&amp; 9\end{pmatrix}</m> dans la base <m>\mathcal{B}= \langle (1,0,1),(-1,2,-3),(2,2,1)\rangle</m>.
    </p>
    <solution><p>On pose <m>P=\begin{pmatrix}  1&amp;-1&amp;2 \\0&amp;2&amp;2\\ 1&amp;-3&amp;1 \end{pmatrix}</m>. Selon la proposition <xref ref="prop-changementdebase"/>, la formule pour écrire <m>A</m> dans la base <m>\mathcal{B}</m> est
    <me>
    A_{\mathcal{B}}=P^{-1}AP
    </me>.
    On utilise Sage pour les calculs.
    </p>
    <sage>
    <input>
    A=column_matrix([[1,2,3],[4,5,6],[7,8,9]])
    P=column_matrix([[1,0,1],[-1,2,-3],[2,2,1]])
    Pinv=P.inverse()
    AB=Pinv*A*P
    show(AB)
    </input>
    </sage>
    <p>On a donc
    <me>
    A_{\mathcal{B}}=\begin{pmatrix} -29 &amp; 38 &amp; -68 \\
-9 &amp; 12 &amp; -21 \\
14 &amp; -20 &amp; 32\end{pmatrix}
    </me>.
    Les colonnes de <m>A_{\mathcal{B}}</m> correspondent à l'écriture des vecteurs <m>(1,2,3),(4,5,6),(7,8,9)</m> dans la base <m>\mathcal{B}</m>.
    </p>
    </solution>
    </statement>
    </example>
    <example><title>D'une base <m>\mathcal{B}</m> à la base standard</title>
    <statement><p>Soit <m>C_{\mathcal{B}}=\begin{pmatrix} 1&amp;1&amp;1&amp;3 \\ 1&amp; -1&amp; 2&amp;-3\\ 1&amp; 1&amp; 2&amp;3\\ 1&amp; -1&amp; 1&amp;-3\end{pmatrix}_{\mathcal{B}}</m> où <m>\mathcal{B}=\langle (4,3,2,1),(-2,4,-6,8),(0,1,1,0),(3,5,-1,0)\rangle</m>. On cherche la représentation de ces vecteurs dans la base standard.</p></statement>
    <solution><p>On procède comme dans l'exemple précédent, avec une matrice <m>C_{\mathcal{B}}=\begin{pmatrix} 1&amp;1&amp;1&amp;3 \\ 1&amp; -1&amp; 2&amp;-3\\ 1&amp; 1&amp; 2&amp;3\\ 1&amp; -1&amp; 1&amp;-3\end{pmatrix}_{\mathcal{B}}</m> et une matrice de passage <m>P=\begin{pmatrix}4&amp; -2&amp; 0&amp;3 \\ 3&amp; 4&amp;1 &amp;5\\ 2&amp; -6&amp;1&amp;1\\ 1&amp;8&amp; 0&amp; 0 \end{pmatrix}</m>. Selon la proposition <xref ref="prop-changementdebase"/>, la formule pour obtenir la matrice <m>C</m> est 
    <me>
    C=PC_{\mathcal{B}}P^{-1}
    </me>. On utilise Sage pour les calculs.</p>
    <sage>
    <input>
    CB=column_matrix([[1,1,1,1],[1,-1,1,-1],[1,2,2,1],[3,-3,3,-3]])
    P=column_matrix([[4,3,2,1],[-2,4,-6,8],[0,1,1,0],[3,5,-1,0]])
    Pinv=P.inverse()
    C=P*CB*Pinv
    show(C)
    </input>
    </sage>
    <p>La matrice dans la base usuelle est donc
    <me>
    C=\begin{pmatrix} -\frac{5}{7} &amp; \frac{33}{14} &amp; \frac{9}{14} &amp; -\frac{1}{2} \\
-\frac{286}{35} &amp; \frac{321}{70} &amp; \frac{939}{70} &amp; \frac{51}{10} \\
\frac{163}{35} &amp; \frac{41}{35} &amp; -\frac{356}{35} &amp; -\frac{29}{5} \\
-\frac{127}{15} &amp; \frac{107}{30} &amp; \frac{403}{30} &amp; \frac{53}{10}\end{pmatrix}
    </me>.
    </p>
    </solution>
    </example>
    <p>On revient maintenant sur l'un des exemples du chapitre <xref ref="chap-transfo"/>, dans lequel on avait trouvé la matrice d'une rotation autour d'un axe quelconque dans <m>\mathbb{R}^3</m>. À ce moment, on avait déplacé l'axe de rotation sur l'axe des <m>z</m> afin d'utiliser la matrice de rotation connue <m>R_{\theta_z}</m>. On peut maintenant procéder autrement avec la formule de changement de base. On aura besoin de la proposition suivante.</p>
    <proposition xml:id="prop-rotR3B">
    <title>Matrice de rotation dans une base orthonormée</title>
    <statement><p>Soit <m>\mathbb{B}=\langle \vec{v}_1,\vec{v}_2,\vec{v}_3 \rangle</m> une base de <m>\mathbb{R}^3</m> telle que 
    <md>
    <mrow>\vec{v}_1\cdot \vec{v}_2&amp;=0</mrow>
    <mrow>\vec{v}_1\cdot \vec{v}_3&amp;=0</mrow>
    <mrow>\vec{v}_2\cdot \vec{v}_3&amp;=0</mrow>
    <mrow>\norm{\vec{v}_1}&amp;=1 &amp;\norm{\vec{v}_2}&amp;=1&amp;\norm{\vec{v}_3}&amp;=1</mrow>
    <mrow>\text{det}(\vec{v}_1,\vec{v}_2,\vec{v}_3)&amp;\gt 0</mrow>
    </md>.
    On dit que la base est orthonormée, c'est-à-dire que les vecteurs sont orthogonaux deux à deux et ont une norme égale à <m>1</m>, et est orientée positivement. Dans ce cas, la matrice de rotation autour de <m>\vec{v}_3</m> dans la base <m>\mathcal{B}</m> est 
    <men xml:id="eq-rotR3B">
    R_{\theta_{\vec{v_3}}\mathcal{B}}=\begin{pmatrix}\cos(\theta)&amp;-\sin(\theta)&amp; 0\\ \sin(\theta)&amp;\cos(\theta)&amp; 0\\ 0&amp;0&amp;1\end{pmatrix}_{\mathcal{B}}
    </men>.
    </p></statement>
    <proof><p>Puisque <m>\vec{v}_3</m> est l'axe de rotation, on doit avoir pour <m>\vec{v}_3=(0,0,1)_{\mathcal{B}}</m> que <m>R_{\theta_{\vec{v_3}}\mathcal{B}}\,\vec{v}_3=R_{\theta_{\vec{v_3}}\mathcal{B}}\begin{pmatrix}0\\ 0\\ 1\end{pmatrix}_{\mathcal{B}}=\begin{pmatrix}0\\ 0\\ 1\end{pmatrix}_{\mathcal{B}}</m> et donc, la troisième colonne de la matrice est déterminée.</p>
    <p>Le même dessin que celui de la figure <xref ref="fig-rotr2"/> peut être utilisé pour voir que le vecteur <m>\vec{v}_1=(1,0,0)_\mathcal{B}</m> sera déplacé à <m>(\cos(\theta),\sin(\theta),0)_{\mathcal{B}}</m> et que <m>\vec{v}_2=(0,1,0)_{\mathcal{B}}</m> sera envoyé sur <m>(-\sin(\theta),\cos(\theta),0)_{\mathcal{B}}</m>. On obtient alors les deux premières colonnes.</p>
    </proof>
    </proposition>
    <p>Il est mentionné que la base formée des vecteurs <m>\vec{v}_1,\vec{v}_2,\vec{v}_3</m> doit être orientée positivement pour que la formule soit correcte. En effet, puisqu'une rotation est par défaut dans le sens antihoraire, il faut que dans le repère, on puisse aller de <m>\vec{v}_2</m> vers <m>\vec{v}_2</m> avec une rotation antihoraire de <m>90^\circ</m>. Il est aussi important que <m>\vec{v}_1</m> et <m>\vec{v}_2</m> soit perpendiculaire afin que <m>\vec{v}_2=(0,1,0)_{\mathcal{B}}</m> corresponde avec l'angle <m>\theta</m> et le rapport d'un quart de tour. Finalement, on demande la norme égale à <m>1</m> puisque les vecteurs <m>(\cos(\theta),\sin(\theta),0)</m> et <m>(-\sin(\theta),\cos(\theta),0)</m> sont unitaires. L'importance des bases orthonormées sera précisée dans la section <xref provisional="sec-orthogonal"/>.</p>
    <example xml:id="ex-rotR3B"><title>Matrice de rotation dans <m>\mathbb{R}^3</m></title>
    <statement><p>On revient sur l'exemple <xref ref="sageex-transfodimsup"/> dans lequel on a trouvé la matrice représentant une rotation autour de l'axe <m>(-1,1,2)</m> de <m>60^\circ</m>. On veut utiliser une matrice de changement de base afin de retrouve l'expression de cette transformation (dans la base standard).</p></statement>
    <solution><p>
    On doit trouver une base <m>\mathcal{B}</m> qui va satisfaire les conditions de la proposition <xref ref="prop-rotR3B"/>. On prend <m>\vec{v}_3=(-1,1,2)</m> l'axe de rotation de rotation. Pour avoir un vecteur perpendiculaire, il suffit de prendre un vecteur qui satisfait l'équation <m>-x+y+2z=0</m>. On opte pour <m>\vec{v}_1=(1,1,0)</m>. Pour le second vecteur, on doit s'assurer qu'il sera perpendiculaire à <m>\vec{v}_1</m> et à <m>\vec{v}_3</m> et que le repère sera orienté positivement. On sait que <m>\langle \vec{v}_1,\vec{v}_3,\vec{v}_1\times \vec{v}_3 \rangle</m> serait orienté positivement selon l'exercice <xref provisional="exo-produitvectorielorientation"/>. On aurait donc que le repère <m>\langle \vec{v}_1,\vec{v}_1\times \vec{v}_3,\vec{v}_3 \rangle</m> serait orienté négativement et finalement, que <m>\langle \vec{v}_1,-\vec{v}_1\times \vec{v}_3,\vec{v}_3 \rangle</m>. On pose donc <m>\vec{v}_2=-\vec{v}_1\times \vec{v}_3</m>.
    </p>
    <sage>
    <input>
v1=vector([1,1,0])
v3=vector([-1,1,2])  
v2=-v1.cross_product(v3)
#Validation
column_matrix([v1,v2,v3]).determinant()>0
    </input>
    </sage>
    <p>En fait, ces vecteurs ne sont pas de norme <m>1</m>, alors on doit les normaliser avant de pouvoir utiliser la formule <xref ref="eq-rotR3B"/>. On utilise ensuite la matrice de passage formé des vecteurs unitaires pour déterminer la matrice <m>A</m> dans la base usuelle.</p>
    <sage>
    <input>
v1=vector([1,1,0])
v3=vector([-1,1,2])  
v2=-v1.cross_product(v3)
v1chap=v1/norm(v1)
v2chap=v2/norm(v2)
v3chap=v3/norm(v3)
t=60/360*2*pi
P=column_matrix([v1chap,v2chap,v3chap])
RB=column_matrix([[cos(t),sin(t),0],[-sin(t),cos(t),0],[0,0,1]])
R=P*RB*(P.inverse())
show(R.apply_map(lambda x: x.trig_reduce()))  
show(R.apply_map(lambda x: x.n()))     
    </input>
    </sage>
    </solution>
    </example>
    <p>Dans certains cas, il pourrait être utile de trouver la représentation d'une matrice dans une base <m>\mathcal{C}</m> à partir de sa représentation dans une base <m>\mathcal{B}</m>. Puisqu'on sait comment passer d'une base quelconque à la base standard et de la base standard à une base usuelle, on devrait pouvoir trouver une manière de passer directement entre les bases non standards.</p>
    <proposition xml:id="prop-changementdebasegeneral">
    <title>Formule générale pour le changement de base</title>
    <statement><p>Soit <m>\mathcal{B},\mathcal{C}</m> deux bases ordonnées et soit <m>P,Q</m> les matrices contenant les vecteurs de ces bases dans leurs colonnes. Soit <m>A</m> une matrice, <m>A_{\mathcal{B}}</m> sa représentation dans la base <m>\mathcal{B}</m> et <m>A_\mathcal{C}</m> celle dans la base <m>\mathcal{C}</m>. Alors
    <md>
    <mrow>A_{\mathcal{C}}&amp;=P_{\mathcal{B}\to \mathcal{C}}A_{\mathcal{B}}P_{\mathcal{B}\to \mathcal{C}}^{-1}</mrow>
    <mrow>A_{\mathcal{B}}&amp;=P_{\mathcal{C}\to \mathcal{B}}A_{\mathcal{C}}P_{\mathcal{C}\to \mathcal{B}}^{-1}</mrow>
    </md>.</p>
    <p>où <m>P_{\mathcal{B}\to \mathcal{C}}=Q^{-1}P</m> est la matrice de changement de base de <m>\mathcal{B}</m> à <m>\mathcal{C}</m> et <m>P_{\mathcal{C}\to \mathcal{B}}=P^{-1}Q</m> est la matrice de changement de base de <m>\mathcal{C}</m> à <m>\mathcal{B}</m>.</p>
    </statement>
    <proof><p>Voir l'exercice <xref ref="exo-changementgeneral"/>.</p></proof>
    </proposition>
    <example>
    <title>Changement de base générale</title>
    <statement><p>Soit <m>\mathcal{B}=\langle (1,1),(1,-1)\rangle</m> et <m>\mathcal{C}=\langle (-2,1),(3,2)\rangle</m> deux bases de <m>\mathbb{R}^2</m> et 
    <me>
    A_{\mathbb{B}}=\begin{pmatrix}3&amp;-2\\ -1&amp; -4 \end{pmatrix}_{\mathcal{B}}
    </me> la matrice d'une transformation linéaire représenté en base <m>\mathcal{B}</m>. On cherche la représentation de <m>A</m> en base <m>\mathcal{C}</m>.</p></statement>
    <solution><p>
    On pose <m>P=\begin{pmatrix}1&amp;1\\ 1&amp; -1 \end{pmatrix}</m> et <m>Q=\begin{pmatrix}-2&amp;3\\ 1&amp; 2 \end{pmatrix}</m>. On a alors
    <md>
    <!--<mrow>P^{-1}&amp;=-\frac{1}{2}\begin{pmatrix}-1&amp;-1\\ -1&amp; 1 \end{pmatrix}</mrow>-->
    <mrow>Q^{-1}&amp;=-\frac{1}{7}\begin{pmatrix}2&amp;-3\\ -1&amp; -2 \end{pmatrix}</mrow>
    </md>,
    Ce qui donne comme matrice de passage 
    <md>
    <mrow>P_{\mathcal{B}\to\mathcal{C}}&amp;=Q^{-1}P</mrow>
    <mrow>&amp;=-\frac{1}{7}\begin{pmatrix}2&amp;-3\\ -1&amp; -2 \end{pmatrix}\begin{pmatrix}1&amp;1\\ 1&amp; -1 \end{pmatrix}</mrow>
    <mrow>&amp;=-\frac{1}{7}\begin{pmatrix}1&amp;-5\\ 3&amp; -1 \end{pmatrix}</mrow>
    </md>.
    L'inverse de cette matrice est
    <md>
    <mrow>P_{\mathcal{B}\to\mathcal{C}}^{-1}&amp;=\left(-\frac{1}{7}\begin{pmatrix}1&amp;-5\\ 3&amp; -1 \end{pmatrix}\right)^{-1}</mrow>
    <mrow>&amp;=-7\begin{pmatrix}1&amp;-5\\ 3&amp; -1 \end{pmatrix}^{-1}&amp;&amp; \text{ selon } <xref ref="prop-invdematscalaire"/></mrow>
    <mrow>&amp;=-7\frac{1}{14}\begin{pmatrix}-1&amp;5\\ -3&amp; 1 \end{pmatrix}</mrow>
<mrow>&amp;=-\frac{1}{2}\begin{pmatrix}-1&amp;5\\ -3&amp; 1 \end{pmatrix}</mrow>
    </md>.
    En utilisant la formule de changement de base de la proposition <xref ref="prop-changementdebasegeneral"/>, on obtient finalement
    <md>
    <mrow>A_{\mathcal{C}}&amp;=P_{\mathcal{B}\to \mathcal{C}}A_{\mathcal{B}}P_{\mathcal{B}\to \mathcal{C}}^{-1}</mrow>
    <mrow>&amp;=-\frac{1}{7}\begin{pmatrix}1&amp;-5\\ 3&amp; -1 \end{pmatrix}\begin{pmatrix}3&amp;-2\\ -1&amp; -4 \end{pmatrix}_{\mathcal{B}}-\frac{1}{2}\begin{pmatrix}-1&amp;5\\ -3&amp; 1 \end{pmatrix}</mrow>
    <mrow>&amp;=-\frac{1}{14}\begin{pmatrix}1&amp;-5\\ 3&amp; -1 \end{pmatrix}\begin{pmatrix}3&amp;13\\ 13&amp; -9 \end{pmatrix}</mrow>
    <mrow>&amp;=-\frac{1}{14}\begin{pmatrix}-62 &amp; 58 \\ -4 &amp; 48 \end{pmatrix}</mrow>
    </md>.
    </p></solution>
    </example>
    <p>On pourrait aussi parler de changement de bases pour des matrices rectangulaires et même entre des espaces vectoriels plus généraux. On verra quelques exemples dans les exercices.</p>
    </subsection>
    <subsection>
    <title>
    Quelques résultats
    </title>
    <p>On termine cette section avec quelques résultats plus théoriques sur la diagonalisation et le changement de base.Lorsque deux matrices sont similaires, elles ont le même polynome caractéristique. En particulier, elles ont les mêmes valeurs propres.</p>
    <proposition xml:id="prop-similvalpropres">
    <title>Les matrices similaires ont les mêmes valeurs propres</title>
    <statement>
    <p>Si <m>A</m> et <m>B</m> sont similaires, elles ont le même polynôme caractéristique. En particulier, elles ont les mêmes valeurs propres.</p>
    </statement>
    <proof><p>Voir l'exercice <xref ref="exo-similvalpropres"/>.</p></proof>
    </proposition>
    <p>Comme deuxième résultat, une condition suffisante, mais pas nécessaire pour qu'une matrice carrée  soit diagonalisable. </p>
<proposition>
<title>Une matrice possédant <m>n</m> valeurs propres est diagonalisable</title>
<statement><p>Soit <m>A</m> une matrice <m>n\times n</m> qui possède <m>n</m> valeurs propres disctinctes. Alors <m>n</m> est diagonalisable.</p></statement>
<proof><p>Selon la proposition <xref ref="prop-propresindep"/>, les vecteurs propres associés à des valeurs propres disctinctes sont indépendants. Alors la matrice <m>P</m> est existe et est inversible. Dans la base <m>\mathcal{B}</m> des vecteurs propres, la matrice <m>A</m> est égale à une matrice diagonale <m>D_{\mathcal{B}}</m> où les entrées sont les valeurs propres. On a donc
<me>
A=PDP^{^1}
</me> selon la formule du changement de base.</p></proof>
</proposition>
<p>Bien sur, il existe des matrices diagonalisable qui n'ont pas <m>n</m> valeurs propres disctinctes, c'est pour cela qu'on dit que la condition est suffisante (dans le sens où, si on l'a c'est suffisant pour garantir), mais pas nécessaire (on peut être diagonalisable sans avoir les valeurs propres disctinctes). En fait, une condition qui est nécessaire et suffisante concerne la relation entre la multiplicité algébrique et la multiplicité géométrique des valeurs propres. On se souvient que pour être diagonalisable, il faut que la matrice possède <m>n</m> vecteurs propres indépendants. Pour chaque valeurs propres, on veut donc produire suffisamment de vecteurs propres pour que le nombre total donne <m>n</m>. On aura besoin des deux résultats suivants pour démontrer la condition.</p>
<lemma xml:id="lem-multgeoalg">
<title>La multiplicité géométrique est plus petite ou égale à la multiplicité algébrique</title>
<statement>
<p>Soit <m>A</m> une matrice carrée et <m>\lambda_0</m> une valeur propres de multiplicité géométrique <m>g</m> et de multiplicité algébrique <m>a</m>. Alors <m>1\leq g\leq a</m>.</p>
</statement>
<proof><p>Par définition, la multiplicité géométrique est la dimension de l'espace propre et celui-ci contient au moins un vecteur, donc <m>1\leq g</m>. Soit <m>\vec{v}_1,\vec{v}_2,\ldots ,\vec{v}_g</m> une base de cet espace prope. Il est toujours possible d'étendre cette base en ajoutant des vecteurs indépendants pour former une base de <m>\mathbb{R}^n</m>. Soit <m>\mathcal{B}=\langle \vec{v}_1,\vec{v}_2,\ldots ,\vec{v}_g,\vec{v}_{g+1},\ldots , \vec{v}_n </m> une base possible. À quoi ressemble la matrice <m>A</m> dans la base <m>\mathcal{B}</m>?</p> <p>Les <m>g</m> premières colonnes correspondent aux vecteurs propres associés à <m>\lambda_0</m>, qui sont les premiers vecteurs de la base. En base <m>\mathcal{B}</m>, l'écriture d'un vecteur<m>\vec{v}_i</m> est équivalente à celle du vecteur <m>\vec{e}_i</m> si <m>1\leq i\leq g</m> et donc <m>A_{\mathcal{B}}\vec{v}_i=\lambda_0\vec{e}_i=</m>, signifiant que la première partie de <m>A_{\mathcal{B}}</m> est équivalente à la première partie de <m>\lambda_0 I</m>. Pour le reste, on ne connait pas, mais on peut écrire la matrice comme
<me>
A_{\mathcal{B}}=\begin{pmatrix}
\lambda_0 I
  &amp; \hspace*{-\arraycolsep}\vline\hspace*{-\arraycolsep} &amp; B \\
\hline
  O &amp; \hspace*{-\arraycolsep}\vline\hspace*{-\arraycolsep} &amp;
C
\end{pmatrix}
</me>
où <m>B</m> est une matrice <m>g\times (n-g)</m> quelconque, la matrice nulle <m>O</m> est de taille <m>(n-g)\times g</m> et <m>C</m> set une matrice carrée <m>(n-g)\times (n-g)</m>. Selon la proposition <xref ref="exo-similvalpropres"/>, les matrices <m>A</m> et <m>A_{\mathcal{B}}</m> ont le même polynôme caractéristique puisqu'elles sont similaires. En développant avec les cofacteurs la matrice <m>A_{\mathcal{B}}-\lambda I</m>, on trouve
<md>
<mrow>\text{det}(A_{\mathcal{B}}-\lambda I)&amp;=\begin{pmatrix}
\lambda_0 I-\lambda I
  &amp; \hspace*{-\arraycolsep}\vline\hspace*{-\arraycolsep} &amp; B \\
\hline
  O &amp; \hspace*{-\arraycolsep}\vline\hspace*{-\arraycolsep} &amp;
C-\lambda I
\end{pmatrix}</mrow>
<mrow>&amp;=(\lambda_0-\lambda)^d\text{det}(C-\lambda I)</mrow>
</md>.
</p>
<p>On voit donc que la multiplicité algébrique de <m>\lambda_0</m> est d'au moins <m>g</m>, puisque le facteur <m>(\lambda_0-\lambda)^g</m> apparait dans le polynôme caractéristique. On a donc <m>g\leq a</m>.</p>
</proof>
</lemma>
<lemma xml:id="lem-sommepropre">
<title>Une combinaison spéciale de vecteurs</title>
<statement><p>Soit <m>A</m> un matrice et <m>\lambda_1,\lambda_2,\ldots , \lambda_k</m> des valeurs propres distinctes. Soit <m>\vec{u}_1,\vec{u}_2,\ldots , \vec{u}_k</m> des vecteurs dans les espaces propres respectifs des valeurs propres. Si 
<me>
\vec{u}_1+\vec{u}_2+\ldots + \vec{u}_k=\vec{0}
</me>,
alors <m>\vec{u}_1=\vec{u}_2=\ldots = \vec{u}_k=\vec{0}</m>.
</p></statement>
<proof><p>On suppose qu'il y a des vecteurs non nuls. On peut simplement éliminer tous les vecteurs nuls de l'équation et obtenir une équation similaire, alors on suppose simplement qu'on a un ensemble de <m>p</m> vecteurs non nuls 
<me>
\vec{v}_1+\vec{v}_2+\ldots + \vec{v}_p=\vec{0}
</me> où chaque vecteur <m>\vec{v}_i</m> est un des vecteurs dans <m>\{\vec{u}_1,\ldots , \vec{u}_k\}</m>. Si ces vecteurs sont non nuls et dans un espace propre, ce sont donc des vecteurs propres. Selon la proposition <xref ref="prop-propresindep"/>, on ne peut pas avoir une combinaison linéaire de vecteurs propres associés à des valeurs propres différentes qui donne le vecteur nul, car ces vecteurs devraient être indépendants. Les vecteurs <m>\vec{v}</m> ne peuvent donc pas être non nuls. On conclut alors que tous les vecteurs <m>\vec{u}</m> sont en effet nuls.  </p></proof>
</lemma>
<p>Avec ces résultats, on est en mesure de démontrer qu'une matrice est diagonalisable si et seulement si les multiplicités algébriques et géométriques sont égales.</p>
<proposition xml:id="prop-matdiagomult">
<title>Une matrice dont les multiplicités géométriques sont les mêmes que les multiplicités algébriques est diagonalisable </title>
<statement><p>
Soit <m>A</m> une matrice <m>n\times n</m> et soit <m>\lambda_1,\lambda_2,\ldots ,\lambda_k</m> ses valeurs propres discinctes. Alors <m>A</m> est diagonalisable si et seulement si les multiplicités géométriques <m>g_i</m> des <m>k</m> valeurs propres sont les mêmes que les multiplicités algébriques <m>a_i</m> et somment à <m>n</m>.
</p>
<p>La condition que les multiplicités somment à <m>n</m> pourra être enlevée en considérant les nombres complexes.</p>
</statement>
<proof>
<p>On suppose dans un premier temps que <m>g_i=a_i</m> pour toutes les valeurs propres et que la somme de ces multiplicités donnent <m>n</m>. Pour chaque valeur propre, il existe une base <m>\mathcal{B}_i=\langle \vec{v}_{i,1}\vec{v}_{i,2}\ldots \vec{v}_{i,g_i} \rangle</m> de <m>g_i</m> vecteurs propres indépendants. On considère l'ensemble <m>\mathcal{B}=\mathcal{B}_1\cup \mathcal{B}_2\cup\cdots \cup \mathcal{B}_k</m>. Cet ensemble contient <m>g_1+g_2+\cdot +g_k=n</m> vecteurs propres.</p>
<p>On suppose qu'on a une combinaison linéaire de tous ces vecteurs qui donne le vecteur nul. On peut écrire cette combinaison de la manière suivante:
<md>
<mrow>\vec{0}&amp;=a_{1,1}\vec{v}_{1,1}+a_{1,2}\vec{v}_{1,2}+\cdots a_{1,g_1}\vec{v}_{1,g_1}</mrow>
<mrow>&amp;\phantom{=}+a_{2,1}\vec{v}_{2,1}+a_{2,2}\vec{v}_{2,2}+\cdots a_{2,g_1}\vec{v}_{2,g_2}</mrow>
<mrow>&amp;\phantom{=}+a_{3,1}\vec{v}_{3,1}+a_{3,2}\vec{v}_{3,2}+\cdots a_{3,g_1}\vec{v}_{3,g_3}</mrow>
<mrow>&amp; \phantom{=} \phantom{a_{3,1}\vec{v}_{3,1}}\vdots</mrow>
<mrow>&amp;\phantom{=}+a_{k,1}\vec{v}_{k,1}+a_{k,2}\vec{v}_{k,2}+\cdots a_{k,g_1}\vec{v}_{k,g_k}</mrow>
</md>.
Pour chaque <m>1\leq i\leq k</m>, on pose
<me>
\vec{u}_i=a_{i,1}\vec{v}_{i,1}+a_{i,2}\vec{v}_{i,2}+\cdots a_{i,g_1}\vec{v}_{i,g_i}
</me>.
Chacun de ces vecteurs appartient à l'espace propre de <m>\lambda_i</m>. La combinaison linéaire des vecteurs dans <m>\mathcal{B}</m> devient donc
<me>
\vec{u}+1+\vec{u}_2+\cdots \vec{u}_k=\vec{0}
</me>.
En vertu du lemme <xref ref="lem-sommepropre"/>, on doit avoir <m>\vec{u}_i=\vec{0}</m> pour tous les <m>i</m>. Ceci entraine à son tour que tous les coefficients <m>a_{i,1},a_{i,2},\ldots , a_{i,g_i}</m> sont nuls car les vecteurs <m>\vec{v}_{i,1}\vec{v}_{i,2}\ldots \vec{v}_{i,g_i}</m> forment une base. Ainsi, l'ensemble <m>\mathcal{B}</m> est indépendant et puisqu'il est composé de <m>n</m> vecteurs dans un espace de dimension <m>n</m>, il forme une base. En posant <m>P</m> la matrice contenant ces vecteurs dans ses colonnes et <m>D</m> la matrice des valeurs propres associés, dans le même ordre d'apparition que les vecteurs dans <m>P</m>, on a bel et bien
<me>
AP=PD
</me>.
Ainsi, <m>A=PDP^{-1}</m> est diagonalisable.
</p> 
<p>Dans un deuxième temps, on suppose que <m>A</m> est diagonalisable. On veut montrer que les multiplicités géométriques et algébriquent concordent et somment à <m>1</m>. </p>
<p>Soit <m>\mathcal{B}</m> une base de vecteurs propres pour l'espace vectoriel. On note <m>\mathcal{B}_i=\mathcal{B}\cap Ep(\lambda_i)</m> l'intersection de cette base avec l'espace propre associé à <m>\lambda_i</m>. Soit <m>n_i</m> le nombre de vecteurs dans <m>\mathcal{B}_i</m>. On a alors <m>n_i\leq g_i</m> puisque <m>Ep(\lambda_i)</m> est de dimension <m>g_i</m> et que les vecteurs dans <m>\mathcal{B}</m> sont indépendants (on ne peut avoir plus de vecteurs indépendants que la dimension du sous-espace, selon la proposition <xref ref="prop-ensemblevecetdim"/>). De plus, selon le lemme <xref ref="lem-multgeoalg"/>, on a <m>g_i\leq m_i</m>. Comme il y a <m>n</m> vecteurs dans <m>\mathcal{B}</m> et que <m>A</m> est diagonalisable, il faut que <m>n_1+n_2+\cdots n_k=n</m>. De même, on peut écrire le polynôme caractéristique de <m>A</m> comme
<me>
\text{det}(A-\lambda I)=(\lambda_1-\lambda)^{m_1}(\lambda_2-\lambda)^{m_2}\cdots(\lambda_k-\lambda)^{m_k}
</me> et comme ce polynôme doit être de degré <m>n</m>, on doit avoir que <m>m_1+m_2+\cdots m_k=n</m>. En combinant le tout, on a
<md>
<mrow>n&amp;\leq n_1+n_2+\cdots n_k</mrow>
 <mrow>    &amp;\leq g_1+g_2+\cdots g_k</mrow>  
 <mrow>&amp;=\leq m_1+m_2+\cdots m_k</mrow>
 <mrow>&amp;\leq n</mrow>
</md>.
Ainsi, <m>n_1+n_2+\cdots n_k=g_1+g_2+\cdots g_k=m_1+m_2+\cdots m_k=n</m>. En particulier, on a
<me>
(a_1-g_1)+(a_2-g_2)+\cdots (a_k-g_k)=0
</me>
et comme chacun de ces termes est plus grand que <m>0</m>, on conclut que <m>a_i=g_i</m> pour tout <m>1\leq i\leq k</m> et que les multiplicités concordent.
</p>
<p>*<em>Bruit de soupir</em>*</p>
</proof>
</proposition>
<p>En se rappelant les matrices <m>E,F</m> de l'exemple <xref ref="ex-vecvalpropres"/>, on constate que la valeur propre <m>\lambda_1=1</m> avait comme multiplicité algébrique <m>2</m>, mais une multiplicité géométrique de <m>1</m> pour la matrice <m>E</m>, alors que dans le cas de la matrice <m>F</m>, on avait multiplicités algébrique et géométrique égales à <m>2</m> pour <m>\lambda_1=1</m>. On peut donc conclure que <m>F</m> est diagonalisable, mais pas <m>E</m>.</p>
<sage>
<input>
E=matrix(QQ,[[1,1,0],[0,1,0],[0,0,2]])
F=matrix(QQ,[[-1,1,0],[0,1,0],[0,0,1]])
show(E.eigenmatrix_right())
show(F.eigenmatrix_right())
</input>
</sage>
<p>On constate en effet que la matrice <m>P</m> donnée par Sage n'est pas inversible et donc que <m>E</m> n'est pas diagonalisable.</p>
    </subsection>
    <!-- Sous-sections à écrire, à même ce fichier -->
    
    <conclusion xml:id="concl-diagonalisation">  <!-- Ajouter le même identifiant de la section après le - du xml:id -->
    <p>Les points importants de cette section sont:
    <ul>
    <li><p>La notion de matrice <xref ref="def-diagonalisable" text="custom">diagonalisable</xref>;</p></li>
    <li><p>La formule de <xref ref="prop-changementdebase" text="custom">changement de base</xref>;</p></li>
    <li><p>La représentation <xref ref="prop-rotR3B" text="custom">matricielle d'une rotation</xref> dans une base orthonormée;</p></li>
    <li><p>Les différentes propositions sur les valeurs propres d'une matrice, notamment celle en lien avec les <xref ref="prop-matdiagomult" text="custom">multiplicités</xref> des valeurs propres.</p></li>
    </ul></p>
    <p>De plus, avec Sage, on a vu la commande <c>eigenmatrix_right</c> permettant de diagonaliser une matrice lorsque possible.</p>
    </conclusion>
   <!--Inclure les exercices de la section ci-dessous--> 
   <xi:include href="Exercices_diagonalisation.xml"/>
</section>